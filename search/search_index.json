{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to panlm docs","text":""},{"location":"#1-start-from","title":"1 start from","text":"<ul> <li>quick-setup-cloud9</li> </ul>"},{"location":"#2-highlights","title":"2 highlights","text":"<p><pre><code>([\"status\":\"myblog\"])\n\n- [$frontmatter:title]($filename): $frontmatter:description\n</code></pre> - Cross Region Reverse Proxy with NLB and Cloudfront: \u8de8\u533a\u57df\u7684 Layer 4 \u53cd\u5411\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528 nlb + cloudfront\uff0c\u8003\u5bdf\u8bc1\u4e66\u4f7f\u7528\u9700\u6c42 - Enable scan on push in ECR and send notification to SNS: \u542f\u7528 ECR \u7684 Scan on push \u4e4b\u540e\uff0c\u81ea\u52a8\u5c06\u626b\u63cf\u7ed3\u679c\u4e2d CRITICAL \u7684\u4fe1\u606f\u53d1\u9001\u5230\u76ee\u6807 SNS \u544a\u8b66 - Export Cloudwatch Log Group to S3: \u5bfc\u51fa cloudwatch \u65e5\u5fd7\u5230 s3 - Using Global SSO to Login China AWS Accounts: \u4f7f\u7528 global sso \u767b\u5f55\u4e2d\u56fd\u533a\u57df aws \u8d26\u53f7 - quick setup cloud9 script: \u7b80\u5316\u8fd0\u884c\u811a\u672c - Setup Cloud9 for EKS: \u4f7f\u7528 cloud9 \u4f5c\u4e3a\u5b9e\u9a8c\u73af\u5883 - Stream EKS Control Panel Logs to S3: \u76ee\u524d EKS \u63a7\u5236\u5e73\u9762\u65e5\u5fd7\u53ea\u652f\u6301\u53d1\u9001\u5230 cloudwatch\uff0c\u4e14\u5728\u540c\u4e00\u4e2a log group \u4e2d\u67095\u79cd\u7c7b\u578b6\u79cd\u524d\u7f00\u7684 log stream \u7684\u65e5\u5fd7\uff0c\u4e0d\u5229\u4e8e\u7edf\u4e00\u67e5\u8be2\u3002\u4e14\u53ea\u6709 audit \u65e5\u5fd7\u662f json \u683c\u5f0f\u5176\u4ed6\u5747\u662f\u5355\u884c\u65e5\u5fd7\uff0c\u4e14\u5b57\u6bb5\u5404\u4e0d\u76f8\u540c\u3002\u672c\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u601d\u8def\u7edf\u4e00\u4fdd\u5b58\u65e5\u5fd7\u4f9b\u540e\u7eed\u5206\u6790\u5904\u7406 - Building Prometheus HA Architect with Thanos: \u7528 Thanos \u89e3\u51b3 Prometheus \u5728\u591a\u96c6\u7fa4\u5927\u89c4\u6a21\u73af\u5883\u4e0b\u7684\u9ad8\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u9650\u5236 &lt;\u2013&gt;</p>"},{"location":"#3-my-aws-blogs","title":"3 my aws blogs","text":"<ul> <li>https://github.com/panlm/blog-private-api-gateway-dataflow</li> </ul>"},{"location":"#4-rendered","title":"4 rendered","text":"<p>{{ pagetree }}</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#awsaimltranscribe","title":"aws/aiml/transcribe","text":"<ul> <li>script-convert-mp3-to-text</li> </ul>"},{"location":"tags/#awsanalytics","title":"aws/analytics","text":"<ul> <li>mwaa-lab</li> </ul>"},{"location":"tags/#awsanalyticsgluedatabrew","title":"aws/analytics/glue/databrew","text":"<ul> <li>Stream EKS Control Panel Logs to S3</li> </ul>"},{"location":"tags/#awsanalyticskinesisfirehose","title":"aws/analytics/kinesis/firehose","text":"<ul> <li>cloudwatch-to-firehose-python</li> <li>Stream EKS Control Panel Logs to S3</li> </ul>"},{"location":"tags/#awschina","title":"aws/china","text":"<ul> <li>Create Public Access EKS Cluster in China Region</li> <li>Cross Region Reverse Proxy with NLB and Cloudfront</li> </ul>"},{"location":"tags/#awscloud9","title":"aws/cloud9","text":"<ul> <li>cloud9</li> <li>quick setup cloud9 script</li> <li>Setup Cloud9 for EKS</li> </ul>"},{"location":"tags/#awscmd","title":"aws/cmd","text":"<ul> <li>acm</li> <li>api-gateway</li> <li>ip-ranges</li> <li>cloud9</li> <li>cloudformation</li> <li>directory service</li> <li>ebs</li> <li>ec2</li> <li>ecr</li> <li>ecs</li> <li>efs</li> <li>iam</li> <li>lambda-cmd</li> <li>rds</li> <li>s3</li> <li>ssm</li> <li>vpc</li> <li>func-create-sg.sh</li> <li>eksdemo</li> <li>script-convert-mp3-to-text</li> </ul>"},{"location":"tags/#awscomputeec2","title":"aws/compute/ec2","text":"<ul> <li>ec2</li> <li>Rescue EC2 Instance</li> </ul>"},{"location":"tags/#awscontainerappmesh","title":"aws/container/appmesh","text":"<ul> <li>appmesh-workshop-eks</li> <li>automated-canary-deployment-using-flagger</li> </ul>"},{"location":"tags/#awscontainerecr","title":"aws/container/ecr","text":"<ul> <li>ecr</li> <li>Enable scan on push in ECR and send notification to SNS</li> </ul>"},{"location":"tags/#awscontainerecs","title":"aws/container/ecs","text":"<ul> <li>ecs</li> </ul>"},{"location":"tags/#awscontainereks","title":"aws/container/eks","text":"<ul> <li>eksctl</li> <li>eksdemo</li> <li>aws-for-fluent-bit</li> <li>aws-load-balancer-controller</li> <li>cert-manager</li> <li>cluster-autoscaler</li> <li>cni-metrics-helper</li> <li>ebs-for-eks</li> <li>efs-for-eks</li> <li>eks-addons-coredns</li> <li>eks-addons-kube-proxy</li> <li>eks-addons-vpc-cni</li> <li>eks-custom-network</li> <li>eks-fargate</li> <li>eksup</li> <li>enable-sg-on-pod</li> <li>karpenter-install-lab</li> <li>metrics-server</li> <li>nginx-ingress-controller-community-ver</li> <li>nginx-ingress-controller-nginx-ver</li> <li>EKS Addons</li> <li>Create EKS Cluster with Terraform</li> <li>Create Private Only EKS Cluster</li> <li>Create Public Access EKS Cluster in China Region</li> <li>Create Public Access EKS Cluster</li> <li>EKS Upgrade Procedure</li> <li>appmesh-workshop-eks</li> <li>automated-canary-deployment-using-flagger</li> <li>flux</li> <li>Stream EKS Control Panel Logs to S3</li> <li>Building Prometheus HA Architect with Thanos</li> <li>EKS Container Insights</li> <li>enable-prometheus-in-cloudwatch</li> <li>install-prometheus-grafana-on-eks</li> <li>quick setup cloud9 script</li> <li>Setup Cloud9 for EKS</li> <li>self-signed-certificates</li> </ul>"},{"location":"tags/#awscontainerfargate","title":"aws/container/fargate","text":"<ul> <li>eks-fargate</li> </ul>"},{"location":"tags/#awscontainerkarpenter","title":"aws/container/karpenter","text":"<ul> <li>karpenter-install-lab</li> </ul>"},{"location":"tags/#awsdatabaserds","title":"aws/database/rds","text":"<ul> <li>rds</li> <li>rds-mysql-replica-cross-region-cross-account</li> </ul>"},{"location":"tags/#awsdatabaseredshift","title":"aws/database/redshift","text":"<ul> <li>redshift</li> <li>redshift-data-api-lab</li> </ul>"},{"location":"tags/#awsintegrationsns","title":"aws/integration/sns","text":"<ul> <li>sns</li> <li>Enable scan on push in ECR and send notification to SNS</li> </ul>"},{"location":"tags/#awsintegrationsqs","title":"aws/integration/sqs","text":"<ul> <li>sqs</li> </ul>"},{"location":"tags/#awsmgmtcdk","title":"aws/mgmt/cdk","text":"<ul> <li>apigw-private-api-alb-cdk</li> </ul>"},{"location":"tags/#awsmgmtcloudformation","title":"aws/mgmt/cloudformation","text":"<ul> <li>cloudformation</li> <li>create standard vpc for lab in china region</li> </ul>"},{"location":"tags/#awsmgmtcloudwatch","title":"aws/mgmt/cloudwatch","text":"<ul> <li>cloudwatch</li> <li>cloudwatch-to-firehose-python</li> <li>Export Cloudwatch Log Group to S3</li> <li>EKS Container Insights</li> <li>enable-prometheus-in-cloudwatch</li> <li>create-dashboard-for-instance-cpu-matrics</li> </ul>"},{"location":"tags/#awsmgmtdirectory-service","title":"aws/mgmt/directory-service","text":"<ul> <li>directory service</li> <li>migrate-filezilla-to-transfer-family</li> </ul>"},{"location":"tags/#awsmgmtsystems-manager","title":"aws/mgmt/systems-manager","text":"<ul> <li>ssm</li> </ul>"},{"location":"tags/#awsnetwork","title":"aws/network","text":"<ul> <li>ip-ranges</li> </ul>"},{"location":"tags/#awsnetworkcloudfront","title":"aws/network/cloudfront","text":"<ul> <li>Cross Region Reverse Proxy with NLB and Cloudfront</li> </ul>"},{"location":"tags/#awsnetworknlb","title":"aws/network/nlb","text":"<ul> <li>Cross Region Reverse Proxy with NLB and Cloudfront</li> </ul>"},{"location":"tags/#awsnetworkroute53","title":"aws/network/route53","text":"<ul> <li>route53</li> <li>externaldns-for-route53</li> </ul>"},{"location":"tags/#awsnetworksecurity-group","title":"aws/network/security-group","text":"<ul> <li>enable-sg-on-pod</li> </ul>"},{"location":"tags/#awsnetworkvpc","title":"aws/network/vpc","text":"<ul> <li>vpc</li> <li>create standard vpc for lab in china region</li> </ul>"},{"location":"tags/#awssecurityacm","title":"aws/security/acm","text":"<ul> <li>acm</li> <li>self-signed-certificates</li> </ul>"},{"location":"tags/#awssecurityiam","title":"aws/security/iam","text":"<ul> <li>iam</li> <li>assume-tool</li> </ul>"},{"location":"tags/#awssecurityidentity-center","title":"aws/security/identity-center","text":"<ul> <li>Using Global SSO to Login China AWS Accounts</li> </ul>"},{"location":"tags/#awsserverlessapi-gateway","title":"aws/serverless/api-gateway","text":"<ul> <li>script-api-resource-method</li> <li>apigw-cross-account-private-endpoint</li> <li>Custom Domain Name in API Gateway</li> <li>Get Source IP in API Gateway</li> <li>apigw-private-api-alb-cdk</li> <li>apigw-regional-api-access-from-vpc</li> </ul>"},{"location":"tags/#awsserverlesslambda","title":"aws/serverless/lambda","text":"<ul> <li>lambda-cmd</li> <li>cloudwatch-to-firehose-python</li> </ul>"},{"location":"tags/#awsstorageebs","title":"aws/storage/ebs","text":"<ul> <li>ebs</li> <li>ebs-for-eks</li> </ul>"},{"location":"tags/#awsstorageefs","title":"aws/storage/efs","text":"<ul> <li>efs</li> <li>efs-for-eks</li> </ul>"},{"location":"tags/#awsstorages3","title":"aws/storage/s3","text":"<ul> <li>s3</li> <li>Export Cloudwatch Log Group to S3</li> <li>Stream EKS Control Panel Logs to S3</li> </ul>"},{"location":"tags/#awsstoragestorage-gateway","title":"aws/storage/storage-gateway","text":"<ul> <li>Storage File Gateway</li> </ul>"},{"location":"tags/#awsstoragetransfer-family","title":"aws/storage/transfer-family","text":"<ul> <li>migrate-filezilla-to-transfer-family</li> </ul>"},{"location":"tags/#bashfunction","title":"bash/function","text":"<ul> <li>func-create-sg.sh</li> </ul>"},{"location":"tags/#cmd","title":"cmd","text":"<ul> <li>api-gateway</li> <li>assume-tool</li> <li>docker</li> <li>iptables</li> <li>linux-cmd</li> </ul>"},{"location":"tags/#cmdjq","title":"cmd/jq","text":"<ul> <li>jq</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>docker</li> </ul>"},{"location":"tags/#flagger","title":"flagger","text":"<ul> <li>automated-canary-deployment-using-flagger</li> </ul>"},{"location":"tags/#github","title":"github","text":"<ul> <li>github-page-howto</li> </ul>"},{"location":"tags/#gitopsargo","title":"gitops/argo","text":"<ul> <li>argocd</li> </ul>"},{"location":"tags/#gitopsweaveworksflux","title":"gitops/weaveworks/flux","text":"<ul> <li>flux</li> </ul>"},{"location":"tags/#grafana","title":"grafana","text":"<ul> <li>Install Grafana on Beanstalk</li> <li>install-prometheus-grafana-on-eks</li> </ul>"},{"location":"tags/#grafanaloki","title":"grafana/loki","text":"<ul> <li>Using Loki for Logging</li> </ul>"},{"location":"tags/#kubernetes","title":"kubernetes","text":"<ul> <li>aws-for-fluent-bit</li> <li>cert-manager</li> <li>cluster-autoscaler</li> <li>eksup</li> <li>externaldns-for-route53</li> <li>kube-no-trouble</li> <li>kube-state-metrics</li> <li>metrics-server</li> <li>pluto</li> <li>EKS Addons</li> <li>horizontal pod autoscaler</li> <li>topology spread constraints</li> <li>Building Prometheus HA Architect with Thanos</li> </ul>"},{"location":"tags/#kubernetescni","title":"kubernetes/cni","text":"<ul> <li>eks-custom-network</li> </ul>"},{"location":"tags/#kubernetesingress","title":"kubernetes/ingress","text":"<ul> <li>aws-load-balancer-controller</li> <li>nginx-ingress-controller-community-ver</li> <li>nginx-ingress-controller-nginx-ver</li> <li>nginx-ingress-controller</li> </ul>"},{"location":"tags/#linux","title":"linux","text":"<ul> <li>docker</li> <li>iptables</li> <li>linux-cmd</li> </ul>"},{"location":"tags/#microsoftpowershell","title":"microsoft/powershell","text":"<ul> <li>powershell</li> </ul>"},{"location":"tags/#microsoftwindows","title":"microsoft/windows","text":"<ul> <li>powershell</li> </ul>"},{"location":"tags/#nginx","title":"nginx","text":"<ul> <li>nginx-ingress-controller-community-ver</li> <li>nginx-ingress-controller-nginx-ver</li> </ul>"},{"location":"tags/#prometheus","title":"prometheus","text":"<ul> <li>Building Prometheus HA Architect with Thanos</li> <li>enable-prometheus-in-cloudwatch</li> <li>install-prometheus-grafana-on-eks</li> <li>POC-prometheus-with-thanos-manually</li> </ul>"},{"location":"tags/#python","title":"python","text":"<ul> <li>jq</li> <li>cloudwatch-to-firehose-python</li> </ul>"},{"location":"tags/#terraform","title":"terraform","text":"<ul> <li>terraform</li> <li>Create EKS Cluster with Terraform</li> </ul>"},{"location":"tags/#todo","title":"todo","text":"<ul> <li>vpc</li> </ul>"},{"location":"CLI/awscli/acm-cmd/","title":"acm","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/cmd","aws/security/acm"]},{"location":"CLI/awscli/acm-cmd/#acm-cmd","title":"acm-cmd","text":"","tags":["aws/cmd","aws/security/acm"]},{"location":"CLI/awscli/acm-cmd/#create-certificate-with-eksdemo-","title":"create-certificate-with-eksdemo-","text":"<ul> <li>https://github.com/awslabs/eksdemo/blob/main/docs/create-acm-cert.md <pre><code>echo ${DOMAIN_NAME}\necho ${AWS_REGION}\n\neksdemo create acm-certificate \"*.${DOMAIN_NAME}\" --region ${AWS_REGION}\n# eksdemo get hosted-zone\n# eksdemo get dns-records -z poc1009.aws.panlm.xyz\n\neksdemo get acm-certificate # get certificate arn \n</code></pre> ^kresvp</li> </ul>","tags":["aws/cmd","aws/security/acm"]},{"location":"CLI/awscli/acm-cmd/#create-certificate-","title":"create-certificate-","text":"<ul> <li> <p>\u521b\u5efa\u5e76\u901a\u8fc7\u6dfb\u52a0 dns \u8bb0\u5f55\u9a8c\u8bc1\u8bc1\u4e66 (create certificate with DNS verification) <pre><code>echo ${DOMAIN_NAME}\n# DOMAIN_NAME=api0413.aws.panlm.xyz\n\nCERTIFICATE_ARN=$(aws acm request-certificate \\\n--domain-name \"*.${DOMAIN_NAME}\" \\\n--validation-method DNS \\\n--query 'CertificateArn' --output text)\n\nsleep 10\naws acm describe-certificate --certificate-arn ${CERTIFICATE_ARN} |tee /tmp/acm.$$.1\nCERT_CNAME_NAME=$(cat /tmp/acm.$$.1 |jq -r '.Certificate.DomainValidationOptions[0].ResourceRecord.Name')\nCERT_CNAME_VALUE=$(cat /tmp/acm.$$.1 |jq -r '.Certificate.DomainValidationOptions[0].ResourceRecord.Value')\n\nenvsubst &gt;certificate-route53-record.json &lt;&lt;-EOF\n{\n  \"Comment\": \"UPSERT a record for certificate xxx \",\n  \"Changes\": [\n    {\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"${CERT_CNAME_NAME}\",\n        \"Type\": \"CNAME\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [\n          {\n            \"Value\": \"${CERT_CNAME_VALUE}\"\n          }\n        ]\n      }\n    }\n  ]\n}\nEOF\n\nZONE_ID=$(aws route53 list-hosted-zones-by-name \\\n--dns-name \"${DOMAIN_NAME}.\" \\\n--query HostedZones[0].Id --output text) \naws route53 change-resource-record-sets \\\n--hosted-zone-id ${ZONE_ID} \\\n--change-batch file://certificate-route53-record.json \naws route53 list-resource-record-sets \\\n--hosted-zone-id ${ZONE_ID} \\\n--query \"ResourceRecordSets[?Name == '${CERT_CNAME_NAME}']\"\n</code></pre></p> </li> <li> <p>\u7b49\u5f85\u72b6\u6001\u8f6c\u53d8\u6210 SUCCESS (wait ValidationStatus to SUCCESS) <pre><code># wait ValidationStatus to SUCCESS\naws acm describe-certificate \\\n--certificate-arn ${CERTIFICATE_ARN} \\\n--query 'Certificate.DomainValidationOptions[0]' \n</code></pre></p> </li> </ul>","tags":["aws/cmd","aws/security/acm"]},{"location":"CLI/awscli/acm-cmd/#create-certificate-with-pca-cross-account","title":"create certificate with pca  cross account","text":"<pre><code>PCA_ARN=arn:aws:acm-pca:us-east-2:xxxxxx:certificate-authority/xxxxxx\naws acm request-certificate \\\n--domain-name '*.api0320.aws.panlm.xyz' \\\n--validation-method DNS \\\n--certificate-authority-arn ${PCA_ARN}\n</code></pre>","tags":["aws/cmd","aws/security/acm"]},{"location":"CLI/awscli/acm-cmd/#list-certificate-by-domain-name","title":"list certificate by domain name","text":"<pre><code>echo $DOMAIN_NAME\nCERTIFICATE_ARN=$(aws acm list-certificates --query 'CertificateSummaryList[?DomainName==`*.'\"${DOMAIN_NAME}\"'`].CertificateArn' --output text)\n</code></pre>","tags":["aws/cmd","aws/security/acm"]},{"location":"CLI/awscli/apigw-cmd/","title":"apigw-cmd","text":"","tags":["cmd","aws/cmd"]},{"location":"CLI/awscli/apigw-cmd/#delete-stage","title":"delete stage","text":"<pre><code>API_ID=3vnuyp4rtl\naws apigateway delete-stage \\\n--rest-api-id ${API_ID} \\\n--stage-name ip3\n</code></pre>","tags":["cmd","aws/cmd"]},{"location":"CLI/awscli/apigw-cmd/#create-apigw-role-","title":"create-apigw-role-","text":"<pre><code>ROLE_NAME=apigatewayrole-`date +%Y%m%d-%H%M`\necho '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": [\n                    \"apigateway.amazonaws.com\"\n                ]\n            },\n            \"Action\": [\n                \"sts:AssumeRole\"\n            ]\n        }\n    ]\n}' |tee role-trust-policy.json\naws iam create-role --role-name ${ROLE_NAME} \\\n  --assume-role-policy-document file://role-trust-policy.json\naws iam attach-role-policy --role-name ${ROLE_NAME} \\\n  --policy-arn \"arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs\"\naws iam list-attached-role-policies --role-name ${ROLE_NAME}\n\nrole_arn=$(aws iam get-role --role-name ${ROLE_NAME} |jq -r '.Role.Arn')\n\necho ${role_arn}\n</code></pre> <p>^0drt2e</p>","tags":["cmd","aws/cmd"]},{"location":"CLI/awscli/apigw-cmd/#attach-policy-to-api","title":"attach policy to api","text":"<p>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-create-attach.html#apigateway-resource-policies-create-attach-console</p> <pre><code>envsubst &gt;resource-policy.json &lt;&lt;-EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Deny\",\n      \"Principal\": \"*\",\n      \"Action\": \"execute-api:Invoke\",\n      \"Resource\": \"execute-api:/*/*/*\",\n      \"Condition\": {\n        \"StringNotEquals\": {\n          \"aws:sourceVpce\": [\"vpce-0941xxx828f\"]\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"execute-api:Invoke\",\n      \"Resource\": \"execute-api:/*/*/*\"\n    }\n  ]\n}\nEOF\nstr=$(cat resource-policy.json |sed 's/\"/\\\\\"/g' |xargs |sed 's/\"/\\\\\"/g')\naws apigateway update-rest-api --rest-api-id ${API_ID} --patch-operations op=replace,path=/policy,value='\"'\"$str\"'\"'\n\naws apigateway create-deployment \\\n--rest-api-id ${API_ID} --stage-name v1 \n</code></pre>","tags":["cmd","aws/cmd"]},{"location":"CLI/awscli/apigw-cmd/#create-api","title":"create api","text":"","tags":["cmd","aws/cmd"]},{"location":"CLI/awscli/apigw-cmd/#using-cli","title":"using cli","text":"<pre><code># create a simple api \n# will be replaced by poc api with vpc link\nAPI_ID=$(aws apigateway create-rest-api \\\n--name 'MyAPI-POC' \\\n--endpoint-configuration types=PRIVATE \\\n--query 'id' --output text)\n\nROOT_RESOURCE_ID=$(aws apigateway get-resources \\\n--rest-api-id ${API_ID} \\\n--query \"items[?path=='/'].id\" --output text)\n\nRESOURCE_ID_1=$(aws apigateway create-resource \\\n--rest-api-id ${API_ID} \\\n--parent-id ${ROOT_RESOURCE_ID} \\\n--path-part \"echo\" \\\n--query 'id' --output text)\n\naws apigateway put-method \\\n--rest-api-id ${API_ID} \\\n--resource-id ${RESOURCE_ID_1} \\\n--http-method GET \\\n--authorization-type NONE\n\naws apigateway put-integration \\\n--rest-api-id ${API_ID} \\\n--resource-id ${RESOURCE_ID_1} \\\n--http-method GET \\\n--type HTTP_PROXY \\\n--integration-http-method GET \\\n--uri 'https://httpbin.org' \n\n# --connection-type VPC_LINK \\\n# --connection-id ${VPCLINK_ID} \\\n# --tls-config insecureSkipVerification=true \\\n</code></pre>","tags":["cmd","aws/cmd"]},{"location":"CLI/awscli/apigw-cmd/#using-api-definition","title":"using api definition","text":"<ul> <li>refer: git/blog-private-api-gateway-dataflow/TC-private-apigw-dataflow</li> <li>refer: POC-apigw</li> </ul>","tags":["cmd","aws/cmd"]},{"location":"CLI/awscli/apigw-cmd/#update-access-log-for-rest-api-","title":"update-access-log-for-rest-api-","text":"<ul> <li>https://forum.serverless.com/t/how-to-setup-custom-access-logging-for-api-gateway-using-serverless/3288/5</li> </ul> <pre><code>echo ${API_ID}\necho $LOGGROUP_ARN\n\necho '{ \n    \"requestId\": \"$context.requestId\", \n    \"caller\": \"$context.identity.caller\", \n    \"user\": \"$context.identity.user\",\n    \"requestTime\": \"$context.requestTime\", \n    \"httpMethod\": \"$context.httpMethod\",\n    \"resourcePath\": \"$context.resourcePath\", \n    \"status\": \"$context.status\",\n    \"protocol\": \"$context.protocol\", \n    \"responseLength\": \"$context.responseLength\",\n    \"ip\": \"$context.identity.sourceIp\", \n    \"xff\": \"$context.requestOverride.header.xff\"\n}' |tee access-log-format.json\n\nformat_str=$(cat access-log-format.json |sed 's/\"/\\\\\"/g' |xargs |sed 's/\"/\\\\\"/g')\n\necho '{\"patchOperations\": []}' |\\\njq '.patchOperations[0] = {\"op\": \"replace\", \"path\": \"/accessLogSettings/format\", \"value\": \"'\"${format_str}\"'\"}' |\\\njq '.patchOperations[1] = {\"op\": \"replace\", \"path\": \"/accessLogSettings/destinationArn\", \"value\": \"'\"${LOGGROUP_ARN}\"'\"}' |tee access-log-settings.json\n\naws apigateway update-stage \\\n--rest-api-id $API_ID \\\n--stage-name v1 \\\n--cli-input-json file://access-log-settings.json\n</code></pre> <p>refer: git/blog-private-api-gateway-dataflow/TC-private-apigw-dataflow</p>","tags":["cmd","aws/cmd"]},{"location":"CLI/awscli/aws-ip-range/","title":"ip-ranges","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/network","aws/cmd"]},{"location":"CLI/awscli/aws-ip-range/#ip-range","title":"ip-range","text":"","tags":["aws/network","aws/cmd"]},{"location":"CLI/awscli/aws-ip-range/#blog","title":"blog","text":"<ul> <li>How to Automatically Update Your Security Groups for Amazon CloudFront and AWS WAF by Using AWS Lambda <ul> <li>(link) (github)</li> </ul> </li> </ul>","tags":["aws/network","aws/cmd"]},{"location":"CLI/awscli/aws-ip-range/#download","title":"download","text":"<pre><code>wget https://ip-ranges.amazonaws.com/ip-ranges.json\n</code></pre>","tags":["aws/network","aws/cmd"]},{"location":"CLI/awscli/aws-ip-range/#cli","title":"CLI","text":"<ul> <li> <p>s3 <pre><code>cat ip-ranges.json |jq -r '.prefixes[] | select(.region==\"cn-northwest-1\") | select(.service==\"S3\")'\n</code></pre></p> </li> <li> <p>dynamodb <pre><code>cat ip-ranges.json |jq -r '.prefixes[] | select(.region==\"cn-northwest-1\") | select(.service==\"DYNAMODB\")'\n</code></pre></p> </li> <li> <p>lambda <pre><code>cat ip-ranges.json |jq -r '.prefixes[] | select(.region==\"cn-northwest-1\") | select(.service==\"LAMBDA\")'\n</code></pre></p> </li> <li> <p>ec2 <pre><code>cat ip-ranges.json |jq -r '.prefixes[] | select(.region==\"cn-northwest-1\") | select(.service==\"EC2\")'\n</code></pre></p> </li> <li> <p>eks <pre><code>cat ip-ranges.json |jq -r '.prefixes[] | select(.region==\"cn-northwest-1\") | select(.service==\"EKS\")'\n</code></pre></p> </li> <li> <p>route 53 <pre><code>cat ip-ranges.json |jq -r '.prefixes[]  | select(.service==\"ROUTE53\")'\n\ncat ip-ranges.json |jq -r '.prefixes[]  | select(.region==\"us-east-2\") | select(.service==\"ROUTE53_RESOLVER\")'\n</code></pre></p> </li> <li> <p>api gateway <pre><code>cat ip-ranges.json |jq -r '.prefixes[] | select(.region==\"us-east-2\") | select(.service==\"API_GATEWAY\")'\n</code></pre></p> </li> </ul>","tags":["aws/network","aws/cmd"]},{"location":"CLI/awscli/cloud9-cmd/","title":"cloud9","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/cloud9","aws/cmd"]},{"location":"CLI/awscli/cloud9-cmd/#cloud9-cmd","title":"cloud9-cmd","text":"","tags":["aws/cloud9","aws/cmd"]},{"location":"CLI/awscli/cloud9-cmd/#spin-up-a-cloud9-instance-in-your-region","title":"spin-up-a-cloud9-instance-in-your-region","text":"<ul> <li>refer: ../../cloud9/setup-cloud9-for-eks </li> </ul> <pre><code>aws cloud9 describe-environments\n</code></pre>","tags":["aws/cloud9","aws/cmd"]},{"location":"CLI/awscli/cloud9-cmd/#disable-aws-credential-management","title":"disable aws credential management","text":"<pre><code>aws cloud9 update-environment  --environment-id $C9_PID --managed-credentials-action DISABLE\nrm -vf ${HOME}/.aws/credentials\n</code></pre>","tags":["aws/cloud9","aws/cmd"]},{"location":"CLI/awscli/cloud9-cmd/#get-subnet-id-and-vpc-id-from-cloud9-instance","title":"get subnet id and vpc id from cloud9 instance","text":"<pre><code># get cloud9 vpc\nC9_INST_ID=$(curl http://169.254.169.254/1.0/meta-data/instance-id 2&gt;/dev/null)\nC9_VPC_ID=$(aws ec2 describe-instances \\\n--instance-ids ${C9_INST_ID} \\\n--query 'Reservations[0].Instances[0].VpcId' --output text)\n\n# get public subnet for external alb\nC9_SUBNETS_ID=$(aws ec2 describe-subnets \\\n--filter \"Name=vpc-id,Values=${C9_VPC_ID}\" \\\n--query 'Subnets[?MapPublicIpOnLaunch==`true`].SubnetId' \\\n--output text)\n\n# get default security group \nC9_DEFAULT_SG_ID=$(aws ec2 describe-security-groups \\\n--filter Name=vpc-id,Values=${C9_VPC_ID} \\\n--query \"SecurityGroups[?GroupName == 'default'].GroupId\" \\\n--output text)\n</code></pre> <p>^wxvp2s</p>","tags":["aws/cloud9","aws/cmd"]},{"location":"CLI/awscli/cloud9-cmd/#share-cloud9-to-others","title":"share cloud9 to others","text":"<pre><code>C9_ID=\naws cloud9 create-environment-membership \\\n    --environment-id ${C9_ID} \\\n    --user-arn arn:aws:sts::${MY_ACCOUNT_ID}:user/panlm \\\n    --permissions read-write\n</code></pre> <pre><code>Value 'arn:aws:sts:::user/panlm' at 'userArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:(aws|aws-cn|aws-us-gov|aws-iso|aws-iso-b):(iam|sts)::\\d+:(root|(user\\/[\\w+=/:,.@-]{1,64}|federated-user\\/[\\w+=/:,.@-]{2,32}|assumed-role\\/[\\w+=:,.@-]{1,64}\\/[\\w+=,.@-]{1,64}))$\n</code></pre>","tags":["aws/cloud9","aws/cmd"]},{"location":"CLI/awscli/cloud9-cmd/#old","title":"old","text":"<pre><code>OWNER_ARN=$(aws sts get-caller-identity  --query 'Arn'  --output text)\nENV_ID=$(aws cloud9 create-environment-ec2 \\\n--name ${STACK_NAME} \\\n--instance-type t3.small \\\n--subnet-id ${PublicSubnet1ID} \\\n--automatic-stop-time-minutes 10080 \\\n--owner-arn ${OWNER_ARN} \\\n--query 'environmentId' --output text )\n\n(C9_URL=https://${AWS_REGION}.console.aws.amazon.com/cloud9/ide/${ENV_ID}\necho \"open cloud9 url:\"\necho \"${C9_URL}\")\n</code></pre> <pre><code>DEFAULT_VPC=$(aws ec2 describe-vpcs --filter Name=is-default,Values=true --query 'Vpcs[0].VpcId' --output text)\n\nif [[ ! -z ${DEFAULT_VPC} ]]; then\n\n  FIRST_SUBNET=$(aws ec2 describe-subnets \\\n    --filters \"Name=vpc-id,Values=${DEFAULT_VPC}\" \\\n    --query \"Subnets[0].SubnetId\" \\\n    --output text)\n\n  aws cloud9 create-environment-ec2 \\\n    --name cloud9-$RANDOM \\\n    --instance-type t3.small \\\n    --subnet-id ${FIRST_SUBNET} \\\n    --automatic-stop-time-minutes 10080 \\\n    --owner-arn arn:aws:iam::123456789012:role/adminrole \\\n\nfi\n\nAn error occurred (ValidationException) when calling the CreateEnvironmentEC2 operation: 1 validation error detected: Value 'arn:aws:iam::861xxxxxx173:role/adminrole' at 'ownerArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:(aws|aws-cn|aws-us-gov|aws-iso|aws-iso-b):(iam|sts)::\\d+:(root|(user\\/[\\w+=/:,.@-]{1,64}|federated-user\\/[\\w+=/:,.@-]{2,32}|assumed-role\\/[\\w+=:,.@-]{1,64}\\/[\\w+=,.@-]{1,64}))$\n</code></pre>","tags":["aws/cloud9","aws/cmd"]},{"location":"CLI/awscli/cloud9-cmd/#turn-off-aws-managed-temporary-credentials","title":"Turn off AWS managed temporary credentials","text":"<p>LINK</p> <p>If you turn off AWS managed temporary credentials, by default the environment cannot access any AWS services, regardless of the AWS entity who makes the request. If you can\u2019t or don\u2019t want to turn on AWS managed temporary credentials for an environment, but you still need the environment to access AWS services, consider the following alternatives:</p> <ul> <li>Attach an instance profile to the Amazon EC2 instance that connects to the environment. For instructions, see\u00a0Create and Use an Instance Profile to Manage Temporary Credentials.</li> <li>Store your permanent AWS access credentials in the environment, for example, by setting special environment variables or by running the\u00a0<code>aws configure</code>\u00a0command. For instructions, see\u00a0Create and store permanent access credentials in an Environment.</li> </ul>","tags":["aws/cloud9","aws/cmd"]},{"location":"CLI/awscli/cloudformation-cmd/","title":"cloudformation","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["aws/mgmt/cloudformation","aws/cmd"]},{"location":"CLI/awscli/cloudformation-cmd/#cloudformation-cli","title":"cloudformation-cli","text":"","tags":["aws/mgmt/cloudformation","aws/cmd"]},{"location":"CLI/awscli/cloudformation-cmd/#describe-stacks","title":"describe-stacks","text":"<pre><code>AWS_REGION=us-east-1\nSTACK_NAME=stack1-23948\n\naws cloudformation --region ${AWS_REGION} describe-stacks --stack-name ${STACK_NAME} --query 'Stacks[0].Outputs[?OutputKey==`VPCID`].OutputValue' --output text\n\naws cloudformation --region ${AWS_REGION} describe-stacks --stack-name ${STACK_NAME} --query 'Stacks[0].StackStatus' --output text\n</code></pre>","tags":["aws/mgmt/cloudformation","aws/cmd"]},{"location":"CLI/awscli/cloudformation-cmd/#create-stack","title":"create-stack","text":"<pre><code>aws cloudformation create-stack --stack-name OELabStack2 \\\n  --parameters ParameterKey=InstanceProfile,ParameterValue=\"\" \\\n               ParameterKey=KeyName,ParameterValue=sample-key \\\n               ParameterKey=WorkloadName,ParameterValue=Test \\\n  --tags Key=env,Value=stack2 \\\n  --template-body file://~/Downloads/OE_Inventory_and_Patch_Mgmt.json\n\n{\n    \"StackId\": \"arn:aws-cn:cloudformation:cn-north-1:24xxxxx11488:stack/OELabStack1/64469510-5339-11ea-8854-022274580dba\"\n}\n</code></pre>","tags":["aws/mgmt/cloudformation","aws/cmd"]},{"location":"CLI/awscli/cloudwatch-cmd/","title":"cloudwatch","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#cloudwatch-cmd","title":"cloudwatch-cmd","text":"","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#log-group-and-log-stream","title":"log group and log stream","text":"","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#create-log-group","title":"create log group","text":"<ul> <li> <p>create standard log group <pre><code>LOGGROUP_NAME=apigw-access-log\naws logs create-log-group \\\n--log-group-name ${LOGGROUP_NAME}\nLOGGROUP_ARN=$(aws logs describe-log-groups \\\n--log-group-name-prefix ${LOGGROUP_NAME} \\\n--query 'logGroups[0].arn' --output text)\nLOGGROUP_ARN=${LOGGROUP_ARN%:*}\n</code></pre></p> </li> <li> <p>create IA log group <pre><code>LOGGROUP_NAME=/aws/eks/ekscluster3/cluster\naws logs delete-log-group --log-group-name ${LOGGROUP_NAME} &amp;&amp; \\\naws logs create-log-group \\\n--log-group-name ${LOGGROUP_NAME} \\\n--log-group-class INFREQUENT_ACCESS\n</code></pre></p> </li> </ul>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#describe-log-stream","title":"describe log stream","text":"<pre><code>aws logs describe-log-streams \\\n  --log-group-name /aws/eks/ekscluster1/cluster \\\n  --log-stream-name-prefix kube-apiserver-audit- \\\n  |jq -r '.logStreams[] | (.creationTime, .logStreamName )' \\\n  |xargs -n 2 |sort -r |sed -n '2,$p' |awk '{print $NF}'\n</code></pre>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#delete-log-stream","title":"delete log stream","text":"<pre><code>aws logs delete-log-stream \\\n  --log-group-name /aws/eks/ekscluster1/cluster \\\n  --log-stream-name kube-apiserver-audit-c26edac46f343347e73694744d70ab2a\n</code></pre>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#check-log-size","title":"check log size","text":"<p>IncomingBytes refer: Which Log Group is causing a sudden increase in my CloudWatch Logs bill?</p> <pre><code>aws cloudwatch get-metric-statistics \\\n  --metric-name IncomingBytes \\\n  --start-time 2022-08-13T00:00:00Z --end-time 2022-08-18T23:59:59Z \\\n  --period 2592000 \\\n  --namespace AWS/Logs --statistics Sum --region us-east-2\n</code></pre> <pre><code># period = 30 days\naws cloudwatch get-metric-statistics \\\n  --metric-name IncomingBytes \\\n  --start-time 2022-08-13T00:00:00Z --end-time 2022-08-18T23:59:59Z \\\n  --period 2592000 \\\n  --namespace AWS/Logs --statistics Sum --region us-east-2 \\\n  --dimensions Name=LogGroupName,Value=/aws/eks/ekscluster1/cluster\n</code></pre>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#export-task","title":"export task","text":"<ul> <li>../../EKS/solutions/logging/export-cloudwatch-log-group-to-s3</li> </ul>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#subscription-firehose","title":"subscription firehose","text":"<ul> <li>../../EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3</li> </ul>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#log-insights","title":"log-insights","text":"<ul> <li>cloudwatch-logs-insights</li> </ul>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#metric","title":"metric","text":"<pre><code>SELECT AVG(WriteIOPS) FROM SCHEMA(\"AWS/ES\", ClientId,DomainName,NodeId) WHERE DomainName = 'myaos-20221210-130610' GROUP BY NodeId, DomainName\n\n{\n    \"metrics\": [\n        [ { \"expression\": \"SEARCH('{AWS/ES,ClientId,DomainName,NodeId} MetricName=ReadIOPS', 'Average', 300)\", \"id\": \"e1\", \"period\": 300 } ],\n        [ { \"expression\": \"SEARCH('{AWS/ES,ClientId,DomainName,NodeId} MetricName=WriteIOPS', 'Average', 300)\", \"id\": \"e2\", \"period\": 300 } ]\n    ],\n    \"view\": \"timeSeries\",\n    \"stacked\": false,\n    \"region\": \"us-east-2\",\n    \"stat\": \"Average\",\n    \"period\": 300\n}\n</code></pre>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/cloudwatch-cmd/#add-alarm","title":"add alarm","text":"<pre><code>account_id=2086xxxx7602\nopensearch_name=opensearch-uez6sk9a\n\naws cloudwatch put-metric-alarm \\\n--alarm-name ClusterStatus-red-abcd \\\n--evaluation-periods 5 \\\n--comparison-operator GreaterThanOrEqualToThreshold \\\n--alarm-description \"OS cluster status red greater than 1 minute\" \\\n--metric-name ClusterStatus.red \\\n--namespace AWS/ES \\\n--statistic Average \\\n--period 60 \\\n--threshold 1 \\\n--treat-missing-data missing \\\n--dimensions Name=ClientId,Value=${account_id} Name=DomainName,Value=${opensearch_name}\n</code></pre>","tags":["aws/mgmt/cloudwatch"]},{"location":"CLI/awscli/directory-service-cmd/","title":"directory service","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/mgmt/directory-service","aws/cmd"]},{"location":"CLI/awscli/directory-service-cmd/#directory-service-cmd","title":"directory-service-cmd","text":"","tags":["aws/mgmt/directory-service","aws/cmd"]},{"location":"CLI/awscli/directory-service-cmd/#create-ms-ad","title":"create ms ad","text":"<pre><code>AD=corp1.aws.panlm.xyz\nPASS=passworD.1\nVPC=vpc-0946\nSUBNETS=subnet-056f,subnet-033c\n\naws ds create-microsoft-ad \\\n--name ${AD} \\\n--short-name ${AD%%.*} \\\n--password ${PASS} \\\n--edition Standard \\\n--vpc-settings VpcId=${VPC},SubnetIds=${SUBNETS}\n</code></pre>","tags":["aws/mgmt/directory-service","aws/cmd"]},{"location":"CLI/awscli/directory-service-cmd/#create-ms-ad-in-default-vpc","title":"create ms ad in default vpc","text":"right-click &amp; open-in-new-tab","tags":["aws/mgmt/directory-service","aws/cmd"]},{"location":"CLI/awscli/directory-service-cmd/#active-directory-windows-2012","title":"active directory - windows 2012","text":"","tags":["aws/mgmt/directory-service","aws/cmd"]},{"location":"CLI/awscli/ebs-cmd/","title":"ebs","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/cmd","aws/storage/ebs"]},{"location":"CLI/awscli/ebs-cmd/#ebs-cmd","title":"ebs-cmd","text":"","tags":["aws/cmd","aws/storage/ebs"]},{"location":"CLI/awscli/ebs-cmd/#create-ebs-volume","title":"create ebs volume","text":"<pre><code>aws ec2 attach-volume\n--device &lt;value&gt;\n--instance-id &lt;value&gt;\n--volume-id &lt;value&gt;\n\naws ec2 create-volume\n--availability-zone &lt;value&gt;\n--size 200\n</code></pre>","tags":["aws/cmd","aws/storage/ebs"]},{"location":"CLI/awscli/ebs-cmd/#change-ebs-gp2-to-gp3","title":"change ebs gp2 to gp3","text":"<ul> <li>Script to automatically change all gp2 volumes to gp3 with aws-cli <pre><code>#! /bin/bash\n\nregion='us-east-1'\n\n# Find all gp2 volumes within the given region\nvolume_ids=$(/usr/bin/aws ec2 describe-volumes --region \"${region}\" --filters Name=volume-type,Values=gp2 | jq -r '.Volumes[].VolumeId')\n\n# Iterate all gp2 volumes and change its type to gp3\nfor volume_id in ${volume_ids};do\n    result=$(/usr/bin/aws ec2 modify-volume --region \"${region}\" --volume-type=gp3 --volume-id \"${volume_id}\" | jq '.VolumeModification.ModificationState' | sed 's/\"//g')\n    if [ $? -eq 0 ] &amp;&amp; [ \"${result}\" == \"modifying\" ];then\n        echo \"OK: volume ${volume_id} changed to state 'modifying'\"\n    else\n        echo \"ERROR: couldn't change volume ${volume_id} type to gp3!\"\n    fi\ndone\n</code></pre></li> </ul>","tags":["aws/cmd","aws/storage/ebs"]},{"location":"CLI/awscli/ebs-cmd/#get-each-snapshot-change-blocks-","title":"get-each-snapshot-change-blocks-","text":"<p>\u4f7f\u7528 ebs \u5feb\u7167\u751f\u547d\u5468\u671f\u7ba1\u7406\u65f6\uff0c\u67e5\u770b\u7279\u5b9a volume \u6bcf\u6b21\u5feb\u7167\u5360\u7528\u7684\u5927\u5c0f\uff0c\u4ee5\u65b9\u4fbf\u8de8\u533a\u57df\u590d\u5236\u65f6\u9884\u4f30\u4f20\u8f93\u91cf\u3002</p> <ul> <li>\u5feb\u7167\u5b8c\u6210\u8de8\u533a\u57df\u590d\u5236\u8017\u65f6\u4e0d\u7b49\uff0c\u4ece 20+ \u5206\u949f\u5230 45 \u5206\u949f\uff0c\u4e0d\u9002\u5408\u4f5c\u4e3a\u5bb9\u707e\u5207\u6362\u4f7f\u7528</li> <li>\u4f46\u53ef\u4ee5\u52a0\u901f\u540e\u7eed\u590d\u5236\u5b8c\u6210\uff0c\u9700\u8981\u6ce8\u610f\u4fdd\u6301\u914d\u7f6e\u4e00\u76f4\u624d\u53ef\u4ee5\u91cd\u7528\u4e4b\u524d\u590d\u5236\u7684\u5185\u5bb9\uff0c\u4f8b\u5982\u4e4b\u524d\u8de8\u533a\u57df\u590d\u5236\u65f6\u4f7f\u7528kms\uff0c\u90a3\u4e48\u5728\u540e\u7eed\u624b\u5de5\u5feb\u7167+\u590d\u5236\u8fc7\u7a0b\u4e2d\u4e5f\u8981\u9009\u62e9kms\uff0c\u53ef\u4ee5\u5feb\u901f\u5b8c\u6210\uff0c\u5426\u5219\u8017\u65f6\u66f4\u591a</li> </ul> <pre><code>VOLUME_ID=vol-06ecbace881bf641a\naws ec2 describe-snapshots  \\\n  |jq -r '(\n  .Snapshots[] \n  | select (.VolumeId==\"'\"${VOLUME_ID}\"'\") \n  | [.SnapshotId, .StartTime])\n  |@tsv' |sort -k2 &gt;/tmp/$$.tsv\n\nLINE_NUM=$(wc -l /tmp/$$.tsv |awk '{print $1}')\nif [[ ${LINE_NUM} -ne 1 ]]; then \n  cat /tmp/$$.tsv |awk '{print $1}' |while read LINE ; do\n    FIRST=${SECOND}\n    SECOND=${LINE}\n    if [[ -z ${FIRST} ]]; then\n      continue\n    fi\n    # echo $FIRST \"-\" $SECOND\n    echo -e \"${FIRST} - ${SECOND}: \\c\"\n    aws ebs list-changed-blocks \\\n      --first-snapshot-id ${FIRST} \\\n      --second-snapshot-id ${SECOND} \\\n      |jq -r '.ChangedBlocks|length'\n  done\nfi\n</code></pre>","tags":["aws/cmd","aws/storage/ebs"]},{"location":"CLI/awscli/ebs-cmd/#dlm-snapshot","title":"dlm - snapshot","text":"<ul> <li>default role doc (link) <pre><code># AWSDataLifecycleManagerDefaultRole\naws dlm create-default-role --resource-type snapshot\n# AWSDataLifecycleManagerDefaultRoleForAMIManagement\naws dlm create-default-role --resource-type image\n\nROLE_NAME=AWSDataLifecycleManagerDefaultRole\nROLE_ARN=$(aws iam get-role \\\n--role-name ${ROLE_NAME} \\\n--query 'Role.Arn' --output text)\n\ncat &gt; $$-snapshot.json &lt;&lt;-EOF\n{\n  \"PolicyType\": \"EBS_SNAPSHOT_MANAGEMENT\",\n  \"ResourceTypes\": [\n    \"INSTANCE\"\n  ],\n  \"TargetTags\": [\n    {\n      \"Key\": \"Name\",\n      \"Value\": \"test-instance1-zhy\"\n    }\n  ],\n  \"VariableTags\": [\n    {\n      \"Key\": \"instance-id\",\n      \"Value\": \"$(instance-id)\"\n    },\n    {\n      \"Key\": \"timestamp\",\n      \"Value\": \"$(timestamp)\"\n    }\n  ],\n  \"Schedules\": [\n    {\n      \"Name\": \"Schedule 1\",\n      \"CopyTags\": true,\n      \"TagsToAdd\": [\n        {\n          \"Key\": \"Project\",\n          \"Value\": \"DR Prep\"\n        }\n      ],\n      \"CreateRule\": {\n        \"Interval\": 1,\n        \"IntervalUnit\": \"HOURS\",\n        \"Times\": [\n          \"08:00\"\n        ]\n      },\n      \"RetainRule\": {\n        \"Count\": 6\n      },\n      \"CrossRegionCopyRules\": [\n        {\n          \"TargetRegion\": \"cn-north-1\",\n          \"Encrypted\": false,\n          \"CopyTags\": true,\n          \"RetainRule\": {\n            \"Interval\": 1,\n            \"IntervalUnit\": \"DAYS\"\n          }\n        }\n      ]\n    }\n  ]\n}\nEOF\n\naws dlm create-lifecycle-policy \\\n    --description \"My second policy\" \\\n    --state ENABLED \\\n    --execution-role-arn ${ROLE_ARN} \\\n    --policy-details file://$$-snapshot.json\n</code></pre></li> </ul>","tags":["aws/cmd","aws/storage/ebs"]},{"location":"CLI/awscli/ebs-cmd/#dlm-ami-policy-sample","title":"dlm - ami - policy sample","text":"<pre><code>{\n  \"PolicyType\": \"IMAGE_MANAGEMENT\",\n  \"ResourceTypes\": [\n    \"INSTANCE\"\n  ],\n  \"TargetTags\": [\n    {\n      \"Key\": \"Name\",\n      \"Value\": \"test-instance2-zhy\"\n    }\n  ],\n  \"Schedules\": [\n    {\n      \"Name\": \"Schedule 1\",\n      \"CopyTags\": true,\n      \"VariableTags\": [\n        {\n          \"Key\": \"instance-id\",\n          \"Value\": \"$(instance-id)\"\n        }\n      ],\n      \"CreateRule\": {\n        \"Interval\": 1,\n        \"IntervalUnit\": \"HOURS\",\n        \"Times\": [\n          \"02:00\"\n        ]\n      },\n      \"RetainRule\": {\n        \"Count\": 6\n      },\n      \"CrossRegionCopyRules\": [\n        {\n          \"TargetRegion\": \"cn-north-1\",\n          \"Encrypted\": false,\n          \"CopyTags\": true,\n          \"RetainRule\": {\n            \"Interval\": 1,\n            \"IntervalUnit\": \"DAYS\"\n          },\n          \"DeprecateRule\": {\n            \"Interval\": 1,\n            \"IntervalUnit\": \"DAYS\"\n          }\n        }\n      ]\n    }\n  ]\n}\n</code></pre>","tags":["aws/cmd","aws/storage/ebs"]},{"location":"CLI/awscli/ebs-cmd/#tools","title":"tools","text":"<ul> <li>list nvme volume script</li> <li>list volume script</li> </ul> <p>download ebsnvme-id.zip </p> <ul> <li>doc</li> </ul> right-click &amp; open-in-new-tab <p></p>","tags":["aws/cmd","aws/storage/ebs"]},{"location":"CLI/awscli/ebs-cmd/#refer","title":"refer","text":"<p>https://github.com/awslabs/amazon-ebs-autoscale</p>","tags":["aws/cmd","aws/storage/ebs"]},{"location":"CLI/awscli/ec2-cmd/","title":"ec2","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#ec2-cmd","title":"ec2 cmd","text":"","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#others","title":"others","text":"<ul> <li>ebs-cmd</li> </ul>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#get-image-id","title":"get image id","text":"","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#get-all-ubuntu-image-from-here-click-to-launch","title":"get all ubuntu image from here (click to launch)","text":"<ul> <li>https://cloud-images.ubuntu.com/locator/ec2/</li> </ul>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#option-1","title":"option 1","text":"<ul> <li>https://aws.amazon.com/blogs/compute/query-for-the-latest-amazon-linux-ami-ids-using-aws-systems-manager-parameter-store/</li> </ul> <pre><code>AWS_REGION=us-east-2\naws ssm get-parameters --names /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 --region ${AWS_REGION}\n</code></pre> <p>2022.6.27 us-east-1: ami-065efef2c739d613b us-east-2: ami-07251f912d2a831a3</p>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#option-2","title":"option 2","text":"<pre><code># ubuntu\nexport region=ap-southeast-1\naws ec2 describe-images --region ${region} --owners 099720109477 \\\n  --filters Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64*  \\\n  --query 'Images[*].[ImageId,CreationDate,Name]' --output text |sort -k2 -r |column -t\n\n# windows 2012\nexport region=ap-southeast-1\naws ec2 describe-images --region ${region}  --owners 801119661308 \\\n  --filter \"Name=name,Values=Windows_Server-2012-R2_RTM-English-64Bit-Base*\"  \\\n  --query 'Images[*].[ImageId,CreationDate,Name]' --output text |sort -k2 -r |column -t\n\n# windows 2012\nexport AWS_DEFAULT_REGION=us-east-2\naws ec2 describe-images --owners 801119661308 \\\n  --filter \"Name=name,Values=Windows_Server-2019-English-Full-Base*\"  \\\n  --query 'Images[*].[ImageId,CreationDate,Name]' --output text |sort -k2 -r |column -t\n\n# amzn2\nexport region=ap-southeast-1\naws ec2 describe-images --region ${region} --owners 137112412989 \\\n  --filters Name=name,Values=amzn2-ami-hvm-*2021*gp2*  \\\n  --query 'Images[*].[ImageId,CreationDate,Name]' --output text |sort -k2 -r |column -t\n\n# centos\nexport region=ap-southeast-1\naws ec2 describe-images --region ${region} --owners 679593333241 \\\n  --filters Name=name,Values='CentOS Linux 7 x86_64*'  \\\n  --query 'Images[*].[ImageId,CreationDate,Name]' --output text \\\n  |sort -k2 -r |column -t\n\nexport region=cn-northwest-1\nawscn ec2 describe-images --region ${region} --owners 336777782633 \\\n  --filters Name=name,Values='CentOS-7*'  \\\n  --query 'Images[*].[ImageId,CreationDate,Name]' --output text \\\n  |sort -k2 -r |column -t\n\n# centos ami\n# https://wiki.centos.org/Cloud/AWS\n# aws --region us-east-1 ec2 describe-images --owners aws-marketplace --filters Name=product-code,Values=cvugziknvmxgqna9noibqnnsy\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#get-cloud9-newest-image","title":"get cloud9 newest image","text":"<pre><code>export AWS_DEFAULT_REGION=us-east-2\naws ec2 describe-images --region ${AWS_DEFAULT_REGION} --owners amazon \\\n  --filters \"Name=name,Values=Cloud9AmazonLinux2-*\" \\\n  --query 'reverse(sort_by(Images, &amp;CreationDate)[].[Name,ImageId])' \\\n  --output text |column -t \n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#create-instance","title":"create instance","text":"<pre><code># IMAGE_ID=ami-026bd3163cafd87ed #ubuntu\nIMAGE_ID=ami-0f511ead81ccde020 #amzn2 ap-southeast-1\nIMAGE_ID=ami-028584814b5504f5b #amzn2 cn-northwest-1\nIMAGE_ID=ami-01b887d5e264569f5 #amzn2 cn-north-1\nregion=cn-north-1\nKEY_NAME=awskey\naws ec2 run-instances --region ${region} --key-name $KEY_NAME \\\n  --image-id $IMAGE_ID --instance-type c5.large --query Instances[*].InstanceId --output text\n\nSUBNET_ID=\naws ec2 run-instances --region ${region} \\\n  --image-id $IMAGE_ID --instance-type c5.large \\\n  --subnet-id ${SUBNET_ID} --query Instances[*].InstanceId --output text\n</code></pre> <pre><code># centos\nIMAGE_ID=ami-07f65177cb990d65b\nAWS_REGION=ap-southeast-1\nKEY_NAME=aws-key\necho '#!/bin/bash\nsudo yum install -y https://s3.'\"$AWS_REGION\"'.amazonaws.com/amazon-ssm-'\"$region\"'/latest/linux_amd64/amazon-ssm-agent.rpm' |tee /tmp/tmp_$$.txt\naws ec2 run-instances --region ${AWS_REGION} --key-name $KEY_NAME \\\n  --image-id $IMAGE_ID --instance-type t2.micro \\\n  --user-data file:///tmp/tmp_$$.txt\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#windows-instance","title":"windows instance","text":"<pre><code># windows 2016 base\nIMAGE_ID=ami-02c88710773712fea\nAWS_REGION=us-east-2\n# INSTANCE_PROFILE_ARN=arn:aws:iam::123456789012:instance-profile/windows-instance\n\n# windows 2016 base in china region\nIMAGE_ID=ami-0cdfdbad775669b71\nAWS_REGION=cn-northwest-1\nINSTANCE_PROFILE_ARN=arn:aws-cn:iam::123456789012:instance-profile/windows-instance\n\nKEY_NAME=awskey\nSTR=$(date +%H%M)\naws ec2 run-instances \\\n    --region ${AWS_REGION} --key-name ${KEY_NAME} \\\n    --image-id ${IMAGE_ID} --instance-type m5.large \\\n    --iam-instance-profile Arn=${INSTANCE_PROFILE_ARN} \\\n    --private-dns-name-options \"HostnameType=ip-name,EnableResourceNameDnsARecord=true,EnableResourceNameDnsAAAARecord=false\"\n    --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=win-'\"${STR}\"'},{Key=os,Value=windows}]' |tee /tmp/instance-$$.1\nINST_ID=$(cat /tmp/instance-$$.1 |jq -r '.Instances[0].InstanceId')\n# private dns name option is important for join domain\n# false/false will run ssm document failed\n# true/false will run ssm document successful\n</code></pre> <p>another example:</p> <ul> <li>../../others/POC-mig-filezilla-to-transfer-family</li> </ul>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#list-instance","title":"list instance","text":"<pre><code># using Name to filter\naws ec2 describe-instances |jq -r '.Reservations[].Instances[] | select((.Tags[]|select(.Key==\"Name\")|.Value) | match(\".*\") ) | [.InstanceId, (del((.Tags[]|select(.Key!=\"Name\")))|.Tags[]|.Value|tostring)]|@tsv'\n\n# list all instance, name, ips\naws ec2 describe-instances |jq -r '.Reservations[].Instances[] | \n  [.InstanceId, .State.Name, .PrivateIpAddress, .PublicIpAddress, (del((.Tags[]|select(.Key!=\"Name\")))|.Tags[]|.Value|tostring)]|@tsv'\n\n# add sort in output\naws ec2 describe-instances |jq -r '.Reservations[].Instances[] |=sortby(.LaunchTime) | \n  [.InstanceId, .State.Name, .PrivateIpAddress, .PublicIpAddress, (del((.Tags[]|select(.Key!=\"Name\")))|.Tags[]|.Value|tostring)]|@tsv'\n</code></pre> <pre><code># by ssh key name\naws ec2 describe-instances --filters \"Name=key-name,Values=sshkey-aws\"\n\n# by name\naws ec2 describe-instances --filters \"Name=tag:Name,Values=eks*\"\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#install-ssm","title":"install ssm","text":"<pre><code>sudo yum install -y https://s3.ap-northeast-2.amazonaws.com/amazon-ssm-ap-northeast-2/latest/linux_amd64/amazon-ssm-agent.rpm\nsystemctl status amazon-ssm-agent\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#region-cmd","title":"region cmd","text":"<pre><code>export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)\nexport AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\n#export AWS_REGION=ap-northeast-1\nexport AZS=($(aws ec2 describe-availability-zones --query 'AvailabilityZones[].ZoneName' --output text --region $AWS_REGION))\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#get-instance-ids","title":"get instance ids","text":"","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#from-vpc","title":"from vpc","text":"<pre><code>INSTANCE_IDS=($(aws ec2 describe-instances \\\n  --filters \"Name=${TAG},Values=owned\" \"Name=vpc-id,Values=vpc-xxx\"\\\n  |jq -r '.Reservations[].Instances[].InstanceId' ) )\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#filter-tags","title":"filter tags","text":"<pre><code>tag=\naws ec2 describe-instances \\\n  --filters \"Name=tag:Name,Values=cloud9-130\" \\\n  |jq -r '.Reservations[].Instances[].InstanceId' \n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#associate-instance-profile-to-ec2","title":"associate instance profile to ec2","text":"<ul> <li>setup-cloud9-for-eks</li> </ul> <pre><code>aws ec2 describe-iam-instance-profile-associations\naws ec2 disassociate-iam-instance-profile\naws ec2 associate-iam-instance-profile\n\nC9_INST_ID=$(curl 169.254.169.254/latest/meta-data/instance-id)\ninstance_profile_arn=$(aws ec2 describe-iam-instance-profile-associations \\\n  --filter Name=instance-id,Values=$C9_INST_ID \\\n  --query IamInstanceProfileAssociations[0].IamInstanceProfile.Arn \\\n  --output text)\n\naws iam get-instance-profile \\\n  --instance-profile-name ${instance_profile_arn##*/}\n\n## add your needed role to it\naws iam add-role-to-instance-profile \\\n  --instance-profile-name ${instance_profile_arn} \n  --role-name ${ROLE_NAME}\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#security-group","title":"security group","text":"<pre><code># ensure you have security group called 'eks-shared-sg'\naws ec2 describe-security-groups  --region $AWS_REGION --filter Name=vpc-id,Values=$VPC_ID --query 'SecurityGroups[*].[GroupName,GroupId]'\n\n# if you have multi eni, do bond sg to instance manually\nexport SG_ID=$(aws ec2 describe-security-groups  --region $AWS_REGION --filter Name=vpc-id,Values=$VPC_ID --query \"SecurityGroups[?GroupName == 'eks-shared-sg'].GroupId\" --output text)\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#func-create-sg-","title":"func-create-sg-","text":"right-click &amp; open-in-new-tab:","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#create-sg-allow-itself","title":"create sg allow itself","text":"<ul> <li>refer: git/git-mkdocs/data-analytics/mwaa-lab</li> </ul>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#count-vcpu","title":"count vcpu","text":"<pre><code>aws ec2 describe-instances --region us-east-2 \\\n--query Reservations[].Instances[].CpuOptions.[CoreCount,ThreadsPerCore] \\\n--output text \\\n|awk 'BEGIN {sum=0} {line=$1*$2;sum=sum+line} END {print sum} '\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#troubleshooting","title":"troubleshooting","text":"<p>https://linuxconfig.org/how-to-name-label-a-partition-or-volume-on-linux</p> <ul> <li>refer:  linux-cmd</li> <li>refer: e2label</li> </ul>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#kb","title":"KB","text":"<ul> <li>How do I move my EC2 instance to another subnet, Availability Zone, or VPC? (LINK)<ul> <li>\u4e0d\u80fd detach primary eni</li> <li>\u53ea\u80fd attach \u540c\u53ef\u7528\u533a\u7684 eni \uff08\u5373\u4fbf\u662f\u53e6\u4e00\u4e2a subnet \uff09</li> </ul> </li> </ul>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#source-destination-check-","title":"source-destination-check-","text":"<ul> <li>disable <code>Change Source / destination check</code> <pre><code>INST_ID=$(curl 169.254.169.254/latest/meta-data/instance-id)\naws ec2 modify-instance-attribute \\\n    --instance-id=${INST_ID} --source-dest-check\n</code></pre></li> </ul>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#create-key-","title":"create-key-","text":"<pre><code>KEY_NAME=aws-key\necho '&lt;paste&gt;' |base64 -w 0 &gt;my-key-pair.pub.b64\naws ec2 import-key-pair --key-name ${KEY_NAME} --public-key-material file://my-key-pair.pub.b64\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#create-instance-by-chatgpt","title":"create instance by chatgpt","text":"<pre><code>KEY_NAME=aws-key\nAMI_ID=$(aws ec2 describe-images \\\n    --region us-east-2 \\\n    --filters \"Name=name,Values=Windows_Server-2019-English-Full-Base-*\" \\\n              \"Name=architecture,Values=x86_64\" \\\n              \"Name=root-device-type,Values=ebs\" \\\n              \"Name=virtualization-type,Values=hvm\" \\\n    --query \"reverse(sort_by(Images, &amp;CreationDate))[0].ImageId\" \\\n    --output text)\n\naws ec2 run-instances \\\n--image-id ${AMI_ID} \\\n--instance-type t3.medium \\\n--key-name ${KEY_NAME} \\\n--subnet-id $(aws ec2 describe-subnets --filters \"Name=default-for-az,Values=true\" \"Name=vpc-id,Values=$(aws ec2 describe-vpcs --filters \"Name=isDefault,Values=true\" --query \"Vpcs[0].VpcId\" --output text)\" --query \"Subnets[0].SubnetId\" --output text) \\\n--iam-instance-profile Name=EC2DomainJoin-Instance-Profile \\\n--user-data '&lt;powershell&gt;\nImport-Module \"C:\\Program Files\\Amazon\\Ec2ConfigService\\Scripts\\InitializeInstance.ps1\"\nInitialize-EC2Instance -Schedule -DomainName \"corp1.aws.panlm.xyz\" -DomainUserName \"admin\" -DomainPassword \"password\"\n&lt;/powershell&gt;' \\\n--associate-public-ip-address \\\n--tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=MyInstance4}]' \\\n--region us-east-2\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#instance-connect-","title":"instance-connect-","text":"","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#ssh-connect","title":"ssh connect","text":"<ul> <li> <p>from public ip  <pre><code>aws ec2-instance-connect ssh \\\n    --instance-id i-xxx\n</code></pre></p> </li> <li> <p>from private ip (need to create instance connect endpoint first) <pre><code>aws ec2-instance-connect ssh \\\n    --instance-id i-xxx \\\n    --instance-ip 172.31.7.81  \\\n    --connection-type eice\n</code></pre></p> </li> </ul>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ec2-cmd/#send-ssh-public-key","title":"send ssh public key","text":"<pre><code>aws ec2-instance-connect send-ssh-public-key \\\n--region us-east-2 \\\n--ssh-public-key file://out.pub \\\n--instance-id i-xxx \\\n--instance-os-user ec2-user\n</code></pre>","tags":["aws/compute/ec2","aws/cmd"]},{"location":"CLI/awscli/ecr-cmd/","title":"ecr","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/ecr","aws/cmd"]},{"location":"CLI/awscli/ecr-cmd/#ecr-cmd","title":"ecr-cmd","text":"<ul> <li> kms </li> <li> backup </li> <li> replication</li> </ul>","tags":["aws/container/ecr","aws/cmd"]},{"location":"CLI/awscli/ecr-cmd/#login","title":"login","text":"<pre><code>export AWS_DEFAULT_REGION=us-east-2\nexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nECR_URL=${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\nAWS_CLI_VERSION=$(aws --version 2&gt;&amp;1 | cut -d/ -f2 | cut -d. -f1)\n\necr_login() {\n    if [ $AWS_CLI_VERSION -gt 1 ]; then\n        aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | \\\n            docker login --username AWS --password-stdin ${ECR_URL}\n    else\n        $(aws ecr get-login --no-include-email)\n    fi\n}\necr_login\n</code></pre>","tags":["aws/container/ecr","aws/cmd"]},{"location":"CLI/awscli/ecr-cmd/#create-repo-","title":"create-repo-","text":"<pre><code>REPO_NAME=osarch\naws ecr create-repository --repository-name ${REPO_NAME}\nREPO_URI=$(aws ecr describe-repositories --repository-names ${REPO_NAME} --query repositories[0].repositoryUri --output text)\n</code></pre> <ul> <li>refer: ../linux/docker-cmd</li> </ul>","tags":["aws/container/ecr","aws/cmd"]},{"location":"CLI/awscli/ecr-cmd/#create-ecr-repo-nginx","title":"create ecr repo nginx","text":"<pre><code>export AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document |jq -r '.region') export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) ECR_URL=${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\n\naws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin ${ECR_URL}\n\naws ecr create-repository --repository-name nginx\n\ndocker tag nginx:latest ${ECR_URL}/nginx:latest\n</code></pre>","tags":["aws/container/ecr","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/","title":"ecs","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#ecs-cmd","title":"ecs-cmd","text":"","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#get-ami-list","title":"get ami list","text":"<ul> <li> <p>get full list <pre><code>aws ssm get-parameters-by-path \\\n    --path /aws/service/ecs/optimized-ami/amazon-linux-2 \\\n    --query 'Parameters[?contains(ARN, `arn:aws:ssm:us-east-2::parameter/aws/service/ecs/optimized-ami/amazon-linux-2/ami-`)==`true`]' \\\n    |jq -r 'sort_by(.LastModifiedDate)' \n</code></pre></p> </li> <li> <p>last 2 AMI ID <pre><code>AMI_IDS=($(\naws ssm get-parameters-by-path \\\n    --path /aws/service/ecs/optimized-ami/amazon-linux-2 \\\n    --query 'sort_by(Parameters,&amp;LastModifiedDate)[?contains(ARN, `arn:aws:ssm:us-east-2::parameter/aws/service/ecs/optimized-ami/amazon-linux-2/ami-`)==`true`]' \\\n    |jq -r '.[].Value' |jq -r '.image_id' \\\n    |tail -n 2\n))\n\nOLD_AMI_ID=${AMI_IDS[0]}\nNEW_AMI_ID=${AMI_IDS[1]}\n</code></pre></p> </li> </ul>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#create-cluster-with-ec2-capacity-provider","title":"create cluster with ec2 capacity provider","text":"<ul> <li>basic info <pre><code>ECS_CLUSTER=myecs1\nVPC_ID=$(aws ec2 describe-vpcs --filter Name=is-default,Values=true --query 'Vpcs[0].VpcId' --output text)\nexport AWS_PAGER=\"\"\nexport AWS_DEFAULT_REGION=us-east-2\n</code></pre></li> <li>get ecs recommended optimized ami <pre><code>ECS_AMI=$(aws ssm get-parameters --names /aws/service/ecs/optimized-ami/amazon-linux-2/recommended |jq -r '.Parameters[0].Value' |jq -r '.image_id')\n</code></pre></li> <li>get default security group (func-create-sg.sh) <pre><code>create-sg -v ${VPC_ID} -c 0.0.0.0/0 # call my function\necho ${SG_ID}\n</code></pre></li> <li>execute function to create launch template (notes/auto-scaling-cmd) <pre><code>echo ${SG_ID}\necho ${OLD_AMI_ID}\ncreate-launch-template ${SG_ID} ${OLD_AMI_ID} # call my function\necho ${LAUNCH_TEMPLATE_ID}\n</code></pre></li> <li>execute function to create ec2 admin role (git/git-mkdocs/CLI/awscli/iam-cmd) <pre><code>ec2-admin-role-create # call my function\necho ${INSTANCE_PROFILE_ARN}\n</code></pre></li> <li> <p>add user data to launch template and make default version is 2 <pre><code>TMP=$(mktemp --suffix .UserData)\nenvsubst &gt;${TMP} &lt;&lt;-EOF\n#!/bin/bash\necho ECS_CLUSTER=${ECS_CLUSTER} &gt;&gt; /etc/ecs/ecs.config\nsudo iptables --insert FORWARD 1 --in-interface docker+ --destination 169.254.169.254/32 --jump DROP\nsudo service iptables save\necho ECS_AWSVPC_BLOCK_IMDS=true &gt;&gt; /etc/ecs/ecs.config\nEOF\nB64STRING=$(cat ${TMP}|base64 -w 0)\naws ec2 create-launch-template-version --launch-template-id ${LAUNCH_TEMPLATE_ID} \\\n    --version-description ${LAUNCH_TEMPLATE_ID}-$(TZ=CST-8 date +%H%M) \\\n    --source-version 1 \\\n    --launch-template-data '{\"UserData\":\"'\"${B64STRING}\"'\",\"IamInstanceProfile\":{\"Arn\":\"'\"${INSTANCE_PROFILE_ARN}\"'\"}}' |tee ${TMP}.out\n\naws ec2 modify-launch-template --launch-template-id ${LAUNCH_TEMPLATE_ID} --default-version \"2\"\n</code></pre></p> </li> <li> <p>execute function to create auto scaling group (notes/auto-scaling-cmd) <pre><code>create-auto-scaling ${LAUNCH_TEMPLATE_ID} # call my function\necho ${ASG_ARN}\n</code></pre></p> </li> <li> <p>create ecs cluster  <pre><code>echo ${ECS_CLUSTER}\n\naws iam create-service-linked-role --aws-service-name ecs.amazonaws.com\naws ecs create-cluster --cluster-name ${ECS_CLUSTER}\n</code></pre></p> </li> <li> <p>capacity provider  <pre><code>ECS_CAP_PROVIDER=mycp1\naws ecs create-capacity-provider \\\n    --name \"${ECS_CAP_PROVIDER}\" \\\n    --auto-scaling-group-provider \"autoScalingGroupArn=${ASG_ARN},managedScaling={status=ENABLED,targetCapacity=100},managedTerminationProtection=ENABLED\"\n\naws ecs put-cluster-capacity-providers \\\n    --cluster ${ECS_CLUSTER} \\\n    --capacity-providers ${ECS_CAP_PROVIDER} \\\n    --default-capacity-provider-strategy capacityProvider=${ECS_CAP_PROVIDER},weight=1\n</code></pre></p> </li> </ul>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#get-container-instance","title":"get container instance","text":"<pre><code>CONTAINER_INST_ARN=($(aws ecs list-container-instances --cluster ${ECS_CLUSTER} \\\n    --query 'containerInstanceArns[]' --output text |xargs ))\nCONTAINER_INST=($(\n    for i in ${CONTAINER_INST_ARN[@]} ; do\n        echo ${i##*/}\n    done\n))\necho ${CONTAINER_INST[@]}\naws ecs describe-container-instances --cluster ${ECS_CLUSTER} \\\n    --container-instances ${CONTAINER_INST[@]} |tee /tmp/$$-instance\ncat /tmp/$$-instance |jq -r '.containerInstances[] | [.ec2InstanceId, .versionInfo.agentVersion]|@tsv' \n</code></pre>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#register-task-definition","title":"register task definition","text":"<pre><code>TASK_NAME=task2-$(TZ=EAT-8 date +%Y%m%d-%H%M)\n\nenvsubst &gt;task-definition.json &lt;&lt;-EOF\n{\n  \"containerDefinitions\": [\n    {\n      \"name\": \"nginx\",\n      \"image\": \"nginx\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 80,\n          \"hostPort\": 80,\n          \"protocol\": \"tcp\",\n          \"name\": \"nginx-80-tcp\",\n          \"appProtocol\": \"http\"\n        }\n      ],\n      \"essential\": true\n    }\n  ],\n  \"family\": \"${TASK_NAME}\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\n    \"EC2\"\n  ],\n  \"cpu\": \"1024\",\n  \"memory\": \"3072\"\n}\nEOF\n\naws ecs register-task-definition \\\n    --cli-input-json file://./task-definition.json\n</code></pre>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#create-service","title":"create service","text":"<ul> <li>create alb and target group first (elb-cmd) <pre><code>create-alb-and-tg\necho ${TG80_ARN}\n</code></pre></li> <li>create service <pre><code>echo ${ECS_CLUSTER}\necho ${TG80_ARN}\necho ${TASK_NAME}\necho ${VPC_ID}\nSERVICE_NAME=${TASK_NAME}-svc\nDEFAULT_SG_ID=$(aws ec2 describe-security-groups \\\n    --filter Name=vpc-id,Values=${VPC_ID} \\\n    --query \"SecurityGroups[?GroupName == 'default'].GroupId\" \\\n    --output text)\nALL_SUBNET_ID=$(aws ec2 describe-subnets \\\n    --filter \"Name=vpc-id,Values=${VPC_ID}\" \\\n    --query 'Subnets[?MapPublicIpOnLaunch==`true`].SubnetId')\n\nenvsubst &gt;svc-definition.json &lt;&lt;-EOF\n{\n    \"serviceName\": \"${SERVICE_NAME}\",\n    \"taskDefinition\": \"${TASK_NAME}\",\n    \"loadBalancers\": [\n        {\n            \"targetGroupArn\": \"${TG80_ARN}\",\n            \"containerName\": \"nginx\",\n            \"containerPort\": 80\n        }\n    ],\n    \"networkConfiguration\": {\n        \"awsvpcConfiguration\": {\n            \"subnets\": ${ALL_SUBNET_ID},\n            \"securityGroups\": [\n                \"${DEFAULT_SG_ID}\"\n            ],\n            \"assignPublicIp\": \"DISABLED\"\n        }\n    }, \n    \"desiredCount\": 3\n}\nEOF\n\naws ecs create-service \\\n    --cluster ${ECS_CLUSTER} \\\n    --service-name ${SERVICE_NAME} \\\n    --cli-input-json file://svc-definition.json\n</code></pre></li> </ul>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#update-launch-template-","title":"update-launch-template-","text":"<ul> <li>update launch template based on default version <pre><code>echo ${NEW_AMI_ID}\n\nLT_DEF_VER=$(aws ec2 describe-launch-templates --launch-template-ids  ${LAUNCH_TEMPLATE_ID} --query 'LaunchTemplates[0].DefaultVersionNumber' --output text)\n\naws ec2 create-launch-template-version --launch-template-id ${LAUNCH_TEMPLATE_ID} \\\n    --version-description ${LAUNCH_TEMPLATE_ID}-$(TZ=CST-8 date +%H%M) \\\n    --source-version ${LT_DEF_VER} \\\n    --launch-template-data '{\"ImageId\":\"'\"${NEW_AMI_ID}\"'\"}' |tee /tmp/$$-new-lt\n\nLT_VER=$(cat /tmp/$$-new-lt |jq -r '.LaunchTemplateVersion.VersionNumber')\n</code></pre></li> </ul>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#update-asg-","title":"update-asg-","text":"<ul> <li>update asg using new template version <pre><code>aws autoscaling update-auto-scaling-group \\\n    --auto-scaling-group-name ${ASG_ARN##*/} \\\n    --launch-template LaunchTemplateId=${LAUNCH_TEMPLATE_ID},Version=${LT_VER}\n</code></pre></li> </ul>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#get-task-definition","title":"get task definition","text":"<pre><code>ECS_SVC_NAME=${SERVICE_NAME}\nECS_TASK_DEF_ARN=$(aws ecs describe-services --cluster ${ECS_CLUSTER} \\\n--services ${ECS_SVC_NAME} \\\n--query \"services[].deployments[].[\"taskDefinition\"]\" --output text)\n</code></pre>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#modify-task-definition-","title":"modify-task-definition-","text":"<ul> <li>update task definition <pre><code># ami-0b7778d86d8789cff / ami-0f896121197c465b6\necho ${NEW_AMI_ID}\naws ecs describe-task-definition --task-definition ${ECS_TASK_DEF_ARN} --query taskDefinition | \\\njq '. + {placementConstraints: [{\"expression\": \"attribute:ecs.ami-id == '\"${NEW_AMI_ID}\"'\", \"type\": \"memberOf\"}]}' | \\\njq 'del(.status)'| jq 'del(.revision)' | jq 'del(.requiresAttributes)' | \\\njq '. + {containerDefinitions:[.containerDefinitions[] + {\"memory\":256, \"memoryReservation\": 128}]}'| \\\njq 'del(.compatibilities)' | jq 'del(.taskDefinitionArn)' | jq 'del(.registeredAt)' | jq 'del(.registeredBy)' &gt; new-task-def.json\n</code></pre></li> <li>register new one <pre><code>aws ecs register-task-definition \\\n    --cli-input-json file://./new-task-def.json |tee /tmp/$$-new-task-def\nTASK_DEF_ARN=$(cat /tmp/$$-new-task-def |jq -r '.taskDefinition.taskDefinitionArn')\n</code></pre></li> </ul>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#update-service","title":"update service","text":"<pre><code>aws ecs update-service --cluster ${ECS_CLUSTER} --service ${SERVICE_NAME} --task-definition ${TASK_DEF_ARN##*/}\n</code></pre>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#func-ecsexec","title":"func ecsexec","text":"<pre><code>ECS_CLUSTER=CapacityProviderDemo\nfunction ecsexec {\naws ecs execute-command \\\n    --cluster ${ECS_CLUSTER} --task $1 \\\n    --command /bin/sh --interactive \\\n    --command \"curl localhost:5000\" \\\n    --region us-east-2 \n}\n</code></pre>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/ecs-cmd/#sample","title":"sample","text":"<ul> <li>create asg &amp; launch template first<ul> <li>auto-scaling-cmd</li> </ul> </li> </ul> <pre><code>ECS_CLUSTER_NAME=MyCluster\naws ecs create-cluster \\\n    --cluster-name ${ECS_CLUSTER_NAME}\n\nECS_CAP_PROVIDER=MyCapacityProvider\naws ecs create-capacity-provider \\\n    --name \"${ECS_CAP_PROVIDER}\" \\\n    --auto-scaling-group-provider \"autoScalingGroupArn=${ASG_ARN},managedScaling={status=ENABLED,targetCapacity=10}\"\n\naws ecs put-cluster-capacity-providers \\\n    --cluster ${ECS_CLUSTER_NAME} \\\n    --capacity-providers ${ECS_CAP_PROVIDER} \\\n    --default-capacity-provider-strategy capacityProvider=${ECS_CAP_PROVIDER},weight=1\n</code></pre>","tags":["aws/container/ecs","aws/cmd"]},{"location":"CLI/awscli/efs-cmd/","title":"efs","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/cmd","aws/storage/efs"]},{"location":"CLI/awscli/efs-cmd/#efs-cmd","title":"efs-cmd","text":"","tags":["aws/cmd","aws/storage/efs"]},{"location":"CLI/awscli/efs-cmd/#create-efs-","title":"create-efs-","text":"<ul> <li>\u5728\u9ed8\u8ba4 vpc \u4e2d\u521b\u5efa efs <pre><code>CLUSTER_NAME=${CLUSTER_NAME:-eks0630}\nAWS_REGION=${AWS_REGION:-us-east-2}\n\nVPC_ID=$(aws ec2 describe-vpcs --filter Name=is-default,Values=true --query 'Vpcs[0].VpcId' --output text)\nVPC_CIDR=$(aws ec2 describe-vpcs --vpc-ids ${VPC_ID} \\\n  --query \"Vpcs[].CidrBlock\"  --region ${AWS_REGION} --output text )\n\n# create security group\nSG_ID=$(aws ec2 create-security-group --description ${CLUSTER_NAME}-efs-eks-sg \\\n  --group-name efs-sg-$RANDOM --vpc-id ${VPC_ID} |jq -r '.GroupId' )\n# allow tcp 2049 (nfs v4)\naws ec2 authorize-security-group-ingress --group-id ${SG_ID}  --protocol tcp --port 2049 --cidr ${VPC_CIDR}\n\n# create efs\nFILESYSTEM_ID=$(aws efs create-file-system \\\n  --creation-token ${CLUSTER_NAME} \\\n  --region ${AWS_REGION} |jq -r '.FileSystemId' )\necho ${FILESYSTEM_ID}\n\nwhile true ; do\naws efs describe-file-systems \\\n--file-system-id ${FILESYSTEM_ID} \\\n--query 'FileSystems[].LifeCycleState' \\\n--output text |grep -q available\nif [[ $? -eq 0 ]]; then\n  break\nelse\n  echo \"wait...\"\n  sleep 10\nfi  \ndone\n\n# create mount target\nPUBLIC_SUBNETS=($(aws ec2 describe-subnets \\\n--filter \"Name=vpc-id,Values=$VPC_ID\" \\\n--query 'Subnets[?MapPublicIpOnLaunch==`true`].SubnetId' \\\n--output text))\n\nfor i in ${PUBLIC_SUBNETS[@]} ; do\n  echo \"creating mount target in: \" $i\n  aws efs create-mount-target --file-system-id ${FILESYSTEM_ID} \\\n    --subnet-id ${i} --security-group ${SG_ID}\ndone\n</code></pre></li> </ul> <p>^d4lka9</p> <p>another example: ../../EKS/addons/efs-for-eks </p>","tags":["aws/cmd","aws/storage/efs"]},{"location":"CLI/awscli/efs-cmd/#refer","title":"refer","text":"<ul> <li>https://repost.aws/knowledge-center/efs-mount-automount-unmount-steps</li> </ul>","tags":["aws/cmd","aws/storage/efs"]},{"location":"CLI/awscli/iam-cmd/","title":"iam","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#iam-cmd","title":"iam cmd","text":"","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#get-role-arn-by-name","title":"get role arn by name","text":"<pre><code>aws iam get-role --role-name ${role_name} --query 'Role.Arn' --output text\n</code></pre>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#get-policy-arn","title":"get policy arn","text":"<pre><code>aws iam list-policies --query 'Policies[*].[PolicyName,Arn]' --output text |grep CloudWatchAgentServerPolicy\naws iam get-policy --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\n</code></pre>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#create-user","title":"create user","text":"<pre><code>aws iam create-user --user-name cwagent-onprem\n# attach user policy\naws iam attach-user-policy --user-name cwagent-onprem --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\n# create access key, save the output\naws iam create-access-key --user-name cwagent-onprem\n</code></pre>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#attach-role-policy","title":"attach role policy","text":"<ul> <li>../../others/file-storage-gateway-lab</li> </ul>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#create-role","title":"create role","text":"","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#func-ec2-admin-role-create-","title":"func-ec2-admin-role-create-","text":"<ul> <li>create ec2 admin role func-ec2-admin-role-create<pre><code># no dependency variable\n# output ROLE_ARN / INSTANCE_PROFILE_ARN\nfunction ec2-admin-role-create () {\n    ROLE_NAME=ec2-admin-role-$(TZ=CST-8 date +%Y%m%d-%H%M)\n    local TMP=$(mktemp --suffix .${ROLE_NAME})\n    cat &gt;${TMP} &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"ec2.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\nEOF\n    aws iam create-role --role-name ${ROLE_NAME} \\\n        --assume-role-policy-document file://${TMP} |tee ${TMP}.out.role\n    aws iam attach-role-policy --role-name ${ROLE_NAME} \\\n        --policy-arn \"arn:aws:iam::aws:policy/AdministratorAccess\"\n    aws iam create-instance-profile --instance-profile-name ${ROLE_NAME} |tee ${TMP}.out.instance-profile\n    aws iam add-role-to-instance-profile --instance-profile-name ${ROLE_NAME} \\\n        --role-name ${ROLE_NAME}\n    ROLE_ARN=$(cat ${TMP}.out.role |jq -r '.Role.Arn')\n    INSTANCE_PROFILE_ARN=$(cat ${TMP}.out.instance-profile |jq -r '.InstanceProfile.Arn')\n}\n</code></pre></li> </ul>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#create-role-for-firehose","title":"create role for firehose","text":"<ul> <li>../../EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3</li> </ul>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#create-role-for-api-gateway","title":"create role for api gateway","text":"<ul> <li>apigw-cmd</li> </ul>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#create-role-for-account","title":"create role for account","text":"<ul> <li>../linux/assume-tool</li> </ul>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#create-service-linked-role","title":"create service-linked role","text":"<pre><code>aws iam create-service-linked-role --aws-service-name SERVICE-NAME.amazonaws.com\n</code></pre>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#assume-another-role-","title":"assume-another-role-","text":"<pre><code>account_id=$(aws sts get-caller-identity \\\n  --query 'Account' --output text)\nrole_arn=arn:aws:iam::${account_id}:role/eksworkshop-admin\ntmp_file=/tmp/$$.1\n\naws sts assume-role --role-arn ${role_arn} \\\n  --duration-seconds 43200 \\\n  --role-session-name Session-$RANDOM |tee ${tmp_file}\n\nexport AWS_ACCESS_KEY_ID=$(cat ${tmp_file} |jq -r '.Credentials.AccessKeyId' )\nexport AWS_SECRET_ACCESS_KEY=$(cat ${tmp_file} |jq -r '.Credentials.SecretAccessKey' )\nexport AWS_SESSION_TOKEN=$(cat ${tmp_file} |jq -r '.Credentials.SessionToken' )\nexport AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\n</code></pre>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#assume-in-credentials-file","title":"assume in credentials file","text":"<ul> <li>https://docs.aws.amazon.com/sdkref/latest/guide/feature-assume-role-credentials.html <pre><code>[profile A]\nsource_profile = B\nrole_arn =  arn:aws:iam::123456789012:role/RoleA\n\n[profile B]\naws_access_key_id=AKIAIOSFODNN7EXAMPLE\naws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n</code></pre></li> </ul>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/iam-cmd/#reference","title":"reference","text":"<ul> <li>\u5982\u4f55\u63d0\u4f9b\u5bf9 Amazon S3 \u5b58\u50a8\u6876\u4e2d\u7684\u5bf9\u8c61\u7684\u8de8\u8d26\u6237\u8bbf\u95ee\u6743\u9650</li> <li>\u7b56\u7565\u8bc4\u4f30\u903b\u8f91</li> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_multi-value-conditions.html</li> </ul>","tags":["aws/security/iam","aws/cmd"]},{"location":"CLI/awscli/lambda-cmd/","title":"lambda-cmd","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/serverless/lambda","aws/cmd"]},{"location":"CLI/awscli/lambda-cmd/#lambda-cmd","title":"lambda-cmd","text":"","tags":["aws/serverless/lambda","aws/cmd"]},{"location":"CLI/awscli/lambda-cmd/#create-execute-role","title":"create execute role","text":"<ul> <li>https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html#permissions-executionrole-console <pre><code>role_name=lambda-ex-$RANDOM\naws iam create-role --role-name ${role_name} --assume-role-policy-document '{\"Version\": \"2012-10-17\",\"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]}'\naws iam attach-role-policy --role-name ${role_name} --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n</code></pre></li> </ul>","tags":["aws/serverless/lambda","aws/cmd"]},{"location":"CLI/awscli/lambda-cmd/#save-code","title":"save code","text":"<pre><code>wget -O index.js 'https://raw.githubusercontent.com/panlm/aws-eks-example/main/lambda/kinesis-firehose-cloudwatch-logs-processor'\nzip function.zip index.js\n</code></pre>","tags":["aws/serverless/lambda","aws/cmd"]},{"location":"CLI/awscli/lambda-cmd/#create-lambda","title":"create lambda","text":"<pre><code>role_arn=$(aws iam get-role --role-name ${role_name} |jq -r '.Role.Arn')\naws lambda create-function \\\n--function-name cwl-s3-${role_name} \\\n--timeout 60 \\\n--runtime 'nodejs14.x' \\\n--role ${role_arn} \\\n--zip-file fileb://function.zip \\\n--handler index.handler\n</code></pre>","tags":["aws/serverless/lambda","aws/cmd"]},{"location":"CLI/awscli/lambda-cmd/#add-layer-to-lambda-","title":"add-layer-to-lambda-","text":"<ul> <li>upload zip package from mac to lambda, seems works</li> </ul> <pre><code>mkdir -p $$/python/lib/python3.8/site-packages\npip install flatten_json -t $$/python/lib/python3.8/site-packages\ncd $$\nzip -r package.zip ./python\n</code></pre> <ul> <li>push layer version <pre><code>aws lambda publish-layer-version --layer-name layer_flatten_json --description \"flatten_json\" --zip-file fileb://package.zip --compatible-runtimes python3.8\nlayer_arn=$(aws lambda list-layer-versions --layer-name layer_flatten_json \\\n--query 'LayerVersions[0].LayerVersionArn' --output text)\n\naws lambda update-function-configuration --function-name ${lambda_name} \\\n--layers ${layer_arn}\n</code></pre></li> </ul>","tags":["aws/serverless/lambda","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/","title":"rds","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/#rds-cmd","title":"rds-cmd","text":"","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/#create-postsql","title":"create postsql","text":"<ul> <li>https://www.eksworkshop.com/beginner/115_sg-per-pod/10_secgroup/</li> </ul> <pre><code>export VPC_ID=vpc-xxx\nexport VPC_CIDR=\"172.31.0.0/16\"\nexport RDS_NAME=rdsworkshop\n\nsudo yum install -y jq \nsudo amazon-linux-extras install -y postgresql12\n\nSG_NAME=${RDS_NAME}-${RANDOM}\naws ec2 create-security-group  \\\n  --description ${SG_NAME}     \\\n  --group-name ${SG_NAME}      \\\n  --vpc-id ${VPC_ID}\n\nexport RDS_SG=$(aws ec2 describe-security-groups      \\\n  --filters Name=group-name,Values=${SG_NAME}         \\\n            Name=vpc-id,Values=${VPC_ID}              \\\n  --query \"SecurityGroups[0].GroupId\" --output text)\n\necho \"RDS security group ID: ${RDS_SG}\"\n\naws ec2 authorize-security-group-ingress  \\\n  --group-id ${RDS_SG}                    \\\n  --protocol tcp                          \\\n  --port 5432                             \\\n  --cidr ${VPC_CIDR}\n\nexport PUBLIC_SUBNETS_ID=$(aws ec2 describe-subnets        \\\n  --filters \"Name=vpc-id,Values=$VPC_ID\"                   \\\n  --query 'Subnets[?MapPublicIpOnLaunch==`true`].SubnetId' \\\n  --output json | jq -c .)\n\n# create a db subnet group\naws rds create-db-subnet-group               \\\n  --db-subnet-group-name ${RDS_NAME}         \\\n  --db-subnet-group-description ${RDS_NAME}  \\\n  --subnet-ids ${PUBLIC_SUBNETS_ID}\n\n# generate a password for RDS\nexport RDS_PASSWORD=\"$(date | md5sum  |cut -f1 -d' ')\"\necho ${RDS_PASSWORD}  &gt; ~/rds_password\n\n# create RDS Postgresql instance\naws rds create-db-instance                 \\\n  --db-instance-identifier ${RDS_NAME}     \\\n  --db-name ${RDS_NAME}                    \\\n  --db-instance-class db.t3.micro          \\\n  --engine postgres                        \\\n  --db-subnet-group-name ${RDS_NAME}       \\\n  --vpc-security-group-ids ${RDS_SG}       \\\n  --master-username ${RDS_NAME}            \\\n  --publicly-accessible                    \\\n  --master-user-password ${RDS_PASSWORD}   \\\n  --backup-retention-period 0              \\\n  --allocated-storage 20\n\naws rds describe-db-instances                \\\n  --db-instance-identifier ${RDS_NAME}       \\\n  --query \"DBInstances[].DBInstanceStatus\"   \\\n  --output text\n\n# get RDS endpoint\nexport RDS_ENDPOINT=$(aws rds describe-db-instances \\\n  --db-instance-identifier ${RDS_NAME} \\\n  --query 'DBInstances[0].Endpoint.Address' \\\n  --output text)\n\necho \"RDS endpoint: ${RDS_ENDPOINT}\"\n\n\ncat &gt; /tmp/pgsql.sql &lt;&lt;-EoF\nCREATE TABLE welcome (column1 TEXT);\ninsert into welcome values ('--------------------------');\ninsert into welcome values ('Welcome to the rdsworkshop');\ninsert into welcome values ('Welcome to the rdsworkshop');\ninsert into welcome values ('Welcome to the rdsworkshop');\ninsert into welcome values ('Welcome to the rdsworkshop');\ninsert into welcome values ('Welcome to the rdsworkshop');\ninsert into welcome values ('--------------------------');\nEoF\n\nexport RDS_PASSWORD=$(cat ~/rds_password)\npsql postgresql://${RDS_NAME}:${RDS_PASSWORD}@${RDS_ENDPOINT}:5432/${RDS_NAME} -f /tmp/pgsql.sql\n</code></pre>","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/#create-mysql","title":"create mysql","text":"<ul> <li>../../data-analytics/rds-mysql-replica-cross-region-cross-account</li> </ul>","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/#quick-create-rds-mysql-in-default-vpc","title":"quick create rds mysql in default vpc","text":"right-click &amp; open-in-new-tab: function get-default-vpc right-click &amp; open-in-new-tab: function create-sg right-click &amp; open in new tab: function get-subnets  <pre><code>get-default-vpc\nVPC_ID=$DEFAULT_VPC\nVPC_CIDR=$DEFAULT_CIDR\n\ncreate-sg ${VPC_ID} ${VPC_CIDR}\necho ${SG_ID}\n\nget-subnets ${VPC_ID} true\necho ${SUBNET_IDS}\n\n# create a db subnet group\naws rds create-db-subnet-group \\\n  --db-subnet-group-name ${SG_ID} \\\n  --db-subnet-group-description ${SG_ID} \\\n  --subnet-ids ${SUBNET_IDS}\n\nDB_ADMIN=admin\nDB_PASSWORD=admin1234567890\nDB_NAME=llm-$(TZ=EAT-8 date +%Y%m%d-%H%M%S)\naws rds create-db-instance \\\n    --db-instance-identifier ${DB_NAME} \\\n    --engine mysql \\\n    --db-instance-class db.r6g.large \\\n    --master-username ${DB_ADMIN} \\\n    --master-user-password ${DB_PASSWORD} \\\n    --db-subnet-group-name ${SG_ID} \\\n    --vpc-security-group-ids ${SG_ID} \\\n    --allocated-storage 100 \n\naws rds wait db-instance-available --db-instance-identifier ${DB_NAME}\n</code></pre>","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/#create-read-replica-cross-region","title":"create read replica cross region","text":"<pre><code># in china region\nsource_db_arn=arn:aws-cn:rds:cn-northwest-1:123456789012:db:database-1\naws rds create-db-instance-read-replica \\\n  --db-instance-identifier database-1-rep-from-cnnw1 \\\n  --region cn-north-1 \\\n  --source-region cn-northwest-1 \\\n  --source-db-instance-identifier ${source_db_arn} \\\n  --kms-key-id arn:aws-cn:kms:cn-north-1:123456789012:alias/aws/rds\n</code></pre>","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/#delete","title":"delete","text":"<pre><code>aws rds delete-db-instance --db-instance-identifier test1 \\\n  --skip-final-snapshot\n</code></pre>","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/#modify","title":"modify","text":"<pre><code>aws rds modify-db-instance --db-instance-identifier test1 \\\n  --no-multi-az --no-publicly-accessible\n</code></pre>","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/#describe-instance","title":"describe-instance","text":"<pre><code>aws rds describe-db-instances\n</code></pre>","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/rds-cmd/#ssl-connection","title":"ssl connection","text":"<ul> <li>download (link)</li> <li>verify with mysql (link) <pre><code>wget 'https://truststore.pki.rds.amazonaws.com/global/global-bundle.pem'\n# maybe need to change surfix to pem\n\ndbhost=xxxx\nmysql -h $dbhost --ssl-ca=global-bundle.pem --ssl-mode=VERIFY_IDENTITY -P 3306 -u admin -p\n</code></pre></li> </ul>","tags":["aws/database/rds","aws/cmd"]},{"location":"CLI/awscli/redshift-cmd/","title":"redshift","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/database/redshift"]},{"location":"CLI/awscli/redshift-cmd/#redshift-cmd","title":"redshift-cmd","text":"","tags":["aws/database/redshift"]},{"location":"CLI/awscli/redshift-cmd/#setup","title":"setup","text":"<p>refer: git/git-mkdocs/data-analytics/redshift-data-api-lab</p>","tags":["aws/database/redshift"]},{"location":"CLI/awscli/redshift-cmd/#unload","title":"unload","text":"<ul> <li>https://docs.aws.amazon.com/zh_cn/redshift/latest/dg/t_unloading_encrypted_files.html </li> <li>https://hevodata.com/learn/redshift-unload-command-usage-and-examples/#r2</li> </ul> <p>CSE: <pre><code>echo 01234567890123456789012345678901 |base64\n</code></pre></p> <pre><code>MDEyMzQ1Njc4OTAxMjM0NTY3ODkwMTIzNDU2Nzg5MDEK\n</code></pre> <pre><code>unload ('select * from dev.public.customer')\nto 's3://template1-rs3bucket-1bor4w2qr4rti/unload_encrypted/'\niam_role 'arn:aws:iam::012345678901:role/RedshiftImmersionRole'\nmaster_symmetric_key 'MDEyMzQ1Njc4OTAxMjM0NTY3ODkwMTIzNDU2Nzg5MDEK'\nencrypted;\n</code></pre> <p>SSE: <pre><code>\n</code></pre></p>","tags":["aws/database/redshift"]},{"location":"CLI/awscli/redshift-cmd/#create-database","title":"create database","text":"<pre><code>create database lab798\n</code></pre>","tags":["aws/database/redshift"]},{"location":"CLI/awscli/redshift-cmd/#create-table","title":"create table","text":"<pre><code>create table table_utf8(\ncol1 integer not null,\ncol2 varchar(100) not null ,\ncol3 varchar(100) not null\n);\n\ninsert into table_utf8 values\n(  1,'\u4e2d\u65871','\u4e2d\u65872'),\n(  2,'\u4e2d\u65871','\u4e2d\u65872'),\n(  3,'\u4e2d\u65871','\u4e2d\u65872');\n</code></pre>","tags":["aws/database/redshift"]},{"location":"CLI/awscli/redshift-cmd/#others","title":"others","text":"<ul> <li>https://github.com/awslabs/amazon-redshift-utils/tree/master/src/UnloadCopyUtility</li> </ul>","tags":["aws/database/redshift"]},{"location":"CLI/awscli/redshift-cmd/#create-cluster","title":"create cluster","text":"<pre><code>aws redshift create-cluster \\\n--cluster-identifier my-redshift-cluster \\\n--node-type dc2.large \\\n--master-username myadmin \\\n--master-user-password passworD.1 \\\n--number-of-nodes 3 \n</code></pre>","tags":["aws/database/redshift"]},{"location":"CLI/awscli/route53-cmd/","title":"route53","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/network/route53"]},{"location":"CLI/awscli/route53-cmd/#route53-cmd","title":"route53-cmd","text":"","tags":["aws/network/route53"]},{"location":"CLI/awscli/route53-cmd/#create-hosted-zone","title":"create hosted zone","text":"right-click &amp; open-in-new-tab:","tags":["aws/network/route53"]},{"location":"CLI/awscli/route53-cmd/#func-create-ns-record-","title":"func-create-ns-record-","text":"<ul> <li>create host zone in your child account and get NS (previous chapter)</li> <li>assume to your parent account to execute this function to add NS record to upstream route53 host zone create-ns-record<pre><code># DOMAIN_NAME=poc0000.aws.panlm.xyz\n# NS='ns-1716.awsdns-22.co.uk.\n# ns-934.awsdns-52.net.\n# ns-114.awsdns-14.com.\n# ns-1223.awsdns-24.org.'\n\nfunction create-ns-record () {\n    OPTIND=1\n    OPTSTRING=\"h?n:s:\"\n    local DOMAIN_NAME=\"\"\n    local NS=\"\"\n    while getopts ${OPTSTRING} opt; do\n        case \"${opt}\" in\n            n) DOMAIN_NAME=${OPTARG} ;;\n            s) NS=${OPTARG} ;;\n            h|\\?) \n                echo \"format: create-host-zone -n DOMAIN_NAME -s \\\"NS_RECORDS\\\" \"\n                echo -e \"\\tsample: create-host-zone -n xxx.domain.com -s \\\"ns-xx.awsdns-xx.com ns-xx.awsdns-xx.com\\\" \"\n                return 0\n            ;;\n        esac\n    done\n    : ${DOMAIN_NAME:?Missing -n}\n    : ${NS:?Missing -s}\n\n    # check NS number\n    local NS_NUM=$(echo $NS |xargs -n 1 |wc -l)\n    if [[ ${NS_NUM} -eq 1 ]]; then\n        echo \"your NS is: \"${NS}\n        echo 'typical NS record should has more than one record'\n        echo 'use double quotes when you use variable for -s '\n        create-ns-record -h\n    fi\n\n    PARENT_DOMAIN_NAME=${DOMAIN_NAME#*.}\n    ZONE_ID=$(aws route53 list-hosted-zones-by-name \\\n    --dns-name \"${PARENT_DOMAIN_NAME}.\" \\\n    --query HostedZones[0].Id --output text)\n\n    envsubst &gt;/tmp/ns-route53-record.json &lt;&lt;-EOF\n{\n  \"Comment\": \"UPSERT a record for poc.xxx.com \",\n  \"Changes\": [\n    {\n      \"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"${DOMAIN_NAME}\",\n        \"Type\": \"NS\",\n        \"TTL\": 172800,\n        \"ResourceRecords\": [\n        ]\n      }\n    }\n  ]\n}\nEOF\n\n    for i in ${NS}; do\n        cat /tmp/ns-route53-record.json |jq '.Changes[0].ResourceRecordSet.ResourceRecords += [{\"Value\": \"'\"${i}\"'\"}]' \\\n            |tee /tmp/ns-route53-record-tmp.json\n        mv -f /tmp/ns-route53-record-tmp.json /tmp/ns-route53-record.json\n    done\n\n    aws route53 change-resource-record-sets --hosted-zone-id ${ZONE_ID} --change-batch file:///tmp/ns-route53-record.json\n\n    aws route53 list-resource-record-sets --hosted-zone-id ${ZONE_ID} --query \"ResourceRecordSets[?Name == '${DOMAIN_NAME}.']\"\n}\n</code></pre></li> </ul>","tags":["aws/network/route53"]},{"location":"CLI/awscli/route53-cmd/#create-cname-record-","title":"create cname record-","text":"<p>refer: link  sample: acm-cmd</p>","tags":["aws/network/route53"]},{"location":"CLI/awscli/route53-cmd/#insert-txt-record-with-multi-values","title":"insert TXT record with multi values","text":"<pre><code>{\n    \"Comment\": \"Update record to add new TXT record\",\n    \"Changes\": [\n        {\n            \"Action\": \"UPSERT\",\n            \"ResourceRecordSet\": {\n                \"Name\": \"@.panlm.com.\",\n                \"Type\": \"TXT\",\n                \"TTL\": 300,\n                \"ResourceRecords\": [\n                    {\n                        \"Value\": \"\\\"test1=1\\\"\"\n                    },\n                    {\n                        \"Value\": \"\\\"test2=1\\\"\"\n                    }\n                ]\n            }\n        }\n    ]\n}\n</code></pre> <pre><code>aws route53 change-resource-record-sets \\\n  --hosted-zone-id Z07xxxxZD1 \\\n  --change-batch file://a.json\n</code></pre>","tags":["aws/network/route53"]},{"location":"CLI/awscli/route53-cmd/#refer","title":"refer","text":"<ul> <li>https://serverfault.com/questions/815841/multiple-txt-fields-for-same-subdomain?rq=1</li> <li>https://serverfault.com/questions/616407/tried-to-create-2-record-set-type-txt-in-route53</li> <li>https://www.learnaws.org/2022/02/04/aws-cli-route53-guide/</li> </ul>","tags":["aws/network/route53"]},{"location":"CLI/awscli/s3-cmd/","title":"s3","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#s3-cmd","title":"s3-cmd","text":"","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#versioning","title":"versioning","text":"","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#create-s3-with-versioning","title":"create s3 with versioning","text":"<pre><code>bucket_name=p1panlm\naws_region=us-east-2\naws s3api create-bucket --bucket $bucket_name \\\n--create-bucket-configuration LocationConstraint=${aws_region} \n\naws s3api put-bucket-versioning --bucket $bucket_name --versioning-configuration Status=Enabled\n</code></pre>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#delete-s3-with-versioning-enabled","title":"delete s3 with versioning enabled","text":"<pre><code>bucket_name=aws-codestar-ap-southeast-1-xxxxxx-proj1-pipe\naws s3api put-bucket-versioning --bucket $bucket_name --versioning-configuration Status=Suspended\naws s3api delete-objects \\\n      --bucket $bucket_name \\\n      --delete \"$(aws s3api list-object-versions \\\n      --bucket $bucket_name | \\\n      jq '{Objects: [.Versions[] | {Key:.Key, VersionId : .VersionId}], Quiet: false}')\"\n</code></pre>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#delete-s3-without-versioning","title":"delete s3 without versioning","text":"<pre><code>for i in $a ; do \naws s3 rm s3://$i --region us-east-1 --recursive\naws s3 rb s3://$i --force --region us-east-1\ndone\n\naws s3api delete-objects --bucket panlm-test-object-1234 --delete '{\"Objects\":[{\"Key\":\"def\"}]}'\n\n ```\n\n## download s3 folder\n\n```sh\naws s3 sync s3://my-exported-logs .\n</code></pre>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#create-folder","title":"create folder","text":"<pre><code>aws s3api put-object \\\n--bucket ${bucket_name} \\\n--key ${folder_name}/\n</code></pre>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#head-object","title":"head object","text":"<pre><code>aws s3api head-object --bucket lcf-1350 --key stop_sensor_data.sh\n\n# sample output: \n{\n    \"AcceptRanges\": \"bytes\",\n    \"Expiration\": \"expiry-date=\\\"Thu, 02 Nov 2023 00:00:00 GMT\\\", rule-id=\\\"rule1\\\"\",\n    \"LastModified\": \"2023-10-22T09:41:20+00:00\",\n    \"ContentLength\": 162,\n    \"ETag\": \"\\\"65c759947d7b4e98624fa5bec23e0df0\\\"\",\n    \"ContentType\": \"text/x-sh\",\n    \"ServerSideEncryption\": \"AES256\",\n    \"Metadata\": {}\n}\n</code></pre> <ul> <li>ETag, is md5</li> <li>Expiration, when you has rule for this object</li> <li>LastModified, only timestamp</li> </ul>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#get-object","title":"get object","text":"","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#get-object_1","title":"get object","text":"<pre><code>aws s3api get-object \\\n  --key results/15c2c468a4c4.txt \\\n  --bucket athena-bucket-1115 \\\n  --region us-east-2 \\\n  download.txt\n</code></pre>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#get-object-from-access-point","title":"get object from access point","text":"<pre><code># using access point alias\naws s3api get-object \\\n  --key results/15c2c468a4c4.txt \\\n  --bucket arn:aws:s3:us-east-2:ACCOUNT_ID:accesspoint/testap-internet \\\n  --region us-east-2 \\\n  download.txt\n</code></pre>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#update-access-point-policy","title":"update access point policy","text":"<pre><code>aws s3control get-access-point-policy \\\n  --region us-east-2 \\\n  --account-id ACCOUNT_ID \\\n  --name testap-internet\n</code></pre> <pre><code>aws s3control put-access-point-policy \\\n  --region us-east-2 \\\n  --account-id ACCOUNT_ID \\\n  --name testap-internet \\\n  --policy file://policy.json\n</code></pre>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#presigned-url","title":"presigned url","text":"","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#for-download","title":"for download","text":"<pre><code>OBJECT_KEY=\"folder/subfolder/file.txt\"\nEXPIRES=3600 # max 7 days\naws s3 presign s3://my-bucket/${OBJECT_KEY} --expires-in ${EXPIRES} --region xxx # keep region same with bucket\n</code></pre>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#for-upload","title":"for upload","text":"","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/s3-cmd/#public-access-","title":"public-access-","text":"<ul> <li>https://repost.aws/knowledge-center/read-access-objects-s3-bucket</li> <li>https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</li> </ul>","tags":["aws/storage/s3","aws/cmd"]},{"location":"CLI/awscli/sns-cmd/","title":"sns","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/integration/sns"]},{"location":"CLI/awscli/sns-cmd/#sns-cmd","title":"sns-cmd","text":"","tags":["aws/integration/sns"]},{"location":"CLI/awscli/sns-cmd/#send-file-as-message","title":"send file as message","text":"<pre><code>TOPIC_ARN=\naws sns publish --topic-arn ${TOPIC_ARN} --message file:///tmp/file1.json\n</code></pre>","tags":["aws/integration/sns"]},{"location":"CLI/awscli/sns-cmd/#subscription-confirmation","title":"subscription confirmation","text":"<pre><code>echo '{\n  \"Type\" : \"SubscriptionConfirmation\",\n  \"MessageId\" : \"xxx\",\n  ...\n  \"SigningCertURL\" : \"https://sns.ap-southeast-1.amazonaws.com/SimpleNotificationService-xxx.pem\"\n}' |tee /tmp/$$.json\n\nmessage_id=$( cat /tmp/$$.json |jq -r '.MessageId' )\narn=$( cat /tmp/$$.json |jq -r '.TopicArn' )\nregion=$(echo $arn |awk -F\":\" '{print $4}')\nurl=$( cat /tmp/$$.json |jq -r '.SubscribeURL' )\ntoken=$( cat /tmp/$$.json |jq -r '.Token')\n\naws sns confirm-subscription --topic-arn $arn --token $token --region $region\n</code></pre> <ul> <li> <p>output <pre><code>{\n    \"SubscriptionArn\": \"arn:aws:sns:ap-southeast-1:123456789012:notificate-to-panlm:xxx\"\n}\n</code></pre></p> </li> <li> <p>or <pre><code>curl -X POST -d @$$.json \\\n  -H 'Connection: Keep-Alive' \\\n  -H 'Content-Type: text/plain; charset=UTF-8' \\\n  -H 'x-amz-sns-message-type: SubscriptionConfirmation' \\\n  -H 'x-amz-sns-message-id: '\"$message_id\" \\\n  -H 'x-amz-sns-topic-arn: '\"$arn\" \\\n  \"$url\"\n</code></pre></p> </li> <li>output <pre><code>&lt;ConfirmSubscriptionResponse xmlns=\"http://sns.amazonaws.com/doc/2010-03-31/\"&gt;\n  &lt;ConfirmSubscriptionResult&gt;\n    &lt;SubscriptionArn&gt;arn:aws:sns:ap-southeast-1:123456789012:notificate-to-panlm:xxx&lt;/SubscriptionArn&gt;\n  &lt;/ConfirmSubscriptionResult&gt;\n  &lt;ResponseMetadata&gt;\n    &lt;RequestId&gt;xxx&lt;/RequestId&gt;\n  &lt;/ResponseMetadata&gt;\n&lt;/ConfirmSubscriptionResponse&gt;\n</code></pre></li> </ul>","tags":["aws/integration/sns"]},{"location":"CLI/awscli/sqs-cmd/","title":"sqs","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/integration/sqs"]},{"location":"CLI/awscli/sqs-cmd/#sqs-cmd","title":"sqs-cmd","text":"","tags":["aws/integration/sqs"]},{"location":"CLI/awscli/sqs-cmd/#send-and-receive-messages","title":"send and receive messages","text":"","tags":["aws/integration/sqs"]},{"location":"CLI/awscli/sqs-cmd/#with-curl","title":"with curl","text":"<pre><code># send message to sqs anonymously\nmessage=test\nsqs_url=https://sqs.ap-northeast-2.amazonaws.com/123456789012/public-queue-seoul\ncurl -d \"Action=SendMessage&amp;Version=2011-10-01&amp;MessageBody=${message}\" ${sqs_url}\n\n# ReceiveMessage\ncurl -d \"Action=ReceiveMessage&amp;Version=2011-10-01\" ${sqs_url}\n</code></pre>","tags":["aws/integration/sqs"]},{"location":"CLI/awscli/sqs-cmd/#with-awscli","title":"with awscli","text":"<pre><code># send 100 message to sqs\nfor i in `seq 1 99` ; do \n  aws sqs send-message  \\\n    --queue-url \"https://cn-northwest-1.queue.amazonaws.com.cn/123456789012/sqs-std1\" \\\n    --message-body \"message$i\"\ndone\n\n# retrieve 1 message\naws sqs receive-message \\\n  --queue-url https://cn-northwest-1.queue.amazonaws.com.cn/123456789012/sqs-std1\n\n\n# one message sample\n{\n    \"Messages\": [\n        {\n            \"Body\": \"aaaaa\",\n            \"ReceiptHandle\": \"===ReceiptHandleString===\",\n            \"MD5OfBody\": \"594f803b380a41396ed63dca39503542\",\n            \"MessageId\": \"d64a9969-c287-44e3-bac7-5025f61b5b39\"\n        }\n    ]\n}\n\n# delete message\naws sqs delete-message \\\n  --queue-url https://cn-northwest-1.queue.amazonaws.com.cn/123456789012/sqs-std1 \\\n  --receipt-handle \"===ReceiptHandleString===\"\n</code></pre>","tags":["aws/integration/sqs"]},{"location":"CLI/awscli/ssm-cmd/","title":"ssm","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#ssm-cmd","title":"ssm-cmd","text":"","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#ssm-agent","title":"ssm agent","text":"<pre><code>sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm\n</code></pre>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#test-connect-available-or-not","title":"test connect available or not","text":"<pre><code>aws ssm get-connection-status \\\n--target i-xxxx8c8d\n</code></pre>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#start-session","title":"start-session","text":"<pre><code>INST_ID=\naws ssm start-session --target  ${INST_ID} --region us-east-1\n</code></pre>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#port-forward","title":"port forward","text":"<ul> <li>https://aws.amazon.com/blogs/aws/new-port-forwarding-using-aws-system-manager-sessions-manager/</li> </ul> <pre><code>INST_ID=\naws ssm start-session --target ${INST_ID} \\\n--document-name AWS-StartPortForwardingSession \\\n--parameters '{\"localPortNumber\":[\"9999\"],\"portNumber\":[\"80\"]}' \n\n# target 81 \n# local 9999\n# curl http://localhost:9999\n</code></pre>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#port-forward-to-remote-host","title":"port forward to remote host","text":"<pre><code>INST_ID=\nREMOTE_HOST=\naws ssm start-session --target ${INST_ID} \\\n--document-name AWS-StartPortForwardingSessionToRemoteHost \\\n--parameters '{\n\"localPortNumber\":[\"9999\"],\n\"host\":[\"'\"${REMOTE_HOST}\"'\"],\n\"portNumber\":[\"443\"]\n}' \n</code></pre>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#send-command","title":"send-command","text":"<ul> <li>ssm-document-runshell</li> </ul>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#create-document-and-run-it","title":"create document and run it","text":"<ul> <li>https://aws.amazon.com/blogs/mt/amazon-ec2-systems-manager-documents-support-for-cross-platform-documents-and-multiple-steps-of-the-same-type/</li> </ul> <pre><code>aws ssm create-document --name step3demo --content file://a.json --document-type Command\n</code></pre> <pre><code># create automation document\nfor FILE in * ; do\n    aws ssm create-document \\\n    --content file://./${FILE} \\\n    --name ${FILE%%.json} \\\n    --document-type Automation\ndone\n</code></pre>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#filter-inventory-","title":"filter-inventory-","text":"<pre><code>aws ssm get-inventory --filter Key=\"Custom:DiskUtilization.Size(GB)\",Values=100,Type=Equal\naws ssm get-inventory --filter Key=Custom:DiskUtilization.Use%,Values=60,Type=GreaterThan\n</code></pre>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#get-parameter","title":"get parameter","text":"<ul> <li>../../EKS/cluster/eks-public-access-cluster</li> <li>ssm-public-parameters</li> </ul>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#join-domain-sample","title":"join domain sample","text":"right-click &amp; open-in-new-tab:  <ul> <li>example <pre><code>aws ssm send-command --document-name \"AWS-JoinDirectoryServiceDomain\" --document-version \"1\" --targets '[{\"Key\":\"InstanceIds\",\"Values\":[\"i-0e23xxxx8bdc6xxxx\"]}]' --parameters '{\"directoryOU\":[\"\"],\"directoryId\":[\"d-9axxxxe3cf\"],\"directoryName\":[\"xxxx.aws.panlm.xyz\"],\"dnsIpAddresses\":[\"172.31.xx.xx\",\"172.31.xx.xx\"]}' --timeout-seconds 600 --max-concurrency \"50\" --max-errors \"0\" --region us-east-2\n</code></pre></li> </ul>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/ssm-cmd/#create-ssm-vpc-endpoint","title":"create ssm vpc endpoint","text":"<p><pre><code>export AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\nexport AWS_PAGER=''\n\n# get cloud9 vpc\nC9_INST_ID=$(curl http://169.254.169.254/1.0/meta-data/instance-id 2&gt;/dev/null)\nC9_VPC_ID=$(aws ec2 describe-instances \\\n--instance-ids ${C9_INST_ID} \\\n--query 'Reservations[0].Instances[0].VpcId' --output text)\n\n# get public subnet \nC9_SUBNETS_ID=$(aws ec2 describe-subnets \\\n--filter \"Name=vpc-id,Values=${C9_VPC_ID}\" \\\n--query 'Subnets[?MapPublicIpOnLaunch==`true`].SubnetId' \\\n--output text)\n\n# get default security group \nC9_DEFAULT_SG_ID=$(aws ec2 describe-security-groups \\\n--filter Name=vpc-id,Values=${C9_VPC_ID} \\\n--query \"SecurityGroups[?GroupName == 'default'].GroupId\" \\\n--output text)\n\n# allow 80/443 from anywhere\nfor i in 80 443 ; do\naws ec2 authorize-security-group-ingress \\\n  --group-id ${C9_DEFAULT_SG_ID} \\\n  --protocol tcp \\\n  --port $i \\\n  --cidr 0.0.0.0/0  \ndone\n\n# ssm ssmmessages\nfor i in ssm ssmmessages ; do\naws ec2 create-vpc-endpoint \\\n--vpc-id ${C9_VPC_ID} \\\n--vpc-endpoint-type Interface \\\n--service-name com.amazonaws.${AWS_DEFAULT_REGION}.${i} \\\n--subnet-ids ${C9_SUBNETS_ID} \\\n--security-group-id ${C9_DEFAULT_SG_ID} \ndone\n</code></pre> ^ssm-vpce-0513</p>","tags":["aws/cmd","aws/mgmt/systems-manager"]},{"location":"CLI/awscli/vpc-cmd/","title":"vpc","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#vpc-cmd","title":"vpc-cmd","text":"","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#vpc-creation","title":"vpc creation","text":"<ul> <li> dns hostname \u2013&gt; true</li> </ul> <pre><code># 10-128-vpc\nCIDR=10.128\nVPC_NAME=vpc-$(echo ${CIDR} |tr '.' '-')\nexport AWS_DEFAULT_REGION=cn-north-1\n\n# first 2 AZs\nAZS=($(aws ec2 describe-availability-zones --query 'AvailabilityZones[].ZoneName' --output text |xargs -n 1 |sed -n '1,2p' |xargs))\n\n# create vpc\nVPC_ID=$(aws ec2 create-vpc --cidr-block ${CIDR}.0.0/16 --tag-specifications 'ResourceType=vpc,Tags=[{Key=Name,Value='\"${VPC_NAME}\"'}]' --query Vpc.VpcId --output text)\naws ec2 modify-vpc-attribute --enable-dns-hostnames --vpc-id ${VPC_ID} \n\n# create subnet 2x pub, 2x priv, 2x tgw\nnum=0\nfor j in pub priv tgw ; do\n    for i in ${AZS[@]}; do\n        output=$j.$(date +%H%M%S)\n        aws ec2 create-subnet --vpc-id ${VPC_ID} \\\n        --cidr-block ${CIDR}.${num}.0/22 \\\n        --availability-zone ${i} \\\n        --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value='\"${VPC_NAME}\"'-'\"${j}\"'-'\"${i##*-}\"'}]' |tee -a ${output}\n        num=$((num+4))\n    done\ndone\n\n# create igw\nIGW_ID=$( aws ec2 create-internet-gateway --tag-specifications 'ResourceType=internet-gateway,Tags=[{Key=Name,Value='\"${VPC_NAME}\"'-igw}]' --query InternetGateway.InternetGatewayId --output text )\naws ec2 attach-internet-gateway --vpc-id ${VPC_NAME} --internet-gateway-id ${IGW_ID}\n</code></pre>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#subnet-find","title":"subnet find","text":"<pre><code>aws ec2 describe-subnets --filters \"Name=vpc-id,Values=${ovpc1_id}\" \\\n  --query \"Subnets[*].{ID:SubnetId,CIDR:CidrBlock}\"\n\naws ec2 describe-subnets --filters \"Name=vpc-id,Values=${ovpc1_id}\" \\\n  --query \"Subnets[*]\" |jq -r '.[].SubnetId'\n</code></pre>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#first-2-subnets","title":"first 2 subnets","text":"<pre><code>FIRST_2AZ=$(aws ec2 describe-availability-zones --query 'AvailabilityZones[].ZoneName' --output text |awk '{print $1,$2}')\nSUBNET_IDS=$(for i in ${FIRST_2AZ}; do \n    aws ec2 describe-subnets \\\n        --filters \"Name=vpc-id,Values=${VPC_ID}\" \\\n        --query 'Subnets[?AvailabilityZone==`'\"${i}\"'`].SubnetId' \\\n        --output text\ndone |xargs)\n</code></pre> <pre><code>FIRST_2AZ=$(aws ec2 describe-availability-zones --query 'AvailabilityZones[].ZoneName' --output text |awk '{print $1,$2}')\nSUBNET_IDS=$(for i in ${FIRST_2AZ}; do \naws ec2 describe-subnets \\\n--filters \"Name=vpc-id,Values=${VPC_ID}\" \\\n--query 'Subnets[?(AvailabilityZone==`'\"${i}\"'` &amp;&amp; MapPublicIpOnLaunch==`true`)].SubnetId' \\\n--output text\ndone |xargs)\n</code></pre>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#list-subnet-in-table","title":"list subnet in table","text":"<pre><code>aws ec2 describe-subnets --filters \"Name=vpc-id,Values=${VPC_ID}\" \\\n  --query \"Subnets[].[AvailabilityZone,SubnetId]\" --output text\n</code></pre> <ul> <li>refer: ../../EKS/addons/eks-custom-network</li> </ul>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#createdelete-transit-gateway","title":"create/delete transit gateway","text":"","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#create","title":"create","text":"<pre><code>aws ec2 create-transit-gateway \\\n  --tag-specifications 'ResourceType=transit-gateway,Tags=[{Key=Name,Value=otgw1}]' \\\n  --query TransitGateway.TransitGatewayId --output text\n</code></pre>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#delete-transit-gateway","title":"delete transit gateway","text":"<pre><code>aws ec2 describe-transit-gateway-attachments |jq -r .TransitGatewayAttachments[].TransitGatewayAttachmentId\naws ec2 delete-transit-gateway-vpc-attachment --transit-gateway-attachment-id tgw-attach-012c31682d0c11f22\n</code></pre>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#vpc-subnet","title":"vpc &amp; subnet","text":"<pre><code>aws ec2 create-subnet --cidr-block 10.1.0.0/20 --vpc-id vpc-xxx --availability-zone-id cnnw1-az1\naws ec2 create-subnet --cidr-block 10.1.16.0/20 --vpc-id vpc-xxx --availability-zone-id cnnw1-az2\naws ec2 create-subnet --cidr-block 10.1.32.0/20 --vpc-id vpc-xxx --availability-zone-id cnnw1-az3\n\n\naws ec2 create-subnet --cidr-block 10.2.0.0/20 --vpc-id vpc-xxx --availability-zone-id cnnw1-az1\naws ec2 create-subnet --cidr-block 10.2.16.0/20 --vpc-id vpc-xxx --availability-zone-id cnnw1-az2\naws ec2 create-subnet --cidr-block 10.2.32.0/20 --vpc-id vpc-xxx --availability-zone-id cnnw1-az3\n</code></pre>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#route-table","title":"route table","text":"","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#peering","title":"peering","text":"<pre><code>CLUSTER_NAME=ekscluster2\nTARGET_CIDR='10.251.0.0/16'\nPEER_ID=pcx-xxx\n\nVPC_ID=$(aws eks describe-cluster \\\n  --name ${CLUSTER_NAME} \\\n  --query \"cluster.resourcesVpcConfig.vpcId\" \\\n  --output text)\n\nROUTE_TABLES=($(aws ec2 describe-route-tables \\\n  --filters \"Name=vpc-id,Values=${VPC_ID}\" \"Name=association.main,Values=false\" \\\n  --query \"RouteTables[].RouteTableId\" \\\n  --output text))\n\nfor i in ${ROUTE_TABLES[@]}; do\n  aws ec2 create-route --route-table-id $i \\\n    --destination-cidr-block ${TARGET_CIDR} \\\n    --vpc-peering-connection-id ${PEER_ID}\ndone\n</code></pre>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#tgw","title":"tgw","text":"","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#cidr-range","title":"cidr range","text":"RFC 1918 range Example CIDR block 10.0.0.0 - 10.255.255.255 (10/8 prefix) 10.0.0.0/16 172.16.0.0 - 172.31.255.255 (172.16/12 prefix) 172.31.0.0/16 192.168.0.0 - 192.168.255.255 (192.168/16 prefix) 192.168.0.0/20","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#func-get-default-vpc-","title":"func-get-default-vpc-","text":"func-get-default-vpc<pre><code>function get-default-vpc () {\n    DEFAULT_VPC=$(aws ec2 describe-vpcs --filter Name=is-default,Values=true --query 'Vpcs[0].VpcId' --output text)\n    DEFAULT_CIDR=$(aws ec2 describe-vpcs --filter Name=is-default,Values=true --query 'Vpcs[0].CidrBlock' --output text)\n}\n</code></pre>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#func-get-subnets-","title":"func-get-subnets-","text":"func-get-subnets<pre><code>function get-subnets () {\n    if [[ $# -lt 1 ]]; then\n        echo \"format: $0 VPC_ID [true|false]\"\n        echo \"parameter 2: true for public, false for private\"\n        return\n    else\n        local VPC_ID=$1\n        local IS_PUBLIC=$(echo $2 |tr 'A-Z' 'a-z') # lower case $2\n    fi\n\n    if [[ -z ${IS_PUBLIC} ]]; then\n        IS_PUBLIC=true\n    fi\n\n    if [[ ${IS_PUBLIC} == 'true' || ${IS_PUBLIC} == 'false' ]]; then\n        echo \"get public subnet is: \" ${IS_PUBLIC}\n    else\n        echo \"parameter 2: true for public, false for private\"\n        return\n    fi\n\n    SUBNET_IDS=$(aws ec2 describe-subnets \\\n        --filter \"Name=vpc-id,Values=${VPC_ID}\" \\\n        --query 'Subnets[?MapPublicIpOnLaunch==`'\"${IS_PUBLIC}\"'`].SubnetId' \\\n        --output text)\n}\n</code></pre> <p>also see sample in ../../EKS/cluster/eks-private-access-cluster</p>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#get-instance-vpc","title":"get instance vpc","text":"<pre><code>VPC_ID=$(aws ec2 describe-instances --instance-ids ${INST_ID} --query 'Reservations[0].Instances[0].VpcId' --output text)\n</code></pre>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/awscli/vpc-cmd/#create-vpc-endpoint","title":"create vpc endpoint","text":"<p>another sample: ssm-cmd another sample: TC-private-apigw-dataflow</p>","tags":["aws/network/vpc","aws/cmd","todo"]},{"location":"CLI/functions/func-create-sg.sh/","title":"func-create-sg.sh","text":"func-create-sg<pre><code># deps: VPC_ID\n# output: SG_ID\n# format: create-sg -v VPC_ID -c VPC_CIDR [-p PORT1] [-p PORT2]\nfunction create-sg () {\n    OPTIND=1\n    OPTSTRING=\"h?v:c:p:\"\n    local VPC_ID=\"\"\n    local VPC_CIDR=\"\"\n    local PORTS=()\n    while getopts ${OPTSTRING} opt; do\n        case \"${opt}\" in\n            v) VPC_ID=${OPTARG} ;;\n            c) VPC_CIDR=${OPTARG} ;;\n            p) PORTS+=(\"${OPTARG}\") ;;\n            h|\\?) \n                echo \"format: create-sg -v VPC_ID -c VPC_CIDR [-p PORT1] [-p PORT2]\"\n                echo -e \"\\tsample: create-sg -v vpc-xxx -p 172.31.0.0/16\"\n                echo -e \"\\tsample: create-sg -v vpc-xxx -p 0.0.0.0/0 -p 80 -p 443\"\n                return 0\n            ;;\n        esac\n    done\n    : ${VPC_ID:?Missing -v}\n    : ${VPC_CIDR:?Missing -c}\n\n    # create sg\n    SG_NAME=mysg-$(TZ=EAT-8 date +%Y%m%d-%H%M%S)\n    SG_ID=$(aws ec2 create-security-group \\\n        --description ${SG_NAME} \\\n        --group-name ${SG_NAME} \\\n        --vpc-id ${VPC_ID} \\\n        --query 'GroupId' --output text )\n\n    # all traffic allowed\n    for i in ${PORTS[@]:--1}; do\n        aws ec2 authorize-security-group-ingress \\\n            --group-id ${SG_ID} \\\n            --protocol tcp \\\n            --port ${i} \\\n            --cidr ${VPC_CIDR}\n    done\n}\n</code></pre>","tags":["aws/cmd","bash/function"]},{"location":"CLI/linux/assume-tool/","title":"assume-tool","text":"<p>[!WARNING] This is a github note</p>","tags":["cmd","aws/security/iam"]},{"location":"CLI/linux/assume-tool/#assume-tool","title":"assume-tool","text":"","tags":["cmd","aws/security/iam"]},{"location":"CLI/linux/assume-tool/#create-role-for-account-to-assume","title":"create role for account to assume","text":"func-create-aws-config-entity<pre><code>function create-aws-config-entity () {\necho WS_NAME=$(TZ=EAT-8 date +%Y%m%d-%H%M)\nACCOUNT_ID=$(GRANTED_QUIET=true . assume panlm --exec \"aws sts get-caller-identity\" |jq -r '.Account')\nLOCAL_ACCOUNT_ID=$(aws sts get-caller-identity |jq -r '.Account')\nROLE_NAME=panlm # easy for remeber and switch from WSParticipantRole\nenvsubst &gt; /tmp/${ROLE_NAME}-trust.json &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": [\n                    \"arn:aws:iam::${ACCOUNT_ID}:root\",\n                    \"arn:aws:iam::${LOCAL_ACCOUNT_ID}:root\"\n                ]\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {}\n        }\n    ]\n}\nEOF\naws iam create-role --role-name ${ROLE_NAME} \\\n    --assume-role-policy-document file:///tmp/${ROLE_NAME}-trust.json \\\n    --max-session-duration 43200 |tee /tmp/${ROLE_NAME}-role.json\naws iam attach-role-policy --role-name ${ROLE_NAME} \\\n    --policy-arn \"arn:aws:iam::aws:policy/AdministratorAccess\"\nROLE_ARN=$(cat /tmp/${ROLE_NAME}-role.json |jq -r '.Role.Arn')\n\nCREDENTIAL_ENTITY_NAME=\"0-ws-${WS_NAME}\"\nenvsubst &gt;&gt; ~/.aws/config &lt;&lt;-EOF\n[profile $CREDENTIAL_ENTITY_NAME]\nrole_arn=${ROLE_ARN}\nsource_profile=panlm\nrole_session_name=granted\nregion=us-east-2\nEOF\necho ${CREDENTIAL_ENTITY_NAME}\n}\n</code></pre> <p>^0de4c9</p>","tags":["cmd","aws/security/iam"]},{"location":"CLI/linux/assume-tool/#modify-role-for-account-to-assume-","title":"modify role for account to assume-","text":"<ul> <li>(deprecated due to new SCP policy do not allow to modify WSParticipantRole\u2019s trust policy)</li> <li>login from macbook CLI</li> <li>modify existed role for login - WSParticipantRole</li> <li>create aws credential entities</li> </ul> <pre><code>assume panlm\n\necho ${AWS_ACCESS_KEY_ID} \necho ${AWS_SECRET_ACCESS_KEY}\necho ${AWS_SESSION_TOKEN}\n\naws sts get-caller-identity\n</code></pre> <pre><code>echo ${WS_NAME:=$(TZ=EAT-8 date +%Y%m%d)}\nACCOUNT_ID=$(GRANTED_QUIET=true . assume panlm --exec \"aws sts get-caller-identity\" |jq -r '.Account')\nROLE_NAME=\"WSParticipantRole\"\n\nTEMP=$(mktemp)\naws iam get-role --role-name ${ROLE_NAME} --output json &gt; ${TEMP}.1\ncat ${TEMP}.1 |jq '.Role.AssumeRolePolicyDocument.Statement[0].Principal.AWS += [\"arn:aws:iam::'\"${ACCOUNT_ID}\"':root\"]' |jq -r '.Role.AssumeRolePolicyDocument' |tee ${TEMP}.2\n\naws iam update-assume-role-policy --role-name ${ROLE_NAME} \\\n  --policy-document file://${TEMP}.2\naws iam attach-role-policy --role-name ${ROLE_NAME} \\\n  --policy-arn \"arn:aws:iam::aws:policy/AdministratorAccess\"\nROLE_ARN=$(cat ${TEMP}.1 |jq -r '.Role.Arn')\n\nCREDENTIAL_ENTITY_NAME=\"0-ws-${WS_NAME}\"\necho '['\"$CREDENTIAL_ENTITY_NAME\"']' &gt;&gt; ~/.aws/credentials\necho 'role_arn='${ROLE_ARN} &gt;&gt; ~/.aws/credentials\necho 'source_profile=panlm' &gt;&gt; ~/.aws/credentials\necho 'role_session_name=granted' &gt;&gt; ~/.aws/credentials\necho 'region=us-east-2' &gt;&gt; ~/.aws/credentials\necho ''\necho ${CREDENTIAL_ENTITY_NAME}\n</code></pre> <p>^b98085</p>","tags":["cmd","aws/security/iam"]},{"location":"CLI/linux/assume-tool/#install","title":"install","text":"<ul> <li>https://docs.commonfate.io/granted/getting-started#installing-the-cli <pre><code>VERSION=0.20.3\ncurl -OL https://releases.commonfate.io/granted/v${VERSION}/granted_${VERSION}_darwin_x86_64.tar.gz\ntar -zxvf ./granted_${VERSION}_darwin_x86_64.tar.gz -C /usr/local/bin/\n\nrm -f /usr/local/bin/assumego\nln -s /usr/local/bin/granted /usr/local/bin/assumego\n</code></pre></li> </ul>","tags":["cmd","aws/security/iam"]},{"location":"CLI/linux/assume-tool/#import-existed-credentials-to-mac-login-keychain","title":"import existed credentials to mac <code>login</code> keychain","text":"<pre><code>granted credentials import example\n</code></pre>","tags":["cmd","aws/security/iam"]},{"location":"CLI/linux/assume-tool/#export-to-aws-credentials-file","title":"export to aws credentials file","text":"<ul> <li>This command can be used to return your credentials to the original insecure plaintext format in the AWS credentials file. <pre><code>granted credentials export-plaintext example\n</code></pre></li> </ul>","tags":["cmd","aws/security/iam"]},{"location":"CLI/linux/assume-tool/#refer","title":"refer","text":"<ul> <li>https://docs.commonfate.io/granted/introduction</li> <li>https://awsu.me/</li> </ul>","tags":["cmd","aws/security/iam"]},{"location":"CLI/linux/docker-cmd/","title":"docker","text":"<p>[!WARNING] This is a github note</p>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#docker-cmd","title":"docker cmd","text":"","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#docker-build","title":"docker build","text":"<pre><code>docker build --tag panlm/ntnx:app2 .\ndocker run -d --name app2 -p 5000:5000 app2\ndocker login -u panlm\ndocker push panlm/ntnx:app2\n</code></pre> <pre><code>docker pull panlm/ntnx:app2\ndocker run -d --name app2 -p 5000:5000 panlm/ntnx:app2\n\ndocker exec -it app2 /bin/bash\n</code></pre>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#docker-buildx-","title":"docker-buildx-","text":"<ul> <li>download binary from link <pre><code>mkdir -p ~/.docker/cli-plugins\nmv &lt;buildx&gt; ~/.docker/cli-plugins/docker-buildx\nchmod a+x ~/.docker/cli-plugins/docker-buildx\n</code></pre></li> <li>check qemu emulators <pre><code>docker buildx ls\ndocker run -it --rm --privileged tonistiigi/binfmt --install all\n</code></pre></li> <li>build multi arch <pre><code>docker buildx build \\\n--platform linux/amd64,linux/arm64 \\\n-t 123456789012.dkr.ecr.us-east-2.amazonaws.com/osarch:latest \\\n--push .\n</code></pre></li> <li>refer: ../../../../WebClip/How to quickly setup an experimental environment to run containers on x86 and AWS Graviton2 based Amazon EC2 instances  AWS Compute Blog</li> </ul>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#docker-image","title":"docker image","text":"","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#clean","title":"clean","text":"<pre><code>docker image prune -a\n</code></pre>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#samples","title":"samples","text":"","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#flask-app","title":"flask app","text":"<ul> <li> <p>python_app.py <pre><code>#!/usr/bin/env python3\nfrom flask import Flask\nimport os\n\napp = Flask(__name__)\n@app.route('/')\ndef index():\n    return f\"{{ OS Architecture: {os.uname().machine} }}\"\n\nif __name__ == '__main__':\n    app().run(host='0.0.0.0')\n</code></pre></p> </li> <li> <p>Dockerfile <pre><code>FROM public.ecr.aws/bitnami/python:3.7\nEXPOSE 5000\nWORKDIR /\nCOPY ./python_app.py /app.py\nRUN pip install flask\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\"]\n</code></pre></p> </li> <li>create repo ../awscli/ecr-cmd</li> <li> <p>build <pre><code>echo ${REPO_URI}\ndocker build -t ${REPO_URI}:latest .\ndocker push ${REPO_URI}:latest\n</code></pre></p> </li> <li> <p>docker run for testing <pre><code>docker run --rm -d -p 8080:5000 --name osarch ${REPO_URI}:latest\n</code></pre></p> </li> </ul>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#build-colorapp-","title":"build-colorapp-","text":"<ul> <li> <p>v1 <pre><code>export AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document |jq -r '.region')\nexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nECR_URL=${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\n\naws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin ${ECR_URL}\n\ncd /tmp\ngit clone https://github.com/sanjeevrg89/samplecolorapp.git\ncd samplecolorapp\n\nPROJ_NAME=sample\nAPP_NAME=colorapp\nECR_IMAGE_NAME=${ECR_URL}/${PROJ_NAME}/${APP_NAME}\naws ecr create-repository \\\n--repository-name ${PROJ_NAME}/${APP_NAME}\ndocker build . -t ${ECR_IMAGE_NAME}:v1\ndocker push ${ECR_IMAGE_NAME}:v1\n</code></pre></p> </li> <li> <p>v2 <pre><code>cd v2\ndocker build . -t ${ECR_IMAGE_NAME}:v2\ndocker push ${ECR_IMAGE_NAME}:v2\n</code></pre></p> </li> <li> <p>refer: https://github.com/sanjeevrg89/samplecolorapp</p> </li> </ul>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#ubuntu-based","title":"ubuntu-based","text":"<pre><code>FROM python:2.7\n#RUN [\"apt-get\", \"update\"]\n#RUN [\"apt-get\", \"install\", \"-y\", \"vim\"]\nCOPY . /app\nWORKDIR /app\nRUN pip install -r requirements.txt\nEXPOSE 5000\nCMD python ./app.py\n</code></pre>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#centos-based","title":"centos-based","text":"<pre><code>FROM centos:7\nRUN [\"yum\", \"install\", \"-y\", \"epel-release\", \"gcc\", \"python-devel\", \"python2-pip\"]\nRUN [\"rpm\", \"-Uvh\", \"https://repo.mysql.com/mysql80-community-release-el7-3.noarch.rpm\"]\nRUN [\"yum\", \"install\", \"-y\", \"--enablerepo=mysql80-community\", \"mysql-community-devel\"]\nCOPY . /app\nWORKDIR /app\nRUN pip install -r requirements.txt\nEXPOSE 5000\nCMD python ./app.py\n</code></pre>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#docker-run","title":"docker run","text":"<ul> <li>docker-run</li> </ul>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#docker-push","title":"docker push","text":"<ul> <li>https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</li> </ul>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/docker-cmd/#docker-environment-variable","title":"docker environment variable","text":"<p>https://phoenixnap.com/kb/docker-environment-variables</p> <p>ENV SPARK_VERSION=3.3.3</p>","tags":["docker","linux","cmd"]},{"location":"CLI/linux/eksctl/","title":"eksctl","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#eksctl-cmd","title":"eksctl-cmd","text":"","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#install","title":"install","text":"<pre><code># install eksctl\n# consider install eksctl version 0.89.0\n# if you have older version yaml \n# https://eksctl.io/announcements/nodegroup-override-announcement/\ncurl --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv -v /tmp/eksctl /usr/local/bin\neksctl completion bash &gt;&gt; ~/.bash_completion\n. /etc/profile.d/bash_completion.sh\n. ~/.bash_completion\n</code></pre> <p>refer: git/git-mkdocs/cloud9/setup-cloud9-for-eks </p>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#iamidentitymapping","title":"iamidentitymapping","text":"<pre><code>eksctl get iamidentitymapping \\\n--cluster ekscluster1 \\\n--region us-east-2\n\neksctl create iamidentitymapping \\\n--cluster ekscluster1 \\\n--region us-east-2 \\\n--arn arn:aws:iam::xxxx:role/eksadmin \\\n--group system:masters \\\n--username admin \n</code></pre>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#create-windows-self-managed-node-group","title":"create windows self-managed node group","text":"<pre><code>cluster_name=myeksctl\nregion_name=ap-southeast-1\neksctl create nodegroup   \\\n  --region $region_name   \\\n  --cluster $cluster_name   \\\n  --name nodegroup-win-1   \\\n  --node-type m5.xlarge   \\\n  --nodes 1   \\\n  --nodes-min 1   \\\n  --nodes-max 2 \\\n  --node-ami-family WindowsServer2019FullContainer \\\n  --managed=false\n</code></pre> <pre><code>cluster_name=myeksctl\nregion_name=ap-southeast-1\neksctl create nodegroup   \\\n  --region $region_name   \\\n  --cluster $cluster_name   \\\n  --name nodegroup-br-1   \\\n  --node-type m5.xlarge   \\\n  --nodes 1   \\\n  --nodes-min 1   \\\n  --nodes-max 2 \\\n  --node-ami-family Bottlerocket\n</code></pre>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#create-nodegroup","title":"create nodegroup","text":"<pre><code>cluster_name=ekscluster2\nregion_name=us-east-1\neksctl create nodegroup   \\\n  --region $region_name   \\\n  --cluster $cluster_name   \\\n  --name mng-a   \\\n  --node-type m5.large   \\\n  --nodes 2   \\\n  --node-private-networking \\\n  --subnet-ids subnet-aaa,subnet-bbb\n</code></pre>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#scale-nodegroup","title":"scale nodegroup","text":"<pre><code>CLUSTER_NAME=ekscluster1\nNODEGROUP_NAME=managed-ng\nAWS_REGION=us-east-2\n\neksctl scale nodegroup \\\n--cluster=${CLUSTER_NAME} \\\n--region ${AWS_REGION} \\\n--nodes=3 \\\n${NODEGROUP_NAME}\n</code></pre>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#func-create-iamserviceaccount-","title":"func-create-iamserviceaccount-","text":"func-create-iamserviceaccount<pre><code>echo ${CLUSTER_NAME}\necho ${NAMESPACE_NAME}\n\nfunction create-iamserviceaccount () {\n    OPTIND=1\n    OPTSTRING=\"h?s:c:n:r:\"\n    local SA_NAME=\"\"\n    local CLUSTER_NAME=\"\"\n    local NAMESPACE_NAME=\"\"\n    local ROLE_ONLY=\"\"\n    while getopts ${OPTSTRING} opt; do\n        case \"${opt}\" in\n            s) SA_NAME=${OPTARG} ;;\n            c) CLUSTER_NAME=${OPTARG} ;;\n            n) NAMESPACE_NAME=${OPTARG} ;;\n            r) ROLE_ONLY=${OPTARG} ;;\n            h|\\?) \n                echo \"format: create-iamserviceaccount -s SERVICE_ACCOUNT_NAME -c CLUSTER_NAME -n NAMESPACE_NAME -r [0|1] \"\n                echo -e \"\\tsample: create-iamserviceaccount -s sa_name -c ekscluster1 -n monitoring -r 1 \"\n                return 0\n            ;;\n        esac\n    done\n    : ${SA_NAME:?Missing -s}\n    : ${CLUSTER_NAME:?Missing -c}\n    : ${NAMESPACE_NAME:?Missing -n}\n    : ${ROLE_ONLY:?Missing -r}\n\n    if [[ ROLE_ONLY -eq 1 ]]; then\n        local ROLE_OPTION=\"--role-only\"\n    else\n        local ROLE_OPTION=\"\"\n    fi\n\n    echo ${SA_NAME:=sa-s3-admin-$(TZ=EAT-8 date +%Y%m%d-%H%M%S)}\n    eksctl create iamserviceaccount -c ${CLUSTER_NAME} \\\n        --name ${SA_NAME} --namespace ${NAMESPACE_NAME} \\\n        --attach-policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess \\\n        --role-name ${SA_NAME}-$(TZ=EAT-8 date +%Y%m%d-%H%M%S) ${ROLE_OPTION} --approve \\\n        --override-existing-serviceaccounts\n    unset S3_ADMIN_ROLE_ARN\n    S3_ADMIN_ROLE_ARN=$(eksctl get iamserviceaccount -c $CLUSTER_NAME \\\n        --name ${SA_NAME} -o json |jq -r '.[].status.roleARN')\n    echo ${S3_ADMIN_ROLE_ARN}\n}\n</code></pre>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#appmesh-cluster","title":"appmesh cluster","text":"<pre><code>eksctl create cluster \\\n--name appmeshtest \\\n--nodes-min 2 \\\n--nodes-max 3 \\\n--nodes 2 \\\n--auto-kubeconfig \\\n--full-ecr-access \\\n--appmesh-access\n</code></pre>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#appmesh-access-","title":"appmesh-access-","text":"<p><code>--appmesh-access</code> will apply customer inline policy for appmesh</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"servicediscovery:CreateService\",\n                \"servicediscovery:DeleteService\",\n                \"servicediscovery:GetService\",\n                \"servicediscovery:GetInstance\",\n                \"servicediscovery:RegisterInstance\",\n                \"servicediscovery:DeregisterInstance\",\n                \"servicediscovery:ListInstances\",\n                \"servicediscovery:ListNamespaces\",\n                \"servicediscovery:ListServices\",\n                \"servicediscovery:GetInstancesHealthStatus\",\n                \"servicediscovery:UpdateInstanceCustomHealthStatus\",\n                \"servicediscovery:GetOperation\",\n                \"route53:GetHealthCheck\",\n                \"route53:CreateHealthCheck\",\n                \"route53:UpdateHealthCheck\",\n                \"route53:ChangeResourceRecordSets\",\n                \"route53:DeleteHealthCheck\",\n                \"appmesh:*\"\n            ],\n            \"Resource\": \"*\",\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n</code></pre>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#full-ecr-access","title":"full-ecr-access","text":"<p><code>--full-ecr-access</code> will apply ECR power user policy to node</p> <ul> <li>and others </li> </ul>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksctl/#refer","title":"refer","text":"<ul> <li>https://eksctl.io/usage/minimum-iam-policies/</li> </ul>","tags":["aws/container/eks"]},{"location":"CLI/linux/eksdemo/","title":"eksdemo","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","aws/cmd"]},{"location":"CLI/linux/eksdemo/#eksdemo","title":"eksdemo","text":"","tags":["aws/container/eks","aws/cmd"]},{"location":"CLI/linux/eksdemo/#install","title":"install","text":"<pre><code>curl --location \"https://github.com/awslabs/eksdemo/releases/latest/download/eksdemo_$(uname -s)_x86_64.tar.gz\" |tar xz -C /tmp\nsudo mv -v /tmp/eksdemo /usr/local/bin\n</code></pre>","tags":["aws/container/eks","aws/cmd"]},{"location":"CLI/linux/eksdemo/#create-eks-cluster-","title":"create eks cluster-","text":"<pre><code>CLUSTER_NAME=ekscluster2\neksdemo create cluster ${CLUSTER_NAME} -i m5.large -N 3\n</code></pre>","tags":["aws/container/eks","aws/cmd"]},{"location":"CLI/linux/eksdemo/#addons-","title":"addons-","text":"<ul> <li>externaldns </li> </ul> <p>refer: ../../EKS/addons/externaldns-for-route53</p> <ul> <li>aws load balancer controller </li> </ul> <p>refer: ../../EKS/addons/aws-load-balancer-controller</p> <ul> <li>2 certificates, one for each domain name in original region </li> </ul> <p>refer: ../awscli/acm-cmd</p>","tags":["aws/container/eks","aws/cmd"]},{"location":"CLI/linux/iptables/","title":"iptables","text":"<p>[!WARNING] This is a github note</p>","tags":["cmd","linux"]},{"location":"CLI/linux/iptables/#iptables","title":"iptables","text":"","tags":["cmd","linux"]},{"location":"CLI/linux/iptables/#example","title":"example","text":"<ul> <li>git/git-mkdocs/CLI/linux/linux-cmd</li> <li>fake-waf-on-ec2-forwarding-https</li> <li>git/git-mkdocs/others/cross-region-reverse-proxy-with-nlb-cloudfront</li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/iptables/#doc","title":"doc","text":"<ul> <li>https://www.frozentux.net/iptables-tutorial/cn/iptables-tutorial-cn-1.1.19.html#TRAVERSINGOFTABLES</li> <li>https://www.digitalocean.com/community/tutorials/a-deep-dive-into-iptables-and-netfilter-architecture#iptables-and-connection-tracking</li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/iptables/#compare-with-nginx","title":"compare with nginx","text":"","tags":["cmd","linux"]},{"location":"CLI/linux/jq-cmd/","title":"jq","text":"<p>[!WARNING] This is a github note</p>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#how-to-use-jq","title":"how-to-use-jq","text":"","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#reference","title":"reference","text":"<ul> <li>https://programminghistorian.org/en/lessons/json-and-jq</li> <li>https://jqplay.org/</li> <li>https://stedolan.github.io/jq/manual/#Invokingjq</li> <li>https://ubuntu.com/blog/improving-cli-output-with-jq</li> </ul>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#sample","title":"sample","text":"","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#snapshot-sort","title":"snapshot sort","text":"<p>refer git/git-mkdocs/CLI/awscli/ebs-cmd</p> <p></p>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#select","title":"select","text":"<pre><code>t = pyjq.all('.entities[] | select (.uuid==\\\"' + i + '\\\") ', full)\n</code></pre>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#output","title":"output","text":"<pre><code>jq -r '[...]|@csv' |column -t -s','\njq -r '[...]|@tsv' |column -t \n</code></pre> <pre><code># add table head\naz resource list |jq -r '\\\n[\"name\",\"resgrp\"], [\"--\",\"--\"], \\\n(.[] | [.name, .resourceGroup])|@tsv' |column -t\n\naz resource list |jq -r '\\\n([\"name\", \"resgrp\", \"Owner\"] | (., map(length*\"-\"))), \\\n(.[] | [.name, .resourceGroup, .tags.owner//\"-\"])|@tsv' |column -t\n</code></pre>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#if-else","title":"if else","text":"<pre><code>aws translate list-text-translation-jobs |jq -r '.TextTranslationJobPropertiesList[] | (if .JobStatus == \"IN_PROGRESS\" then .JobStatus, .JobName, .JobId else empty end)' |xargs\n</code></pre>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#combine-output-to-a-array","title":"combine output to a array","text":"<pre><code>... |jq -s .\n</code></pre>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#filter-value-based-on-regexp","title":"filter value based on regexp","text":"<p><pre><code>#get all script from blueprint\ncat *.json | jq -r '.. | select (.|tostring|test(\"^#!.*\"))'\n</code></pre> and <pre><code>#sample, replace REGEXHERE to your string\n#Key name is \"Name\", replace it\n| select((.Tags[]|select(.Key==\"Name\")|.Value) | match(\"REGEXHERE\") )\n</code></pre></p>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#edit-json-file-directly-","title":"edit-json-file-directly-","text":"<pre><code>jq '. += { \"cpuManagerPolicy\":\"static\"}' /etc/kubernetes/kubelet/kubelet-config.json\n</code></pre> <ul> <li>refer: <ul> <li>../awscli/ecs-cmd</li> <li>../awscli/apigw-cmd</li> <li>../../cloud9/setup-cloud9-for-eks</li> <li>assume-tool</li> <li>../awscli/route53-cmd</li> </ul> </li> </ul>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#good-sample","title":"good sample","text":"<pre><code>developer:\n  android:\n    members:\n\n    - alice\n    - bob\n    oncall:\n    - bob\nhr:\n  members:\n  - charlie\n  - doug\nthis:\n  is:\n    really:\n      deep:\n        nesting:\n          members:\n          - example deep nesting\n</code></pre> <p>to</p> <pre><code>developer-android-members:\n\n  - alice\n  - bob\ndeveloper-android-oncall:\n  - bob\nhr-members:\n  - charlie\n  - doug\nthis-is-really-deep-nesting-members:\n  - example deep nesting\n</code></pre> <p>code </p> <pre><code>yq . | # convert yaml to json using python-yq\n    jq ' \n    . as $input | # Save the input for later\n    . | paths | # Get the list of paths \n        select(.[-1] | tostring | test(\"^(members|oncall|priv)$\"; \"ix\")) | # Only find paths which end with members, oncall, and priv\n        . as $path | # save each path in the $path variable\n    ( $input | getpath($path) ) as $members | # Get the value of each path from the original input\n    {\n        \"key\": ( $path | join(\"-\") ), # The key is the join of all path keys\n        \"value\": $members  # The value is the list of members\n    }\n    ' |\n    jq -s 'from_entries' | # collect kv pairs into a full object using slurp\n    yq --sort-keys -y . # Convert back to yaml using python-yq\n</code></pre>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#reinvent-breakout-session","title":"reinvent breakout session","text":"<p>download json <pre><code>youtube-dl https://www.youtube.com/playlist?list=PL2yQDdvlhXf-Jdg0SkHt85s-YvTUaNmgT --skip-download --write-info-json --write-annotations\n</code></pre></p> <p>get title url and description <pre><code>cat *json |jq -r '[\n(if (.title|test(\"\\\\([A-Z]{3}[0-9]{3}\")) then (.title|scan(\"[A-Z]{3}[0-9]{3}\")) else (\" \") end),\n.title,\n.webpage_url,\n(.description\n|gsub(\"\\n\";\"#\")\n|gsub(\"\\\"\";\"\")\n|gsub(\"#Subscribe:.*$\";\"\")\n|gsub(\"Learn more about[^#]*#\";\"\"))\n]|@csv'  &gt; ../a.txt\n</code></pre> upload a.txt</p> <p>match following line <pre><code>xx xx xx xx xx xx xx (CON312 xx xx xx xx\n</code></pre></p> <p>and print following line, if not match, then print \u201d \u201c <pre><code>CON312\n</code></pre></p> <pre><code>#!/bin/bash\n\nfor i in $a ; do\n    file=$(ls |egrep '\\('\"$i\"'\\)' )\n    if [[ -z $file ]]; then\n        echo $i\n    else\n        url=$(cat \"$file\" |jq -r '.webpage_url')\n        echo $i $url\n    fi\ndone\n</code></pre>","tags":["cmd/jq","python"]},{"location":"CLI/linux/jq-cmd/#get-lengh","title":"get lengh","text":"<pre><code>jq -r '.TransitGateways | length'\n</code></pre>","tags":["cmd/jq","python"]},{"location":"CLI/linux/linux-cmd/","title":"linux-cmd","text":"<p>[!WARNING] This is a github note</p>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#linux-cmd","title":"linux cmd","text":"","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#awk","title":"awk","text":"<pre><code>cat docs/CLI/awscli/vpc-cmd.md |awk '/^```sh title=\"func-.*/,/^```$/ {print}' &gt; /tmp/$$.1\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#brew","title":"brew","text":"<ul> <li>install <pre><code>sudo yum install -y git gcc make curl\ngit clone https://github.com/Homebrew/brew.git\nsudo cp brew/bin/brew /usr/local/bin/\n</code></pre></li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#curl","title":"curl","text":"<ul> <li>curl-sample-1</li> <li>badssl.com</li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#check-http-return-code","title":"check http return code","text":"<pre><code>curl -sL -w '%{http_code}' -o /dev/null \"https://httpbin.org/status/302\"\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#datediff","title":"datediff","text":"<ul> <li>Releases \u00b7 hroptatyrdateutils <pre><code>git clone https://github.com/hroptatyr/dateutils.git\ncd dateutils\nsudo yum install -y texinfo gperf\nautoreconf -i\n./configure\nmake\nsudo make install \n</code></pre></li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#ec2-instance-selector","title":"ec2-instance-selector","text":"<pre><code>brew tap aws/tap\nbrew install ec2-instance-selector\n</code></pre> <pre><code>ec2-instance-selector -c 4 -m 16 -r us-east-2 -a arm64\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#envsubst","title":"envsubst","text":"<pre><code>var1=string1\nvar2=string2\n\ncat &gt;$$.yaml &lt;&lt;-'EOF'\n$var1\n$var2\n$var3\nEOF\n\nexport var1 var2\ncat $$.yaml |envsubst '$var1 $var2' &gt; $$-new.yaml\n</code></pre> <ul> <li>refer envsubst</li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#firewall-cmd","title":"firewall-cmd","text":"<pre><code>firewall-cmd --permanent --add-port=80/tcp\nfirewall-cmd --reload\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#function","title":"function","text":"","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#less-or-no-parameters","title":"less or no parameters","text":"<ul> <li>\u5728 func \u4e2d\u5b9a\u4e49 local \u53d8\u91cf\uff0cexport \u540e\u5728 func \u5916\u90e8\u4f9d\u7136\u65e0\u6cd5\u8bbf\u95ee <pre><code>function a() {\n local var1=cccc\n echo $var1\n export var1\n}\n\nfunction b() {\n  echo $var1\n  var1=bbb\n}\n\nvar1=abc\n\na\necho $var1\n\nb\necho $var1\n</code></pre></li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#parse-parameter","title":"parse parameter","text":"<p><code>c:p:</code> \u53c2\u6570 c \u548c p \u90fd\u9700\u8981\u53c2\u6570 <code>cp</code> \u53c2\u6570 c \u548c p \u90fd\u4e0d\u9700\u8981\u53c2\u6570</p> <pre><code>function parsepara () {\n    OPTIND=1\n    OPTSTRING=\"h?v:c:p:\"\n    local VPC_ID=\"\"\n    local VPC_CIDR=\"\"\n    local PORTS=()\n    while getopts ${OPTSTRING} opt; do\n        case \"${opt}\" in\n            v) VPC_ID=${OPTARG} ;;\n            c) VPC_CIDR=${OPTARG} ;;\n            p) PORTS+=(\"${OPTARG}\") ;;\n            h|\\?) \n                echo \"format: create-sg -v VPC_ID -c VPC_CIDR [-p PORT1] [-p PORT2]\"\n                echo -e \"\\tsample: create-sg -v vpc-xxx -p 172.31.0.0/16\"\n                echo -e \"\\tsample: create-sg -v vpc-xxx -p 0.0.0.0/0 -p 80 -p 443\"\n                return 0\n            ;;\n        esac\n    done\n    : ${VPC_ID:?Missing -v}\n    : ${VPC_CIDR:?Missing -c}\n\n    echo \"CIDR:\"${CIDR}\n    echo \"PORTS:\"${PORTS}\n    echo \"PORTS[@]\"${PORTS[@]}\n\n    for i in ${PORTS[@]:--1}; do\n        echo \"i:\"$i\n    done\n}\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#history","title":"history","text":"<pre><code>bash                        # open a new session.\nunset HISTFILE              # avoid recording commands to file.\ncommands not recorded\n.\n.\nexit\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#ip-forward-","title":"ip-forward-","text":"<pre><code>echo 'net.ipv4.ip_forward = 1\nnet.ipv4.conf.default.rp_filter = 0\nnet.ipv4.conf.default.accept_source_route = 0\n' |tee -a /etc/sysctl.conf\nsysctl -p\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#iptables-","title":"iptables-","text":"","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#masquerade-","title":"MASQUERADE-","text":"<pre><code>iptables -t nat -A POSTROUTING  -j MASQUERADE\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#iptables","title":"iptables","text":"<pre><code>yum install iptables-services -y;\n\n# Start and configure iptables:\nsystemctl enable iptables;\nsystemctl start iptables;\n\n\n# Configuration below allows allows all traffic:\n# Set the default policies for each of the built-in chains to ACCEPT:\niptables -P INPUT ACCEPT;\niptables -P FORWARD ACCEPT;\niptables -P OUTPUT ACCEPT;\n\n# Flush the nat and mangle tables, flush all chains (-F), and delete all non-default chains (-X):\niptables -t nat -F;\niptables -t mangle -F;\niptables -F;\niptables -X;\n\n# Configure nat table to hairpin traffic back to GWLB:\niptables -t nat -A PREROUTING -p udp -s $gwlb_ip -d $instance_ip -i eth0 -j DNAT --to-destination $gwlb_ip:6081;\niptables -t nat -A POSTROUTING -p udp --dport 6081 -s $gwlb_ip -d $gwlb_ip -o eth0 -j MASQUERADE;\n\n# Save iptables:\nservice iptables save;\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#lsblk-","title":"lsblk-","text":"<pre><code>lsblk -o name,mountpoint,label,size,uuid\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#network-monitor","title":"network monitor","text":"<ul> <li>https://www.tecmint.com/linux-network-bandwidth-monitoring-tools/</li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#iperf","title":"iperf","text":"<ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/network-throughput-benchmark-linux-ec2/</li> <li>server  <pre><code>sudo iperf -s\n</code></pre></li> <li>client  <pre><code>iperf -c 172.31.30.41 --parallel 40 -i 1 -t 2\n</code></pre></li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#rsync","title":"rsync","text":"","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#notable-folder","title":"notable folder","text":"<pre><code>rsync -narv --delete /home/ubuntu/.notable /home/ubuntu/OneDrive/CrossSync/\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#work-notes","title":"work-notes","text":"<pre><code>bash\n\nexport HISTSIZE=0\ncd ~/Documents/\nrsync -avr --delete ./work-notes stevenpan@10.68.69.100:/Users/stevenpan/Documents/\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#sed","title":"sed","text":"<pre><code>file=file.md\ngsed -i 's/^!\\[\\[\\([^]]\\+\\)\\]\\]/![](\\1)/' ${file}\n# change \n# ![[stream-k8s-control-panel-logs-to-s3-21.png]]\n# to \n# ![](stream-k8s-control-panel-logs-to-s3-21.png)\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#sponge-tee-redirect-to-same-file","title":"sponge &amp; tee &amp; redirect to same file","text":"<p>sponge  reads  standard input and writes it out to the specified file. Unlike a shell redirect, sponge soaks up all its input before opening the output file. This allows constructing pipelines that read from and write to the same file.</p>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#tar","title":"tar","text":"<pre><code>tar cf a.tar ./blue-green-upgrade/ --exclude=\".terraform\"\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#tcp-setting-time_wiat","title":"tcp setting - TIME_WIAT","text":"<pre><code>net.ipv4.tcp_fin_timeout = 30\nnet.ipv4.ip_local_port_range = 15000 65000\nnet.ipv4.tcp_tw_recycle = 1\nnet.ipv4.tcp_tw_reuse = 1\n\nnet.ipv4.ip_forward=1\nnet.ipv4.conf.all.accept_source_route = 1\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#tc-traffic-control","title":"tc - traffic control","text":"<pre><code>yum install iproute-tc\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#doc","title":"doc","text":"<ul> <li>https://lartc.org/lartc.html#LARTC.COOKBOOK.FULLNAT.INTRO</li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#scenario","title":"scenario","text":"<ul> <li>connection-network-with-overlap-cidrs</li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#xfs","title":"xfs","text":"","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#xfs-mount-","title":"xfs-mount-","text":"<pre><code>mount -t xfs -o nouuid /dev/nvme1n1 /mnt\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#get-uuid","title":"get uuid","text":"<pre><code>xfs_db -c uuid /dev/nvme1n1\n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#xtop","title":"xtop","text":"<ul> <li>top</li> <li>htop</li> <li>atop</li> <li>iftop <pre><code>iftop -t -s 10 &gt; output\n</code></pre></li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#ip-address-calc","title":"ip address calc","text":"<pre><code>yum -y install sipcalc --enablerepo=epel\n</code></pre> <pre><code>brew install sipcalc \n</code></pre>","tags":["cmd","linux"]},{"location":"CLI/linux/linux-cmd/#others","title":"others","text":"<ul> <li>web-press-testing-tool</li> <li>httpbin.org</li> <li>badssl.com</li> </ul>","tags":["cmd","linux"]},{"location":"CLI/linux/terraform-cmd/","title":"terraform","text":"<p>[!WARNING] This is a github note</p>","tags":["terraform"]},{"location":"CLI/linux/terraform-cmd/#terraform-cmd","title":"terraform-cmd","text":"","tags":["terraform"]},{"location":"CLI/linux/terraform-cmd/#install-","title":"install-","text":"<ul> <li>RHEL <pre><code>sudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo\nsudo yum -y install terraform\n</code></pre></li> <li>AL2 <pre><code>sudo yum install -y yum-utils shadow-utils\nsudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo\nsudo yum -y install terraform\n</code></pre></li> </ul>","tags":["terraform"]},{"location":"CLI/linux/terraform-cmd/#variable","title":"variable","text":"<pre><code>variable \"region\" {\n  description = \"region\"\n  type        = string\n  default = \"us-east-2\"\n}\n</code></pre> <pre><code>terraform apply -var \"region=us-east-2\"\n</code></pre>","tags":["terraform"]},{"location":"CLI/linux/terraform-cmd/#workspace","title":"workspace","text":"<pre><code># create new one\nterraform workspace new test\n# show current \nterraform workspace show\n# list all \nterraform workspace list\n</code></pre>","tags":["terraform"]},{"location":"CLI/linux/terraform-cmd/#state","title":"state","text":"<pre><code>terraform state list\nterraform state show xxxxxx\n</code></pre>","tags":["terraform"]},{"location":"CLI/windows/powershell/","title":"powershell","text":"<p>[!WARNING] This is a github note</p>","tags":["microsoft/windows","microsoft/powershell"]},{"location":"CLI/windows/powershell/#powershell","title":"powershell","text":"","tags":["microsoft/windows","microsoft/powershell"]},{"location":"CLI/windows/powershell/#download-file","title":"download file","text":"<pre><code>Invoke-WebRequest -uri 'https://github.com/microsoft/windows-container-tools/releases/download/v1.1/LogMonitor.exe' -OutFile 'c:\\Logmonitor.exe'\n\nhttps://github.com/prometheus-community/windows_exporter/releases/download/v0.18.1/windows_exporter-0.18.1-amd64.msi\n\nInvoke-WebRequest -uri 'https://github.com/prometheus-community/windows_exporter/releases/download/v0.18.1/windows_exporter-0.18.1-amd64.msi' -OutFile 'c:\\windows_exporter-0.18.1-amd64.msi'\n</code></pre>","tags":["microsoft/windows","microsoft/powershell"]},{"location":"CLI/windows/powershell/#download-awsnvmezip-","title":"download-AWSNVMe.zip-","text":"<pre><code>invoke-webrequest https://s3.amazonaws.com/ec2-windows-drivers-downloads/NVMe/Latest/AWSNVMe.zip -outfile $env:USERPROFILE\\nvme_driver.zip\nexpand-archive $env:userprofile\\nvme_driver.zip -DestinationPath $env:userprofile\\nvme_driver\n</code></pre>","tags":["microsoft/windows","microsoft/powershell"]},{"location":"EKS/","title":"Index","text":"<p>[!WARNING] This is a github note</p>"},{"location":"EKS/#eks","title":"EKS","text":""},{"location":"EKS/#1-cluster","title":"1 cluster","text":"<p><pre><code>(path:git/git-mkdocs/eks/cluster file:.md)\n\n- [$frontmatter:title]($filename): $frontmatter:description\n</code></pre> - EKS Addons: EKS \u5e38\u7528\u63d2\u4ef6\u6e05\u5355 - Create EKS Cluster with Terraform: \u4f7f\u7528 Terraform \u521b\u5efa EKS \u96c6\u7fa4 - Create Private Only EKS Cluster: \u5728\u5df2\u6709 VPC \u4e2d\u521b\u5efa\u79c1\u6709\u8bbf\u95ee\u7684 EKS \u96c6\u7fa4 - Create Public Access EKS Cluster: \u521b\u5efa\u516c\u6709\u8bbf\u95ee\u7684 EKS \u96c6\u7fa4 - Create Public Access EKS Cluster in China Region: \u5728\u4e2d\u56fd\u533a\u57df\uff0c\u521b\u5efa\u5171\u6709\u8bbf\u95ee\u7684 EKS \u96c6\u7fa4 - EKS Upgrade Procedure: EKS \u96c6\u7fa4\u5347\u7ea7 &lt;\u2013&gt;</p>"},{"location":"EKS/#2-kubernetes","title":"2 kubernetes","text":"<p><pre><code>(path:git/git-mkdocs/eks/kubernetes file:.md)\n\n- [$frontmatter:title]($filename): $frontmatter:description\n</code></pre> - horizontal pod autoscaler: horizontal pod autoscaler - topology spread constraints: topology spread constraints &lt;\u2013&gt;</p>"},{"location":"EKS/#3-solutions","title":"3 solutions","text":"<p><pre><code>( path:git/git-mkdocs/eks/solutions file:.md )\n\n- [$frontmatter:title]($filename): $frontmatter:description\n</code></pre> - appmesh-workshop-eks: appmesh workshop - argocd: argocd - automated-canary-deployment-using-flagger: \u81ea\u52a8\u5316 canary \u90e8\u7f72 - cloudwatch-to-firehose-python: \u5728 firehose \u4e0a\uff0c\u5904\u7406\u4ece cloudwatch \u53d1\u9001\u6765\u7684\u65e5\u5fd7 - EKS Container Insights: \u542f\u7528 EKS \u7684 container insight \u529f\u80fd - enable-prometheus-in-cloudwatch: \u5c06 EKS \u96c6\u7fa4\u7684 prometheus \u6570\u636e\u6c47\u603b\u5230 cloudwatch - Export Cloudwatch Log Group to S3: \u5bfc\u51fa cloudwatch \u65e5\u5fd7\u5230 s3 - flux: flux - Install Grafana on Beanstalk: \u5728 EC2 / beanstalk / EKS \u4e0a\u5b89\u88c5 grafana  - install-prometheus-grafana-on-eks: \u5b89\u88c5 grafana \u548c prometheus - Using Loki for Logging: \u4f7f\u7528 loki \u6536\u96c6\u65e5\u5fd7 - Stream EKS Control Panel Logs to S3: \u76ee\u524d EKS \u63a7\u5236\u5e73\u9762\u65e5\u5fd7\u53ea\u652f\u6301\u53d1\u9001\u5230 cloudwatch\uff0c\u4e14\u5728\u540c\u4e00\u4e2a log group \u4e2d\u67095\u79cd\u7c7b\u578b6\u79cd\u524d\u7f00\u7684 log stream \u7684\u65e5\u5fd7\uff0c\u4e0d\u5229\u4e8e\u7edf\u4e00\u67e5\u8be2\u3002\u4e14\u53ea\u6709 audit \u65e5\u5fd7\u662f json \u683c\u5f0f\u5176\u4ed6\u5747\u662f\u5355\u884c\u65e5\u5fd7\uff0c\u4e14\u5b57\u6bb5\u5404\u4e0d\u76f8\u540c\u3002\u672c\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u601d\u8def\u7edf\u4e00\u4fdd\u5b58\u65e5\u5fd7\u4f9b\u540e\u7eed\u5206\u6790\u5904\u7406 - Building Prometheus HA Architect with Thanos: \u7528 Thanos \u89e3\u51b3 Prometheus \u5728\u591a\u96c6\u7fa4\u5927\u89c4\u6a21\u73af\u5883\u4e0b\u7684\u9ad8\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u9650\u5236 &lt;\u2013&gt;</p>"},{"location":"EKS/#4-addons","title":"4 addons","text":"<p><pre><code>(file:.md path:git/git-mkdocs/eks/addons) \n\n- [$frontmatter:title]($filename): $frontmatter:description\n</code></pre> - aws-for-fluent-bit:  - aws-load-balancer-controller: \u4f7f\u7528 aws \u8d1f\u8f7d\u5747\u8861\u63a7\u5236\u5668 - cert-manager: cert-manager - cluster-autoscaler: EKS \u96c6\u7fa4\u4e2d\u5b89\u88c5 Cluster Autoscaler - cni-metrics-helper: cni-metrics-helper - ebs-for-eks: \u4f7f\u7528 ebs \u4f5c\u4e3a pod \u6301\u4e45\u5316\u5b58\u50a8  - efs-for-eks: \u4f7f\u7528 efs \u4f5c\u4e3a pod \u6301\u4e45\u5316\u5b58\u50a8 - eks-addons-coredns: eks-addons-coredns - eks-addons-kube-proxy: eks-addons-kube-proxy - eks-addons-vpc-cni: eks-addons-vpc-cni - eks-custom-network: custom network \u53ef\u4ee5\u89e3\u51b3\u5b50\u7f51\u5730\u5740\u6bb5\u8017\u5c3d\u7684\u95ee\u9898 - eks-fargate: \u5728 eks \u96c6\u7fa4\u4e2d\u4f7f\u7528 fargate - eksup: eksup - enable-sg-on-pod: \u542f\u7528 pod \u5b89\u5168\u7ec4 - externaldns-for-route53: \u4f7f\u7528 externaldns \u7ec4\u4ef6 - karpenter-install-lab: \u4f7f\u7528 Karpenter \u4ee3\u66ff Cluster Autoscaler - kube-no-trouble: kube-no-trouble - kube-state-metrics: kube-state-metrics - metrics-server: EKS \u96c6\u7fa4\u4e2d\u5b89\u88c5 metrics server - nginx-ingress-controller: nginx-ingress-controller - nginx-ingress-controller-community-ver: \u4f7f\u7528 nginx ingress - nginx-ingress-controller-nginx-ver: nginx-ingress-controller-nginx-ver - pluto: pluto &lt;\u2013&gt;</p>"},{"location":"EKS/addons/aws-for-fluent-bit/","title":"aws-for-fluent-bit","text":"<p>[!WARNING] This is a github note</p>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/aws-for-fluent-bit/#aws-for-fluent-bit","title":"aws-for-fluent-bit","text":"<ul> <li>https://github.com/aws/aws-for-fluent-bit</li> <li>https://github.com/aws/eks-charts/blob/master/stable/aws-for-fluent-bit/README.md</li> </ul>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/aws-load-balancer-controller/","title":"aws-load-balancer-controller","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#aws-load-balancer-controller","title":"aws-load-balancer-controller","text":"","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#github","title":"github","text":"<ul> <li>https://github.com/kubernetes-sigs/aws-load-balancer-controller</li> <li>https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/how-it-works/</li> </ul>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#workshop","title":"workshop","text":"<ul> <li>awslbc-ingress-lab-echoserver</li> <li>https://www.eksworkshop.com/beginner/180_fargate/prerequisites-for-alb/</li> <li>\u5e38\u7528ingress\u7684\u76f8\u5173\u914d\u7f6e (awslbc-ingress-settings)</li> <li>\u4f7f\u7528\u5df2\u6709ingress\u7684\u76f8\u5173\u914d\u7f6e (awslbc-ingress-settings-ingress-group)</li> <li>pod rediness gate (link)</li> </ul>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#install","title":"install","text":"","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#install-with-eksdemo-","title":"install-with-eksdemo-","text":"<ul> <li>https://github.com/awslabs/eksdemo/blob/main/docs/install-awslb.md</li> <li>remove service account if existed  <pre><code>echo ${CLUSTER_NAME}\neksctl delete iamserviceaccount -c ${CLUSTER_NAME} \\\n    --name aws-load-balancer-controller --namespace kube-system\n</code></pre></li> <li>create  <pre><code>echo ${CLUSTER_NAME}\necho ${AWS_DEFAULT_REGION}\n\neksdemo install aws-lb-controller -c ${CLUSTER_NAME} --namespace kube-system \n</code></pre> ^yddjq0</li> </ul>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#install-","title":"install-","text":"<ul> <li> <p>https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/deploy/installation/</p> </li> <li> <p>Install AWS Load Balancer Controller <pre><code>CLUSTER_NAME=ekscluster1\nAWS_REGION=us-east-2\nexport AWS_DEFAULT_REGION=${AWS_REGION}\nexport AWS_PAGER=\"\"\n\neksctl utils associate-iam-oidc-provider \\\n  --cluster ${CLUSTER_NAME} \\\n  --approve\n\n# curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.1/docs/install/iam_policy.json\ngit clone https://github.com/kubernetes-sigs/aws-load-balancer-controller.git\n\n# check iamserviceaccount has been create by eksctl\naws cloudformation describe-stacks --stack-name eksctl-${CLUSTER_NAME}-addon-iamserviceaccount-kube-system-aws-load-balancer-controller 2&gt;&amp;1 1&gt;/dev/null\nif [[ $? -ne 0 ]]; then\n\nif [[ ${AWS_REGION%%-*} == \"cn\" ]]; then \n  # aws china region\n  IAM_POLICY_TEMPLATE=iam_policy_cn.json \nelse\n  # aws commercial region\n  IAM_POLICY_TEMPLATE=iam_policy.json \nfi\ncp aws-load-balancer-controller/docs/install/${IAM_POLICY_TEMPLATE} .\n\npolicy_name=AWSLoadBalancerControllerIAMPolicy-`date +%m%d%H%M`\npolicy_arn=$(aws iam create-policy \\\n  --policy-name ${policy_name}  \\\n  --policy-document file://${IAM_POLICY_TEMPLATE} \\\n  --query 'Policy.Arn' \\\n  --output text)\n\neksctl create iamserviceaccount \\\n  --cluster=${CLUSTER_NAME} \\\n  --namespace=kube-system \\\n  --name=aws-load-balancer-controller \\\n  --role-name=${policy_name} \\\n  --attach-policy-arn=${policy_arn} \\\n  --override-existing-serviceaccounts \\\n  --approve\n\n# check iamserviceaccount has been create by eksctl\nfi\n\nhelm repo add eks https://aws.github.io/eks-charts\nhelm repo update\n\n# following helm cmd will fail if you use 3.9.0 version\n# downgrade to helm 3.8.2\n# and another solved issue is here: [[ingress-controller-lab-issue]]\nif [[ ${AWS_REGION%%-*} == \"cn\" ]]; then \n  # aws china region\n  helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller \\\n    -n kube-system \\\n    --set clusterName=${CLUSTER_NAME} \\\n    --set serviceAccount.create=false \\\n    --set serviceAccount.name=aws-load-balancer-controller \\\n    --set image.repository=961992271922.dkr.ecr.cn-northwest-1.amazonaws.com.cn/amazon/aws-load-balancer-controller \\\n    # --set region=${AWS_DEFAULT_REGION} \\\n    # --set vpcId=${VPC_ID} \nelse\n  # aws commercial region\n  helm install aws-load-balancer-controller eks/aws-load-balancer-controller \\\n    -n kube-system \\\n    --set clusterName=${CLUSTER_NAME} \\\n    --set serviceAccount.create=false \\\n    --set serviceAccount.name=aws-load-balancer-controller \nfi\n\nkubectl get deployment -n kube-system aws-load-balancer-controller\n</code></pre></p> </li> </ul> <p>awslbc-ingress-controller-lab-issue</p>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#install-in-china-region","title":"install-in-china-region","text":"<pre><code># using china region ecr url\nhelm upgrade -i aws-load-balancer-controller \\\n    eks/aws-load-balancer-controller \\\n    -n kube-system \\\n    --set clusterName=${CLUSTER_NAME} \\\n    --set serviceAccount.create=false \\\n    --set serviceAccount.name=aws-load-balancer-controller \\\n    --set image.repository=961992271922.dkr.ecr.cn-northwest-1.amazonaws.com.cn/amazon/aws-load-balancer-controller \\\n    # --set region=${AWS_DEFAULT_REGION} \\\n    # --set vpcId=${VPC_ID} \n</code></pre> <p>find registry url from eks-container-image-registries-url-by-region using parameter <code>image.repository</code>  (refer LINK)</p> <p>if you got <code>ImagePullBackOff</code>, could replace domain name as following <pre><code>kubectl -n kube-system edit deployment aws-load-balancer-controller\n</code></pre></p> <pre><code># https://docs.aws.amazon.com/eks/latest/userguide/add-ons-images.html\n# add `.cn` postfix for china region\nREGISTRY=602401143452.dkr.ecr.us-east-1.amazonaws.com\n# REGISTRY=961992271922.dkr.ecr.cn-northwest-1.amazonaws.com.cn\n</code></pre>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#upgrade","title":"upgrade","text":"<ul> <li>Migrate v1 to v2 </li> </ul>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#supported-kubernetes-versions","title":"Supported Kubernetes versions","text":"<ul> <li>AWS Load Balancer Controller v2.0.0~v2.1.3 requires Kubernetes 1.15+</li> <li>AWS Load Balancer Controller v2.2.0~v2.3.1 requires Kubernetes 1.16-1.21</li> <li>AWS Load Balancer Controller v2.4.0+ requires Kubernetes 1.19+</li> <li>AWS Load Balancer Controller v2.5.0+ requires Kubernetes 1.22+</li> </ul>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#check-version","title":"check version","text":"<pre><code>helm list -n kube-system\n</code></pre>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#in-private-cluster","title":"in private cluster","text":"<p>\u5982\u679c\u8282\u70b9\u7ec4\u65e0\u6cd5\u8bbf\u95ee\u516c\u7f51\uff0c\u5219\u521b\u5efa ingress \u65f6\u611f\u89c9\u5f88\u6162\uff0c\u7ea6 5-6 \u5206\u949f\u624d\u80fd\u770b\u5230 alb\uff0c\u5206\u6790\u65e5\u5fd7\u770b\u5230\uff0c\u521b\u5efa alb \u8fc7\u7a0b\u4e2d\u4f1a\u8bbf\u95ee <code>shield</code> \u548c <code>wafv2</code> \u7b49\u670d\u52a1\u65f6\u8d85\u65f6\u5bfc\u81f4</p>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#blog","title":"blog","text":"<ul> <li>How To Expose Multiple Applications on Amazon EKS Using a Single Application Load Balancer</li> <li>Expose Amazon EKS pods through cross-account load balancer</li> </ul>","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/aws-load-balancer-controller/#refer","title":"refer","text":"","tags":["aws/container/eks","kubernetes/ingress"]},{"location":"EKS/addons/cert-manager/","title":"cert-manager","text":"<p>[!WARNING] This is a github note</p>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cert-manager/#cert-manager","title":"cert-manager","text":"","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cert-manager/#install","title":"install","text":"","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cert-manager/#install-with-eksdemo","title":"install with eksdemo","text":"<ul> <li>https://github.com/awslabs/eksdemo/blob/main/docs/install-cert-manager.md <pre><code>echo ${CLUSTER_NAME}\necho ${AWS_REGION}\neksdemo install cert-manager -c ${CLUSTER_NAME}\n\nkubectl get clusterissuer\n# default name is letsencrypt-prod\n</code></pre></li> </ul>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cert-manager/#install-with-helm","title":"install with helm","text":"<ul> <li>https://cert-manager.io/docs/installation/helm/</li> </ul>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cert-manager/#install-manually","title":"install manually","text":"<ul> <li>https://cert-manager.io/docs/installation/</li> <li>install newest version  <pre><code>kubectl create ns cert-manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml \\\n-n cert-manager\n# CM_VERSION=v1.12.3 (2023/07)\n# https://github.com/cert-manager/cert-manager/releases/download/${CM_VERSION}/cert-manager.yaml\n</code></pre></li> </ul>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cert-manager/#issuer-certificates-","title":"issuer-certificates-","text":"<pre><code>TEST_DOMAIN=thanos-gateway.poc1109.aws.panlm.xyz\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: ${TEST_DOMAIN}\n  namespace: monitoring\nspec:\n  secretName: thanos-gateway-tls\n  dnsNames:\n\n    - ${TEST_DOMAIN}\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\nEOF\n</code></pre>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cert-manager/#newest-version-v1123-202307","title":"newest version v1.12.3 (2023/07)","text":"","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/","title":"cluster-autoscaler","text":"<p>[!WARNING] This is a github note</p>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/#cluster-autoscaler","title":"cluster-autoscaler","text":"","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/#link","title":"link","text":"<ul> <li>workshop</li> <li>troubleshooting</li> <li>github</li> </ul>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/#blog","title":"blog","text":"<ul> <li>Creating Kubernetes Auto Scaling Groups for Multiple Availability Zones</li> </ul>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/#install","title":"install","text":"","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/#manual","title":"manual","text":"<p>https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/autoscaling.html</p>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/#create-service-account","title":"create service account","text":"<pre><code>CLUSTER_NAME=ekscluster1\nAWS_REGION=us-east-2\n\ncat &gt; cluster-autoscaler-policy.json &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"autoscaling:SetDesiredCapacity\",\n                \"autoscaling:TerminateInstanceInAutoScalingGroup\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:ResourceTag/k8s.io/cluster-autoscaler/&lt;my-cluster&gt;\": \"owned\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"VisualEditor1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"autoscaling:DescribeAutoScalingInstances\",\n                \"autoscaling:DescribeAutoScalingGroups\",\n                \"ec2:DescribeLaunchTemplateVersions\",\n                \"autoscaling:DescribeTags\",\n                \"autoscaling:DescribeLaunchConfigurations\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\nEOF\n\nARN=$(aws iam create-policy \\\n    --policy-name AmazonEKSClusterAutoscalerPolicy-$RANDOM \\\n    --policy-document file://cluster-autoscaler-policy.json |jq -r '.Policy.Arn')\n\neksctl create iamserviceaccount \\\n  --cluster=${CLUSTER_NAME} \\\n  --namespace=kube-system \\\n  --name=cluster-autoscaler \\\n  --attach-policy-arn=${ARN} \\\n  --override-existing-serviceaccounts \\\n  --approve\n</code></pre>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/#install-from-yaml","title":"install from yaml","text":"<pre><code>curl -o cluster-autoscaler-autodiscover.yaml https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml\nsed -i \"s/.YOUR CLUSTER NAME./${CLUSTER_NAME}/\" cluster-autoscaler-autodiscover.yaml\n\nkubectl apply -f cluster-autoscaler-autodiscover.yaml\n\nkubectl patch deployment cluster-autoscaler \\\n  -n kube-system \\\n  -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"}}}}}'\n</code></pre> <pre><code>kubectl -n kube-system edit deployment.apps/cluster-autoscaler\n</code></pre> <pre><code>#add following parameter\n--balance-similar-node-groups\n--skip-nodes-with-system-pods=false\n</code></pre> <p>get newest version for your cluster, for example 1.21.3 / 1.22.3 <pre><code>VER=1.21.3\nkubectl set image deployment cluster-autoscaler \\\n  -n kube-system \\\n  cluster-autoscaler=k8s.gcr.io/autoscaling/cluster-autoscaler:v${VER}\n</code></pre></p> <pre><code>kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler\n</code></pre>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/#helm","title":"helm","text":"<p>https://github.com/kubernetes/autoscaler/blob/master/charts/cluster-autoscaler/README.md</p> <ul> <li>create service account in previous chapter</li> <li> <p>install from helm <pre><code>helm repo add autoscaler https://kubernetes.github.io/autoscaler\nhelm install myca-release autoscaler/cluster-autoscaler \\\n    -n kube-system \\\n    --set autoDiscovery.clusterName=${CLUSTER_NAME} \\\n    --set awsRegion=${AWS_REGION} \\\n    --set rbac.serviceAccount.create=false \\\n    --set rbac.serviceAccount.name=cluster-autoscaler\n\n# refer values\n# wget -O myca-values.yaml https://github.com/kubernetes/autoscaler/raw/master/charts/cluster-autoscaler/values.yaml\n</code></pre></p> </li> <li> <p>check version <pre><code>helm list -n kube-system\n</code></pre></p> </li> </ul>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cluster-autoscaler/#compatibility-and-upgrade","title":"compatibility and upgrade","text":"<p>https://github.com/kubernetes-sigs/metrics-server#compatibility-matrix</p>","tags":["kubernetes","aws/container/eks"]},{"location":"EKS/addons/cni-metrics-helper/","title":"cni-metrics-helper","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["aws/container/eks"]},{"location":"EKS/addons/cni-metrics-helper/#cni-metrics-helper","title":"cni-metrics-helper","text":"","tags":["aws/container/eks"]},{"location":"EKS/addons/cni-metrics-helper/#install","title":"install","text":"<ul> <li>https://github.com/aws/amazon-vpc-cni-k8s</li> <li>https://github.com/aws/amazon-vpc-cni-k8s/blob/master/cmd/cni-metrics-helper/README.md</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/","title":"ebs-for-eks","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#ebs-for-eks","title":"ebs-for-eks","text":"","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#install","title":"install","text":"","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#install-using-eksdemo-","title":"install-using-eksdemo-","text":"<ul> <li>if you already have a service account called <code>ebs-csi-controller-sa</code>, delete it <pre><code>echo ${CLUSTER_NAME}\neksctl delete iamserviceaccount -c ${CLUSTER_NAME} \\\n--name ebs-csi-controller-sa --namespace kube-system\n</code></pre></li> <li>install ebs plugin <pre><code>echo ${CLUSTER_NAME}\necho ${AWS_DEFAULT_REGION}\n\neksdemo install storage-ebs-csi -c ${CLUSTER_NAME} --namespace kube-system\n</code></pre></li> </ul>","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#install_1","title":"install","text":"<p>https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/install.md</p>","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#ebs-csi","title":"ebs-csi","text":"<pre><code>echo ${CLUSTER_NAME:=ekscluster1}\necho ${AWS_REGION:=us-east-2}\n\ngit clone https://github.com/kubernetes-sigs/aws-ebs-csi-driver.git\nkubectl apply -k aws-ebs-csi-driver/deploy/kubernetes/overlays/stable\n\n# verify pod running\nkubectl get pods -n kube-system\n</code></pre>","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#assign-policy-to-node","title":"assign policy to node","text":"<pre><code># # (option) using customer managed policy\n# aws iam create-policy \\\n#     --policy-name Amazon_EBS_CSI_Driver \\\n#     --policy-document file://./aws-ebs-csi-driver/docs/example-iam-policy.json \\\n#     --region ${AWS_REGION}\n# POLICY_NAME=$(aws iam list-policies \\\n#   --query 'Policies[?PolicyName==`Amazon_EBS_CSI_Driver`].Arn' \\\n#   --output text --region ${AWS_REGION} )\n\n# using aws managed policy\nPOLICY_ARN=$(aws iam list-policies \\\n  --query 'Policies[?PolicyName==`AmazonEBSCSIDriverPolicy`].Arn' \\\n  --output text --region ${AWS_REGION} )\n# check detail permission in this policy (need --no-cli-pager)\n# aws iam get-policy-version --policy-arn ${POLICY_ARN} --version-id v1 --no-cli-pager\n\n# get vpc id\nVPC_ID=$(aws eks describe-cluster \\\n  --name ${CLUSTER_NAME} --region ${AWS_REGION} \\\n  --query \"cluster.resourcesVpcConfig.vpcId\" --output text )\n# get nodegroups' instance profiles\nTAG=tag:kubernetes.io/cluster/${CLUSTER_NAME}\nINSTANCE_PROFILES=($(aws ec2 describe-instances \\\n  --filters \"Name=${TAG},Values=owned\" \"Name=vpc-id,Values=${VPC_ID}\"\\\n  |jq -r '.Reservations[].Instances[].IamInstanceProfile.Arn' ) )\n# get role arns for instance profiles\nROLE_ARNS=($(for i in ${INSTANCE_PROFILES[@]}; do\n  aws iam get-instance-profile \\\n    --instance-profile-name ${i##*/} |jq -r '.InstanceProfile.Roles[0].Arn'\ndone |sort -u ))\necho ${ROLE_ARNS[@]}\n\n# attach policy to role\nfor i in ${ROLE_ARNS[@]}; do\n  aws iam attach-role-policy --policy-arn ${POLICY_ARN} \\\n    --role-name ${i##*/} --region ${AWS_REGION}\ndone\n</code></pre>","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#verify","title":"verify","text":"<pre><code>kubectl apply -f aws-ebs-csi-driver/examples/kubernetes/dynamic-provisioning/manifests/\n</code></pre>","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#cross-az-pod-definition","title":"cross az pod definition","text":"<ul> <li>check pv status</li> <li>check pod on which az</li> <li>delete pod and launch to another az</li> <li>check pod status pending</li> <li>kill pod and launch back to original az</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app\nspec:\n#  nodeSelector:\n#    topology.kubernetes.io/zone: cn-northwest-1b\n  containers:\n\n  - name: app\n    image: centos\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /data/out.txt; sleep 5; done\"]\n    volumeMounts:\n    - name: persistent-storage\n      mountPath: /data\n  volumes:\n  - name: persistent-storage\n    persistentVolumeClaim:\n      claimName: ebs-claim\n  terminationGracePeriodSeconds: 0\n</code></pre>","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#check-log","title":"check log","text":"<pre><code>k logs -f deploy/ebs-csi-controller csi-provisioner -n kube-system \n</code></pre>","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/ebs-for-eks/#ebs-csi-pod-has-6-container","title":"ebs-csi-pod has 6 container","text":"<ul> <li> <p>https://www.velotio.com/engineering-blog/kubernetes-csi-in-action-explained-with-features-and-use-cases</p> </li> <li> <p>ebs-plugin </p> </li> <li>csi-provisioner </li> <li>csi-attacher </li> <li>csi-snapshotter </li> <li>csi-resizer </li> <li>liveness-probe</li> </ul>","tags":["aws/storage/ebs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/","title":"efs-for-eks","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#efs-for-eks","title":"efs-for-eks","text":"","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#link","title":"link","text":"<ul> <li>efs workshop</li> <li>efs-on-eks-mini-priviledge</li> <li>Introducing Amazon EFS CSI dynamic provisioning</li> </ul>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#create-efs","title":"create efs","text":"<pre><code>echo ${CLUSTER_NAME:=ekscluster1}\necho ${AWS_REGION:=cn-northwest-1}\n\nVPC_ID=$(aws eks describe-cluster \\\n  --name ${CLUSTER_NAME} --region ${AWS_REGION} \\\n  --query \"cluster.resourcesVpcConfig.vpcId\" --output text )\nVPC_CIDR=$(aws ec2 describe-vpcs --vpc-ids ${VPC_ID} \\\n  --query \"Vpcs[].CidrBlock\"  --region ${AWS_REGION} --output text )\n\n# create security group\nSG_ID=$(aws ec2 create-security-group --description ${CLUSTER_NAME}-efs-eks-sg \\\n  --group-name efs-sg-$RANDOM --vpc-id ${VPC_ID} |jq -r '.GroupId' )\n# allow tcp 2049 (nfs v4)\naws ec2 authorize-security-group-ingress --group-id ${SG_ID}  --protocol tcp --port 2049 --cidr ${VPC_CIDR}\n\n# create efs\nFILESYSTEM_ID=$(aws efs create-file-system \\\n  --creation-token ${CLUSTER_NAME} \\\n  --region ${AWS_REGION} |jq -r '.FileSystemId' )\necho ${FILESYSTEM_ID}\n\nwhile true ; do\naws efs describe-file-systems \\\n--file-system-id ${FILESYSTEM_ID} \\\n--query 'FileSystems[].LifeCycleState' \\\n--output text |grep -q available\nif [[ $? -eq 0 ]]; then\n  break\nelse\n  echo \"wait...\"\n  sleep 10\nfi  \ndone\n\n# create mount target\nTAG=tag:kubernetes.io/role/internal-elb\nSUBNETS=($(aws eks describe-cluster --name ${CLUSTER_NAME} \\\n  --region ${AWS_REGION} |jq -r '.cluster.resourcesVpcConfig.subnetIds[]'))\nPRIV_SUBNETS=($(aws ec2 describe-subnets --filters \"Name=${TAG},Values=1\" \\\n  --subnet-ids ${SUBNETS[@]} |jq -r '.Subnets[].SubnetId' ) )\nfor i in ${PRIV_SUBNETS[@]} ; do\n  echo \"creating mount target in: \" $i\n  aws efs create-mount-target --file-system-id ${FILESYSTEM_ID} \\\n    --subnet-id ${i} --security-group ${SG_ID}\ndone\n</code></pre> <p>^mgh326</p>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#install-from-github","title":"install from github","text":"<p>\u76f4\u63a5\u5b89\u88c5\u4e0d\u989d\u5916\u914d\u7f6e\u6743\u9650\u7684\u8bdd\uff0c\u53ea\u80fd\u9a8c\u8bc1\u9759\u6001\u5236\u5907 \u5982\u679c\u9a8c\u8bc1\u52a8\u6001\u5236\u5907\uff0c\u4f1a\u6709\u6743\u9650\u4e0d\u591f\u7684\u544a\u8b66\uff0c\u56e0\u4e3a\u9700\u8981\u52a8\u6001\u521b\u5efaaccess point\uff0c\u53ef\u4ee5\u901a\u8fc7\u8282\u70b9role\u65b9\u5f0f\u52a0\u8f7d\u6743\u9650\uff0c\u6216\u8005\u91cd\u65b0\u90e8\u7f72\u4e3airsa</p> <pre><code>git clone https://github.com/kubernetes-sigs/aws-efs-csi-driver.git\nkubectl apply -k ./aws-efs-csi-driver/deploy/kubernetes/overlays/stable\n\n# verify pod running\nkubectl get pods -n kube-system\n\n# verify version\npod1=$(kubectl get pod -n kube-system -l app=efs-csi-controller |tail -n 1 |awk '{print $1}')\nkubectl exec -it ${pod1} -n kube-system -- mount.efs --version\n</code></pre>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#uninstall-efs-csi","title":"uninstall efs-csi","text":"<pre><code>cd aws-efs-csi-driver/deploy/kubernetes/overlays/stable\nkubectl kustomize |kubectl delete -f -\n</code></pre>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#install-from-helm","title":"install from helm","text":"<p>link</p>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#node-role-alternative","title":"node role (alternative)","text":"<ul> <li>you will got error in creating pod:  <code>User: arn:aws:sts::xxx:assumed-role/eksctl-ekscluster1-nodegroup-mana-NodeInstanceRole-1LTHOM1WRDBIS/i-xxx is not authorized to perform: elasticfilesystem:DescribeMountTargets on the specified resource</code>, if you miss this step <pre><code>wget -O iam-policy.json 'https://github.com/kubernetes-sigs/aws-efs-csi-driver/raw/master/docs/iam-policy-example.json'\n# cp aws-efs-csi-driver/docs/iam-policy-example.json iam-policy.json\n\n## Create an IAM policy \nPOLICY_ARN=$(aws iam create-policy \\\n  --policy-name EFSCSIControllerIAMPolicy-$RANDOM \\\n  --policy-document file://iam-policy.json |jq -r '.Policy.Arn' )\necho ${POLICY_ARN}\n\n## attach policy to node role\n</code></pre></li> </ul>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#sa-role","title":"sa role","text":"<pre><code># do steps in **node role** chapter\n\nCLUSTER_NAME=eks0630\nAWS_REGION=cn-northwest-1\n\neksctl utils associate-iam-oidc-provider \\\n  --cluster ${CLUSTER_NAME} \\\n  --region ${AWS_REGION} \\\n  --approve\neksctl create iamserviceaccount \\\n  --cluster=${CLUSTER_NAME} \\\n  --region ${AWS_REGION} \\\n  --namespace=kube-system \\\n  --name=efs-csi-controller-sa \\\n  --override-existing-serviceaccounts \\\n  --attach-policy-arn=${POLICY_ARN} \\\n  --approve\n</code></pre>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#install","title":"install","text":"<p><pre><code>helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver\nhelm repo update\n\n# get url for your region \n# https://docs.aws.amazon.com/eks/latest/userguide/add-ons-images.html\n# add `.cn` postfix for china region\nif [[ ${AWS_REGION%%-*} == \"cn\" ]]; then \n  REGISTRY=961992271922.dkr.ecr.cn-northwest-1.amazonaws.com.cn\nelse \n  REGISTRY=602401143452.dkr.ecr.us-east-1.amazonaws.com\nfi\nhelm upgrade -i aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \\\n  --namespace kube-system \\\n  --set image.repository=${REGISTRY}/eks/aws-efs-csi-driver \\\n  --set controller.serviceAccount.create=false \\\n  --set controller.serviceAccount.name=efs-csi-controller-sa\n</code></pre> find registry url from eks-container-image-registries-url-by-region</p>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#for-private-cluster","title":"for private cluster","text":"<p>doc</p> <pre><code>--set sidecars.livenessProbe.image.repository=602401143452.dkr.ecr.region-code.amazonaws.com/eks/livenessprobe \\\n--set sidecars.node-driver-registrar.image.repository=602401143452.dkr.ecr.region-code.amazonaws.com/eks/csi-node-driver-registrar \\\n--set sidecars.csiProvisioner.image.repository=602401143452.dkr.ecr.region-code.amazonaws.com/eks/csi-provisioner\n</code></pre>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#uninstall","title":"uninstall","text":"<pre><code>helm uninstall aws-efs-csi-driver -n kube-system\n</code></pre>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#verify","title":"verify","text":"","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#static-provisioning","title":"static provisioning","text":"<p>link using static provision with mount, no access point needed, so no additional iam policy</p> <pre><code>echo ${FILESYSTEM_ID}\n\ncat &gt; storageclass.yaml &lt;&lt;-EOF\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: efs-sc-mnt\nprovisioner: efs.csi.aws.com\nEOF\n\n# ensure volumeHandle has been replaced by your efs filesystem id\nenvsubst &gt; pv.yaml &lt;&lt;-EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: efs-pv-mnt\nspec:\n  capacity:\n    storage: 6Gi             # equal or bigger pvc, but it's a soft limit\n  volumeMode: Filesystem\n  accessModes:\n\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: efs-sc-mnt\n  csi:\n    driver: efs.csi.aws.com\n    volumeHandle: ${FILESYSTEM_ID}\nEOF\n\ncat &gt; pvc.yaml &lt;&lt;-EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: efs-claim-mnt\nspec:\n  accessModes:\n\n    - ReadWriteMany\n  storageClassName: efs-sc-mnt\n  resources:\n    requests:\n      storage: 5Gi           # it's a soft limit\nEOF\n\necho '---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: efs-app1\nspec:\n  containers:\n\n  - name: efs-app1\n    image: busybox\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /data/out1.txt; sleep 5; done\"]\n    volumeMounts:\n    - name: persistent-storage\n      mountPath: /data\n  volumes:\n  - name: persistent-storage\n    persistentVolumeClaim:\n      claimName: efs-claim-mnt\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: efs-app2\nspec:\n  containers:\n  - name: efs-app2\n    image: busybox\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /data/out2.txt; sleep 5; done\"]\n    volumeMounts:\n    - name: persistent-storage\n      mountPath: /data\n  volumes:\n  - name: persistent-storage\n    persistentVolumeClaim:\n      claimName: efs-claim-mnt\n' |tee pod.yaml\n\n\nkubectl apply -f storageclass.yaml\nkubectl apply -f pv.yaml\nkubectl apply -f pvc.yaml\nkubectl apply -f pod.yaml\n</code></pre>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#dynamic-provisioning-with-efs-access-point","title":"dynamic provisioning with efs access point","text":"<p>link need additional iam policy, execute <code>node role</code> part, or reinstall with irsa</p> <pre><code>echo ${FILESYSTEM_ID}\n\n# ensure volumeHandle has been replaced by your efs filesystem id\nenvsubst &gt; ./dy-storageclass.yaml &lt;&lt;-EOF\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: efs-sc\nprovisioner: efs.csi.aws.com\n# reclaimPolicy: Ratain # delete pod\nparameters:\n  provisioningMode: efs-ap\n  fileSystemId: ${FILESYSTEM_ID}\n  directoryPerms: \"700\"\n  gidRangeStart: \"1000\" # optional\n  gidRangeEnd: \"2000\" # optional\n  basePath: \"/dynamic_provisioning\" # optional\nEOF\n\necho '---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: efs-claim\nspec:\n  accessModes:\n\n    - ReadWriteMany\n  storageClassName: efs-sc\n  resources:\n    requests:\n      storage: 5Gi                      # soft limit\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: efs-app-dy1\nspec:\n  containers:\n    - name: app\n      image: centos\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /data/out1; sleep 5; done\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: efs-claim\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: efs-app-dy2\nspec:\n  containers:\n    - name: app\n      image: centos\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /data/out2; sleep 5; done\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: efs-claim\n' |tee ./dy-pod.yaml\n\nkubectl apply -f dy-storageclass.yaml\nkubectl apply -f dy-pod.yaml\n</code></pre>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#check-log","title":"check log","text":"<pre><code>kubectl logs deployment/efs-csi-controller -n kube-system -c efs-plugin\nkubectl logs daemonset/efs-csi-node -n kube-system -c efs-plugin\n</code></pre>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#reference","title":"reference","text":"<ul> <li>Amazon EFS quotas and limits</li> </ul>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/efs-for-eks/#redeploy-deployment-and-daemonset","title":"redeploy deployment and daemonset","text":"<pre><code>kubectl rollout restart deploy efs-csi-controller -n kube-system\nkubectl rollout restart ds efs-csi-node -n kube-system\n</code></pre>","tags":["aws/storage/efs","aws/container/eks"]},{"location":"EKS/addons/eks-addons-coredns/","title":"eks-addons-coredns","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-coredns/#eks-addons-coredns","title":"eks-addons-coredns","text":"<ul> <li>github</li> <li>updating<ul> <li>from cli</li> <li>from webui</li> </ul> </li> <li>others</li> <li>refer</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-coredns/#github","title":"github","text":"<ul> <li>doc </li> </ul> <p>latest version </p> <p>To improve the stability and availability of the CoreDNS Deployment, versions\u00a0<code>v1.9.3-eksbuild.5</code>\u00a0and later and\u00a0<code>v1.10.1-eksbuild.2</code>\u00a0are deployed with a\u00a0<code>PodDisruptionBudget</code>. If you\u2019ve deployed an existing\u00a0<code>PodDisruptionBudget</code>, your upgrade to these versions might fail. If the upgrade fails, completing one of the following tasks should resolve the issue:</p> <ul> <li>When doing the upgrade of the Amazon EKS add-on, choose to override the existing settings as your conflict resolution option. </li> <li>Remove your existing\u00a0<code>PodDisruptionBudget</code>\u00a0and try the upgrade again.</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-coredns/#updating","title":"updating","text":"","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-coredns/#from-cli","title":"from cli","text":"<pre><code>CLUSTER_NAME=ekscluster1\nexport AWS_DEFAULT_REGION=us-east-2\naws eks describe-addon --cluster-name ${CLUSTER_NAME} \\\n--addon-name coredns --query \"addon.addonVersion\" --output text\n\naws eks update-addon --cluster-name ${CLUSTER_NAME} \\\n--addon-name coredns --addon-version v1.9.3-eksbuild.5 \\\n--resolve-conflicts PRESERVE \n#--configuration-values '{\"replicaCount\":3}'\n</code></pre>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-coredns/#from-webui","title":"from webui","text":"<p>select <code>PRESERVE</code> </p>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-coredns/#others","title":"others","text":"<ul> <li>managed-coredns</li> <li>self-managed-coredns</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-coredns/#refer","title":"refer","text":"<ul> <li>https://github.com/coredns/deployment/blob/master/kubernetes/Upgrading_CoreDNS.md</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-kube-proxy/","title":"eks-addons-kube-proxy","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-kube-proxy/#eks-addons-kube-proxy","title":"eks-addons-kube-proxy","text":"<ul> <li>github</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-kube-proxy/#github","title":"github","text":"<ul> <li>doc</li> </ul> <p>There are two types of the\u00a0<code>kube-proxy</code>\u00a0container image available for each Amazon EKS cluster version:</p> <ul> <li>Default\u00a0\u2013 This image type is based on a Debian-based Docker image that is maintained by the Kubernetes upstream community.    </li> <li>Minimal\u00a0\u2013 This image type is based on a\u00a0minimal base image\u00a0maintained by Amazon EKS Distro, which contains minimal packages and doesn\u2019t have shells. For more information, see\u00a0Amazon EKS Distro.</li> </ul> <p></p> <pre><code>aws eks describe-addon-versions --addon-name kube-proxy |jq -r '.addons[].addonVersions[].addonVersion'\n</code></pre>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-vpc-cni/","title":"eks-addons-vpc-cni","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-vpc-cni/#eks-addons-vpc-cni","title":"eks-addons-vpc-cni","text":"<ul> <li>github</li> <li>Updating add-on<ul> <li>from webui (prefer)</li> <li>from cli</li> <li>re-install</li> </ul> </li> <li>additional</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-vpc-cni/#github","title":"github","text":"<ul> <li>github </li> <li>doc </li> </ul> <p>minor version </p> <p>latest version  20230731 update: 1.13.3-eksbuild.1 </p> <p>Upgrading (or downgrading) the VPC CNI version should result in no downtime. Existing pods should not be affected and will not lose network connectivity. New pods will be in pending state until the VPC CNI is fully initialized and can assign pod IP addresses. In v1.12.0+, VPC CNI state is restored via an on-disk file:\u00a0<code>/var/run/aws-node/ipam.json</code>. In lower versions, state is restored via calls to container runtime.</p>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-vpc-cni/#updating-add-on","title":"Updating add-on","text":"","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-vpc-cni/#from-webui-prefer","title":"from webui (prefer)","text":"<p>works select <code>PRESERVE</code> </p>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-vpc-cni/#from-cli","title":"from cli","text":"<p>not success</p> <ul> <li> <p>check addon version <pre><code>CLUSTER_NAME=ekscluster1\nexport AWS_DEFAULT_REGION=us-east-2\naws eks describe-addon --cluster-name ${CLUSTER_NAME} \\\n--addon-name vpc-cni --query addon.addonVersion --output text\n</code></pre></p> </li> <li> <p>backup <pre><code>kubectl get daemonset aws-node -n kube-system -o yaml &gt; aws-k8s-cni-old.yaml\n</code></pre></p> </li> <li> <p>upgrade <pre><code>SOURCE_VERSION=v1.13.2-eksbuild.1\nTARGET_VERSION=v1.13.3-eksbuild.1\naws eks update-addon --cluster-name ${CLUSTER_NAME} \\\n--addon-name vpc-cni --addon-version ${TARGET_VERSION} \\\n--resolve-conflicts PRESERVE \n</code></pre></p> </li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-vpc-cni/#re-install","title":"re-install","text":"<pre><code># arn:aws:iam::xxx:role/eksctl-ekscluster1-addon-vpc-cni-Role1\n\n# --configuration-values '{\"env\":{\"ENABLE_IPv4\":\"true\",\"ENABLE_IPv6\":\"false\"}}'\n\naws eks create-addon --cluster-name ${CLUSTER_NAME} \\\n--addon-name vpc-cni --addon-version v1.13.2-eksbuild.1 \\\n--service-account-role-arn arn:aws:iam::xxx:role/eksctl-ekscluster1-addon-vpc-cni-Role1 \\\n--resolve-conflicts OVERWRITE\n\n# aws eks delete-addon --cluster-name ekscluster1 --addon-name vpc-cni\n</code></pre>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-addons-vpc-cni/#additional","title":"additional","text":"<ul> <li>security-group-for-pod<ul> <li>enable-sg-on-pod</li> </ul> </li> <li>upgrade-vpc-cni</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/addons/eks-custom-network/","title":"eks-custom-network","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","kubernetes/cni"]},{"location":"EKS/addons/eks-custom-network/#eks-custom-network-lab","title":"eks-custom-network-lab","text":"","tags":["aws/container/eks","kubernetes/cni"]},{"location":"EKS/addons/eks-custom-network/#link","title":"link","text":"<ul> <li>link</li> <li>Leveraging CNI custom networking alongside security groups for pods in Amazon EKS</li> <li> <p>Automating custom networking to solve IPv4 exhaustion in Amazon EKS Containers</p> </li> <li> <p>There are a limited number of IP addresses available in a subnet. Using different subnets for pods allows you to increase the number of available IP addresses</p> </li> <li>For security reasons, your pods must use different security groups or subnets than the node\u2019s primary network interface.</li> <li>The nodes are configured in public subnets and you want the pods to be placed in private subnets using a NAT Gateway</li> </ul>","tags":["aws/container/eks","kubernetes/cni"]},{"location":"EKS/addons/eks-custom-network/#lab-","title":"lab-","text":"<p>https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/cni-custom-network.html</p> <ul> <li> <p>prep cidr / subnet id  <pre><code>CLUSTER_NAME=ekscluster1\nAWS_REGION=us-east-2\nexport AWS_DEFAULT_REGION=${AWS_REGION}\n\nVPC_ID=$(aws eks describe-cluster \\\n  --name ${CLUSTER_NAME} --region ${AWS_REGION} \\\n  --query \"cluster.resourcesVpcConfig.vpcId\" --output text )\n\naws ec2 describe-subnets --filters \"Name=vpc-id,Values=${VPC_ID}\" \\\n  --query \"Subnets[].[AvailabilityZone,SubnetId]\" --output text\n\naws ec2 describe-subnets --filters \"Name=vpc-id,Values=${VPC_ID}\" \\\n--query 'Subnets[].{Name:Tags[?Key==`Name`].Value|[0],SubnetId:SubnetId,AZ:AvailabilityZone,CIDR:CidrBlock}' --output table\n\n# copy paste origin private subnet to following variables (10.x.x.x)\nsubnet_id_1=subnet-xxx\nsubnet_id_2=subnet-xxx\n# copy paste new private subnet to following variables (100.64.x.x)\nnew_subnet_id_1=subnet-xxx\nnew_subnet_id_2=subnet-xxx\n\nAZ_1=$(aws ec2 describe-subnets --subnet-ids $subnet_id_1 --query 'Subnets[*].AvailabilityZone' --output text)\nAZ_2=$(aws ec2 describe-subnets --subnet-ids $subnet_id_2 --query 'Subnets[*].AvailabilityZone' --output text)\n</code></pre></p> </li> <li> <p>enable CUSTOM NETWORK  <pre><code>kubectl describe ds aws-node -n kube-system |grep -e CUSTOM_NETWORK -e EXTERNALSNAT\n# AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG\nkubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true\n# AWS_VPC_K8S_CNI_EXTERNALSNAT\nkubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_EXTERNALSNAT=true\n# need a default value for all NEW NODE in ANY nodegroup\nkubectl set env daemonset aws-node -n kube-system ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone\n\nCLUSTER_SECURITY_GROUP_ID=$(aws eks describe-cluster --name ${CLUSTER_NAME} --query cluster.resourcesVpcConfig.clusterSecurityGroupId --output text)\n</code></pre></p> </li> <li> <p>ENIConfig <pre><code>envsubst &gt;${AZ_1}.yaml &lt;&lt;-EOF\napiVersion: crd.k8s.amazonaws.com/v1alpha1\nkind: ENIConfig\nmetadata: \n  name: ${AZ_1}\nspec: \n  securityGroups: \n    - ${CLUSTER_SECURITY_GROUP_ID}\n  subnet: ${new_subnet_id_1}\nEOF\n\ncat &gt;${AZ_2}.yaml &lt;&lt;EOF\napiVersion: crd.k8s.amazonaws.com/v1alpha1\nkind: ENIConfig\nmetadata: \n  name: ${AZ_2}\nspec: \n  securityGroups: \n\n    - ${CLUSTER_SECURITY_GROUP_ID}\n  subnet: ${new_subnet_id_2}\nEOF\n\nkubectl apply -f ${AZ_1}.yaml\nkubectl apply -f ${AZ_2}.yaml\nkubectl get ENIConfigs\n</code></pre></p> </li> <li> <p>create new node group or scale existed node group</p> </li> </ul>","tags":["aws/container/eks","kubernetes/cni"]},{"location":"EKS/addons/eks-custom-network/#run-pod-on-specific-node","title":"run pod on specific node","text":"<pre><code>cat &gt;pod-ubuntu1.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-ubuntu1\nspec:\n  containers:\n\n  - name: ubuntu\n    image: ubuntu\n    command: [\"sh\", \"-c\", \"while true ; do sleep 10; done\"]\n  nodeSelector:\n    alpha.eksctl.io/nodegroup-name: mng1\n#    kubernetes.io/hostname: ip-10-x-x-x.us-east-2.compute.internal\nEOF\nkubectl apply -f pod-ubuntu1.yaml\n</code></pre> <p>^hennaq</p>","tags":["aws/container/eks","kubernetes/cni"]},{"location":"EKS/addons/eks-fargate-lab/","title":"eks-fargate","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["aws/container/eks","aws/container/fargate"]},{"location":"EKS/addons/eks-fargate-lab/#eks-fargate-lab","title":"eks-fargate-lab","text":"","tags":["aws/container/eks","aws/container/fargate"]},{"location":"EKS/addons/eks-fargate-lab/#_1","title":"\u73af\u5883\u51c6\u5907","text":"<ul> <li>\u767b\u5f55\u4f60\u7684\u5b9e\u9a8c\u73af\u5883 (LINK)\uff0c\u5e76\u4e14\u6253\u5f00 <code>AWS Console</code> </li> <li> <p>\u8fdb\u5165 aws cloud9 (LINK)\uff0c\u6253\u5f00\u5df2\u7ecf\u51c6\u5907\u597d\u7684 <code>EKSLabIDE</code> \u684c\u9762\uff0c\u70b9\u51fb <code>+</code> \u65b0\u5efa\u4e00\u4e2a <code>Terminal</code> \u7a97\u53e3</p> </li> <li> <p>\u5b89\u88c5\u5fc5\u8981\u7684\u8f6f\u4ef6 <pre><code>aws s3 cp s3://ee-assets-prod-us-east-1/modules/bd7b369f613f452dacbcea2a5d058d5b/v6/eksinit.sh . \nchmod +x eksinit.sh\n./eksinit.sh \nsource ~/.bash_profile \nsource ~/.bashrc\n\nkubectl get nodes\n</code></pre></p> </li> <li> <p>\u5982\u679c\u53ef\u4ee5\u6b63\u5e38\u663e\u793a\u8282\u70b9\u4fe1\u606f\uff0c\u8868\u793a\u73af\u5883\u5df2\u7ecf\u5c31\u7eea\u3002</p> </li> </ul>","tags":["aws/container/eks","aws/container/fargate"]},{"location":"EKS/addons/eks-fargate-lab/#create-fargate-profile","title":"create fargate profile","text":"<ul> <li>\u5728\u73b0\u6709\u96c6\u7fa4\u4e2d\u6dfb\u52a0 fargate \u652f\u6301</li> </ul> <pre><code>CLUSTER_NAME=eksworkshop-eksctl\nAWS_REGION=${AWS_DEFAULT_REGION}\nNAMESPACE=game-2048\n\n# pods in namespace called `game-2048` will be deployed to fargate profile\neksctl create fargateprofile \\\n  --cluster ${CLUSTER_NAME} \\\n  --region ${AWS_REGION} \\\n  --name ${NAMESPACE} \\\n  --namespace ${NAMESPACE}\n\neksctl get fargateprofile \\\n  --cluster ${CLUSTER_NAME} \\\n  -o yaml\n</code></pre> <ul> <li>\u4f60\u53ef\u4ee5\u767b\u5f55 eks \u7ba1\u7406\u754c\u9762\u786e\u8ba4 fargate profile \u521b\u5efa\u6210\u529f </li> <li>\u622a\u4e2a\u56fe</li> </ul>","tags":["aws/container/eks","aws/container/fargate"]},{"location":"EKS/addons/eks-fargate-lab/#install-aws-load-balancer-controller","title":"install aws load balancer controller","text":"<ul> <li>\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u5b89\u88c5\u4e00\u4e2a\u5e94\u7528\uff0c\u5e76\u4e14\u5bf9\u5916\u53d1\u5e03\uff0c\u8fd9\u91cc\u9700\u8981\u7528\u5230 aws load balancer controller<ul> <li>refer aws-load-balancer-controller</li> </ul> </li> </ul> <pre><code>eksctl utils associate-iam-oidc-provider \\\n    --region ${AWS_REGION} \\\n    --cluster ${CLUSTER_NAME} \\\n    --approve\n\n# china region link\n# wget -O iam_policy.json https://github.com/kubernetes-sigs/aws-load-balancer-controller/raw/main/docs/install/iam_policy_cn.json\n# global region link\nwget -O iam_policy.json https://github.com/kubernetes-sigs/aws-load-balancer-controller/raw/main/docs/install/iam_policy.json\nPOLICY_ARN=$(aws iam create-policy \\\n    --policy-name AWSLoadBalancerControllerIAMPolicy-$RANDOM \\\n    --policy-document file://iam_policy.json |jq -r '.Policy.Arn' )\necho ${POLICY_ARN}\n\neksctl create iamserviceaccount \\\n  --cluster ${CLUSTER_NAME} \\\n  --namespace kube-system \\\n  --name aws-load-balancer-controller \\\n  --attach-policy-arn ${POLICY_ARN} \\\n  --override-existing-serviceaccounts \\\n  --approve\n\nkubectl get sa aws-load-balancer-controller -n kube-system -o yaml\n\nkubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master\"\n\nhelm repo add eks https://aws.github.io/eks-charts\n\nhelm upgrade -i aws-load-balancer-controller \\\n    eks/aws-load-balancer-controller \\\n    -n kube-system \\\n    --set clusterName=${CLUSTER_NAME} \\\n    --set serviceAccount.create=false \\\n    --set serviceAccount.name=aws-load-balancer-controller\n\nkubectl -n kube-system rollout status deployment aws-load-balancer-controller\n</code></pre> <ul> <li>\u786e\u8ba4 <code>aws-load-balancer-controller</code> \u90e8\u7f72\u6210\u529f <pre><code>kubectl get deploy aws-load-balancer-controller -n kube-system\n</code></pre></li> </ul>","tags":["aws/container/eks","aws/container/fargate"]},{"location":"EKS/addons/eks-fargate-lab/#deploy-game-2048","title":"deploy game 2048","text":"<ul> <li>\u90e8\u7f72\u5e94\u7528</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/2048/2048_full.yaml\n</code></pre> <ul> <li> <p>\u89c2\u5bdf\u5230\u6709 <code>fargate-</code> \u5f00\u5934\u7684\u8282\u70b9\u88ab\u52a0\u5165\u5230\u96c6\u7fa4\u4e2d <pre><code>kubectl get all -n $NAMESPACE -o wide\n</code></pre></p> </li> <li> <p>\u89c2\u5bdf\u5e94\u7528\u8fd0\u884c\u5728 fargate \u8282\u70b9\u4e0a\uff0c\u4e14\u72b6\u6001\u5df2\u7ecf\u4e3a <code>running</code> <pre><code>kubectl get no  -o wide\n</code></pre></p> </li> <li> <p>\u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee\u5e94\u7528URL <pre><code>kubectl get ing -n ${NAMESPACE} -o=custom-columns=\"URL\":.status.loadBalancer.ingress[*].hostname\n</code></pre></p> </li> </ul>","tags":["aws/container/eks","aws/container/fargate"]},{"location":"EKS/addons/eks-fargate-lab/#other-fargate-labs","title":"other fargate labs","text":"<ul> <li>immersion workshop</li> <li>eksworkshop</li> </ul>","tags":["aws/container/eks","aws/container/fargate"]},{"location":"EKS/addons/eksup/","title":"eksup","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/eksup/#eksup","title":"eksup","text":"<pre><code>min_depth: 2\nmax_depth: 4\n</code></pre>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/eksup/#github","title":"github","text":"<p>https://clowdhaus.github.io/eksup/</p>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/enable-sg-on-pod/","title":"enable-sg-on-pod","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","aws/network/security-group"]},{"location":"EKS/addons/enable-sg-on-pod/#enable-sg-on-pod","title":"enable-sg-on-pod","text":"","tags":["aws/container/eks","aws/network/security-group"]},{"location":"EKS/addons/enable-sg-on-pod/#useful-env-sg-on-pod","title":"useful-env-sg-on-pod","text":"<p>considerations https://docs.amazonaws.cn/en_us/eks/latest/userguide/security-groups-for-pods.html</p> <p></p> <p>upgrade-vpc-cni</p> <p></p> <p>\u7b2c\u4e00\u4e2a\u53c2\u6570\u5141\u8bb8pod \u6302sg \u7b2c\u4e8c\u4e2a\u53c2\u6570\u5141\u8bb8\u6709sg\u7684pod\u540c\u65f6\u9075\u5b88network policy\u63a7\u5236 \u7b2c\u4e09\u4e2a\u53c2\u6570snat disable\uff0c\u5141\u8bb8\u8de8vpc\u4fdd\u7559pod ip\uff0c\u4e0d\u4f1a\u8f6c\u6210primary eni ip (eks-external-snat)</p> <pre><code>kubectl -n kube-system set env daemonset aws-node ENABLE_POD_ENI=true\nkubectl -n kube-system set env daemonset aws-node POD_SECURITY_GROUP_ENFORCING_MODE=standard\nkubectl -n kube-system set env daemonset aws-node AWS_VPC_K8S_CNI_EXTERNALSNAT=true\nkubectl -n kube-system rollout status ds aws-node\n</code></pre> <p>more explanation for ENV on vpc cni github</p> <ul> <li>https://github.com/aws/amazon-vpc-cni-k8s/blob/master/README.md#:~:text=recycle%20the%20instance.-,POD_SECURITY_GROUP_ENFORCING_MODE,-(v1.11.0%2B)</li> </ul>","tags":["aws/container/eks","aws/network/security-group"]},{"location":"EKS/addons/enable-sg-on-pod/#workshop","title":"workshop","text":"<p>https://www.eksworkshop.com/beginner/115_sg-per-pod/</p> <ul> <li> <p>create SG <pre><code>CLUSTER_NAME=ekscluster1\n\nexport VPC_ID=$(aws eks describe-cluster \\\n  --name ${CLUSTER_NAME} \\\n  --query \"cluster.resourcesVpcConfig.vpcId\" \\\n  --output text)\n\n# create the POD security group\naws ec2 create-security-group \\\n    --description 'POD SG' \\\n    --group-name 'POD_SG' \\\n    --vpc-id ${VPC_ID}\n\n# save the security group ID for future use\nexport POD_SG=$(aws ec2 describe-security-groups \\\n    --filters Name=group-name,Values=POD_SG Name=vpc-id,Values=${VPC_ID} \\\n    --query \"SecurityGroups[0].GroupId\" --output text)\n\necho \"POD security group ID: ${POD_SG}\"\n</code></pre></p> </li> <li> <p>for managed node group, allow DNS ingress <pre><code>NODE_GROUP_SG=$(aws ec2 describe-security-groups \\\n  --filters Name=tag:Name,Values=eks-cluster-sg-${CLUSTER_NAME}-* Name=vpc-id,Values=${VPC_ID} \\\n  --query \"SecurityGroups[0].GroupId\" \\\n  --output text)\necho \"Node Group security group ID: ${NODE_GROUP_SG}\"\n\n# allow POD_SG to connect to NODE_GROUP_SG using TCP 53\naws ec2 authorize-security-group-ingress \\\n    --group-id ${NODE_GROUP_SG} \\\n    --protocol tcp \\\n    --port 53 \\\n    --source-group ${POD_SG}\n\n# allow POD_SG to connect to NODE_GROUP_SG using UDP 53\naws ec2 authorize-security-group-ingress \\\n    --group-id ${NODE_GROUP_SG} \\\n    --protocol udp \\\n    --port 53 \\\n    --source-group ${POD_SG}\n</code></pre></p> </li> <li> <p>for unmanaged node group created by <code>eksctl</code>, allow DNS ingress <pre><code># eksctl-ekscluster1-nodegroup-ng1-SG-1XX82XXXXXX5Y)\nUNMANAGED_NODE_GROUP_SG=$(aws ec2 describe-security-groups \\\n  --filters Name=tag:Name,Values=eksctl-${CLUSTER_NAME}-nodegroup-* Name=vpc-id,Values=${VPC_ID} \\\n  --query \"SecurityGroups[0].GroupId\" \\\n  --output text)\necho \"UNMANAGED Node Group security group ID: ${UNMANAGED_NODE_GROUP_SG}\"\n\n# allow POD_SG to connect to NODE_GROUP_SG using TCP 53\naws ec2 authorize-security-group-ingress \\\n    --group-id ${UNMANAGED_NODE_GROUP_SG} \\\n    --protocol tcp \\\n    --port 53 \\\n    --source-group ${POD_SG}\n\n# allow POD_SG to connect to NODE_GROUP_SG using UDP 53\naws ec2 authorize-security-group-ingress \\\n    --group-id ${UNMANAGED_NODE_GROUP_SG} \\\n    --protocol udp \\\n    --port 53 \\\n    --source-group ${POD_SG}\n</code></pre></p> </li> <li> <p>CNI config <pre><code># ensure cluster role has AmazonEKSVPCResourceController policy\nexport CLUSTER_ROLE=$(aws eks describe-cluster \\\n  --name ${CLUSTER_NAME} \\\n  --query \"cluster.roleArn\" \\\n  --output text)\n\naws iam list-attached-role-policies --role-name ${CLUSTER_ROLE##*/}\n</code></pre></p> </li> </ul> <pre><code>kubectl -n kube-system set env daemonset aws-node ENABLE_POD_ENI=true\nkubectl -n kube-system set env daemonset aws-node AWS_VPC_K8S_CNI_EXTERNALSNAT=true\nkubectl -n kube-system set env daemonset aws-node POD_SECURITY_GROUP_ENFORCING_MODE=standard\nkubectl -n kube-system rollout status ds aws-node\n\nkubectl get nodes --show-labels\n# will see `vpc.amazonaws.com/has-trunk-attached=true`\n</code></pre> <ul> <li> <p>create resources (if you create resource on fargate, see below) <pre><code># check CRDs existed\nkubectl get crd securitygrouppolicies.vpcresources.k8s.aws\n\nenvsubst &gt; ./sg-policy.yaml &lt;&lt;-EoF\napiVersion: vpcresources.k8s.aws/v1beta1\nkind: SecurityGroupPolicy\nmetadata:\n  name: allow-rds-access\nspec:\n  podSelector:\n    matchLabels:\n      app: echoserver\n  securityGroups:\n    groupIds:\n\n      - ${POD_SG}\nEoF\n\nk create ns echo1\nk apply -f sg-policy.yaml -n echo1\nk apply -f deployment-service.yaml -n echo1\nk apply -f ingress1.yaml -n echo1\n\nk create ns echo2\nk apply -f sg-policy.yaml -n echo2\nk apply -f deployment-service.yaml -n echo2\nk apply -f ingress2.yaml -n echo2\n\nk create ns echo3\nk apply -f deployment-service.yaml -n echo3\nk apply -f ingress3.yaml -n echo3\n</code></pre></p> </li> <li> <p>echoserver deployment &amp; service <pre><code>cat &gt; ./deployment-service.yaml &lt;&lt;-EoF\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\nspec:\n  selector:\n    matchLabels:\n      app: echoserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      containers:\n      - image: k8s.gcr.io/e2e-test-images/echoserver:2.5\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: echoserver\nspec:\n  ports:\n    - port: 80\n      targetPort: 8080\n      protocol: TCP\n  type: NodePort\n  selector:\n    app: echoserver\nEoF\n</code></pre></p> </li> <li> <p>echoserver ingress for echo1</p> </li> <li>echoserver ingress for echo2</li> <li> <p>echoserver ingress for echo3 refer awslbc-ingress-settings-ingress-group</p> </li> <li> <p>resource yaml for pod on fargate <pre><code>envsubst &gt; ./sg-policy.yaml &lt;&lt;-EoF\napiVersion: vpcresources.k8s.aws/v1beta1\nkind: SecurityGroupPolicy\nmetadata:\n  name: allow-rds-access\nspec:\n  podSelector:\n    matchLabels:\n      app: echoserver\n  securityGroups:\n    groupIds:\n      - ${POD_SG}\n      - ${NODE_GROUP_SG} # cluster SG\nEoF\n</code></pre></p> </li> </ul>","tags":["aws/container/eks","aws/network/security-group"]},{"location":"EKS/addons/externaldns-for-route53/","title":"externaldns-for-route53","text":"<p>[!WARNING] This is a github note</p>","tags":["kubernetes","aws/network/route53"]},{"location":"EKS/addons/externaldns-for-route53/#externaldns-for-route53","title":"externaldns-for-route53","text":"","tags":["kubernetes","aws/network/route53"]},{"location":"EKS/addons/externaldns-for-route53/#func-create-hosted-zone-","title":"func-create-hosted-zone-","text":"<ul> <li> <p>\u6267\u884c\u4e0b\u9762\u547d\u4ee4\u521b\u5efa Hosted Zone\uff0c\u7136\u540e\u624b\u5de5\u6dfb\u52a0 NS \u8bb0\u5f55\u5230\u4e0a\u6e38\u7684\u57df\u540d\u670d\u52a1\u5668 domain registrar \u4e2d (create hosted zone, and then add NS records to upstream domain registrar) func-setup-hosted-zone<pre><code>echo ${DOMAIN_NAME}\n\nfunction create-hosted-zone () {\n    OPTIND=1\n    OPTSTRING=\"h?n:\"\n    local DOMAIN_NAME=\"\"\n    while getopts ${OPTSTRING} opt; do\n        case \"${opt}\" in\n            n) DOMAIN_NAME=${OPTARG} ;;\n            h|\\?) \n                echo \"format: create-host-zone -n DOMAIN_NAME \"\n                echo -e \"\\tsample: create-host-zone -n xxx.domain.com \"\n                return 0\n            ;;\n        esac\n    done\n    : ${DOMAIN_NAME:?Missing -n}\n\n    aws route53 create-hosted-zone --name \"${DOMAIN_NAME}.\" \\\n      --caller-reference \"external-dns-test-$(date +%s)\"\n\n    local ZONE_ID=$(aws route53 list-hosted-zones-by-name --output json \\\n      --dns-name \"${DOMAIN_NAME}.\" --query HostedZones[0].Id --out text)\n\n    local NS=$(aws route53 list-resource-record-sets --output text \\\n      --hosted-zone-id $ZONE_ID --query \\\n      \"ResourceRecordSets[?Type == 'NS'].ResourceRecords[*].Value | []\")\n\n    echo '###'\n    echo '# get bash function from here: https://panlm.github.io/CLI/awscli/route53-cmd/#func-create-ns-record-'\n    echo '# copy below output to add NS record on your upstream domain registrar'\n    echo '###'\n    echo 'DOMAIN_NAME='${DOMAIN_NAME}\n    echo 'NS=\"'${NS}'\"'\n    echo 'create-ns-record -n ${DOMAIN_NAME} -s \"${NS}\"'\n    echo ''\n}\n</code></pre></p> </li> <li> <p>refer: ../../CLI/awscli/route53-cmd </p> </li> <li>refer: route53-subdomian</li> <li>you also could create private hosted zone and associate to your vpc. plugin will insert/update record in your private hosted zone. (link)</li> </ul>","tags":["kubernetes","aws/network/route53"]},{"location":"EKS/addons/externaldns-for-route53/#install","title":"install","text":"","tags":["kubernetes","aws/network/route53"]},{"location":"EKS/addons/externaldns-for-route53/#install-","title":"install-","text":"<ul> <li>https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md</li> <li> <p>\u521b\u5efa\u6240\u9700\u8981\u7684\u670d\u52a1\u8d26\u53f7 (create service account)</p> <ul> <li>\u786e\u4fdd EKS \u96c6\u7fa4\u540d\u79f0\u6b63\u786e (ensure eks cluster name is correct)</li> <li>\u786e\u4fdd\u4f7f\u7528\u6b63\u786e\u7684 Region (ensure region is correct)</li> <li>\u786e\u4fdd\u57df\u540d\u5339\u914d\u6240\u9700 (ensure domain name is correct) <pre><code>CLUSTER_NAME=ekscluster1\nAWS_REGION=us-east-2\nDOMAIN_NAME=api0413.aws.panlm.xyz\nEXTERNALDNS_NS=externaldns\nexport AWS_PAGER=\"\"\n\n# create namespace if it does not yet exist\nkubectl get namespaces | grep -q $EXTERNALDNS_NS || \\\n  kubectl create namespace $EXTERNALDNS_NS\n\ncat &gt;externaldns-policy.json &lt;&lt;-EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\nEOF\n\nPOLICY_NAME=AllowExternalDNSUpdates-${RANDOM}\naws iam create-policy --policy-name ${POLICY_NAME} --policy-document file://externaldns-policy.json\n\n# example: arn:aws:iam::XXXXXXXXXXXX:policy/AllowExternalDNSUpdates\nexport POLICY_ARN=$(aws iam list-policies \\\n --query 'Policies[?PolicyName==`'\"${POLICY_NAME}\"'`].Arn' --output text)\n\neksctl create iamserviceaccount \\\n  --cluster ${CLUSTER_NAME} \\\n  --name \"external-dns\" \\\n  --namespace ${EXTERNALDNS_NS:-\"default\"} \\\n  --override-existing-serviceaccounts \\\n  --attach-policy-arn $POLICY_ARN \\\n  --approve\n</code></pre></li> </ul> </li> <li> <p>\u4f7f\u7528\u4e0a\u8ff0\u670d\u52a1\u8d26\u53f7\u5b89\u88c5 ExternalDNS (install ExternalDNS with existed SA) <pre><code>echo ${EXTERNALDNS_NS}\necho ${DOMAIN_NAME}\necho ${AWS_REGION}\n\nenvsubst &gt;externaldns-with-rbac.yaml &lt;&lt;-EOF\n# comment out sa if it was previously created\n# apiVersion: v1\n# kind: ServiceAccount\n# metadata:\n#   name: external-dns\n#   labels:\n#     app.kubernetes.io/name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nrules:\n\n  - apiGroups: [\"\"]\n    resources: [\"services\",\"endpoints\",\"pods\",\"nodes\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n  - apiGroups: [\"extensions\",\"networking.k8s.io\"]\n    resources: [\"ingresses\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\n  labels:\n    app.kubernetes.io/name: external-dns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n  - kind: ServiceAccount\n    name: external-dns\n    namespace: ${EXTERNALDNS_NS} # change to desired namespace: externaldns, kube-addons\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n        - name: external-dns\n          image: registry.k8s.io/external-dns/external-dns:v0.13.2\n          args:\n            - --source=service\n            - --source=ingress\n            - --domain-filter=${DOMAIN_NAME} # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n            - --provider=aws\n            - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n            - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n            - --registry=txt\n            - --txt-owner-id=external-dns\n          env:\n            - name: AWS_DEFAULT_REGION\n              value: ${AWS_REGION} # change to region where EKS is installed\n     # # Uncommend below if using static credentials\n     #        - name: AWS_SHARED_CREDENTIALS_FILE\n     #          value: /.aws/credentials\n     #      volumeMounts:\n     #        - name: aws-credentials\n     #          mountPath: /.aws\n     #          readOnly: true\n     #  volumes:\n     #    - name: aws-credentials\n     #      secret:\n     #        secretName: external-dns\nEOF\n\nkubectl create --filename externaldns-with-rbac.yaml \\\n  --namespace ${EXTERNALDNS_NS:-\"default\"}\n</code></pre></p> </li> </ul>","tags":["kubernetes","aws/network/route53"]},{"location":"EKS/addons/externaldns-for-route53/#install-with-eksdemo-","title":"install-with-eksdemo-","text":"<ul> <li>https://github.com/awslabs/eksdemo/blob/main/docs/install-edns.md <pre><code>echo ${CLUSTER_NAME}\necho ${AWS_DEFAULT_REGION}\n\neksdemo install external-dns -c ${CLUSTER_NAME} \n</code></pre> ^a2vlmo</li> </ul>","tags":["kubernetes","aws/network/route53"]},{"location":"EKS/addons/externaldns-for-route53/#sample","title":"sample","text":"<ul> <li>https://github.com/panlm/thanos-example/blob/main/POC-template/query/thanos-query-frontend-service.yaml</li> </ul>","tags":["kubernetes","aws/network/route53"]},{"location":"EKS/addons/externaldns-for-route53/#verify","title":"verify","text":"<ul> <li>create namespace <pre><code>NS=verify\nkubectl create ns ${NS}\n</code></pre></li> </ul> <p>service sample</p> <ul> <li> <p>create nlb (no more clb, 20230423) with service definition <pre><code>envsubst &gt;verify-nginx.yaml &lt;&lt;-EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.${DOMAIN_NAME}\n    service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    name: http\n    targetPort: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n          name: http\nEOF\n\nkubectl create --filename verify-nginx.yaml -n ${NS:-default}\n</code></pre></p> </li> <li> <p>wait nlb available and execute <pre><code>aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\\n  --query \"ResourceRecordSets[?Name == 'nginx.${DOMAIN_NAME}.']|[?Type == 'A']\"\n\naws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\\n  --query \"ResourceRecordSets[?Name == 'nginx.${DOMAIN_NAME}.']|[?Type == 'TXT']\"\n\ndig +short nginx.${DOMAIN_NAME}. A\n\ncurl http://nginx.${DOMAIN_NAME}\n</code></pre></p> </li> </ul> <p>ingress sample</p> <ul> <li> <p>ensure certificate is existed and create alb  <pre><code>echo ${CERTIFICATE_ARN}\n\nenvsubst &gt;verify-nginx-ingress.yaml &lt;&lt;-EOF\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=test,Application=nginx\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    alb.ingress.kubernetes.io/certificate-arn: ${CERTIFICATE_ARN}\nspec:\n  ingressClassName: alb\n  rules:\n\n    - host: server.${DOMAIN_NAME}\n      http:\n        paths:\n          - backend:\n              service:\n                name: nginx\n                port:\n                  number: 80\n            path: /\n            pathType: Prefix\nEOF\n\nkubectl create --filename verify-nginx-ingress.yaml -n ${NS:-default}\n</code></pre></p> </li> <li> <p>wait alb available and execute <pre><code>aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\\n  --query \"ResourceRecordSets[?Name == 'server.${DOMAIN_NAME}.']\"\n\ndig +short server.${DOMAIN_NAME}. A\n\ncurl https://server.${DOMAIN_NAME}\n</code></pre></p> </li> </ul>","tags":["kubernetes","aws/network/route53"]},{"location":"EKS/addons/karpenter-lab/","title":"karpenter-install-lab","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/karpenter","aws/container/eks"]},{"location":"EKS/addons/karpenter-lab/#karpenter-lab","title":"karpenter-lab","text":"","tags":["aws/container/karpenter","aws/container/eks"]},{"location":"EKS/addons/karpenter-lab/#install","title":"install","text":"<ul> <li>https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/</li> </ul> <pre><code>echo ${CLUSTER_NAME:=eks-upgrade-demo}\necho ${AWS_DEFAULT_REGION:=us-east-2}\n\nexport KARPENTER_VERSION=v0.27.5\nexport AWS_ACCOUNT_ID=\"$(aws sts get-caller-identity --query Account --output text)\"\nexport TEMPOUT=$(mktemp)\n\necho $KARPENTER_VERSION $CLUSTER_NAME $AWS_DEFAULT_REGION $AWS_ACCOUNT_ID $TEMPOUT\n</code></pre> <pre><code>curl -fsSL https://karpenter.sh/\"${KARPENTER_VERSION}\"/getting-started/getting-started-with-karpenter/cloudformation.yaml  &gt; $TEMPOUT \\\n&amp;&amp; aws cloudformation deploy \\\n  --stack-name \"Karpenter-${CLUSTER_NAME}\" \\\n  --template-file \"${TEMPOUT}\" \\\n  --capabilities CAPABILITY_NAMED_IAM \\\n  --parameter-overrides \"ClusterName=${CLUSTER_NAME}\"\n</code></pre>","tags":["aws/container/karpenter","aws/container/eks"]},{"location":"EKS/addons/karpenter-lab/#instance-family","title":"instance family","text":"<pre><code>spec:\n  requirements:\n\n  - key: karpenter.k8s.aws/instance-family\n    operator: In\n    values: [c5, m5, r5]\n</code></pre>","tags":["aws/container/karpenter","aws/container/eks"]},{"location":"EKS/addons/karpenter-lab/#install-eks-node-viewer","title":"install eks-node-viewer","text":"<pre><code>go install github.com/awslabs/eks-node-viewer/cmd/eks-node-viewer@latest\nsudo mv -v ~/go/bin/eks-node-viewer /usr/local/bin\n</code></pre>","tags":["aws/container/karpenter","aws/container/eks"]},{"location":"EKS/addons/kube-no-trouble/","title":"kube-no-trouble","text":"<p>[!WARNING] This is a github note</p>","tags":["kubernetes"]},{"location":"EKS/addons/kube-no-trouble/#kube-no-trouble","title":"kube-no-trouble","text":"","tags":["kubernetes"]},{"location":"EKS/addons/kube-no-trouble/#github","title":"github","text":"<p>https://github.com/doitintl/kube-no-trouble</p>","tags":["kubernetes"]},{"location":"EKS/addons/kube-no-trouble/#install","title":"install","text":"<pre><code>sh -c \"$(curl -sSL https://git.io/install-kubent)\"\n</code></pre>","tags":["kubernetes"]},{"location":"EKS/addons/kube-no-trouble/#check-target-version-125","title":"check target version 1.25","text":"<pre><code>kubent -t 1.25\n</code></pre>","tags":["kubernetes"]},{"location":"EKS/addons/kube-no-trouble/#windows","title":"windows","text":"<ul> <li>https://github.com/doitintl/kube-no-trouble/releases/download/nightly-0.7.0-53-g4fa4920/kubent-nightly-0.7.0-53-g4fa4920-windows-amd64.tar.gz</li> </ul>","tags":["kubernetes"]},{"location":"EKS/addons/kube-state-metrics/","title":"kube-state-metrics","text":"<p>[!WARNING] This is a github note</p>","tags":["kubernetes"]},{"location":"EKS/addons/kube-state-metrics/#kube-state-metrics","title":"kube-state-metrics","text":"","tags":["kubernetes"]},{"location":"EKS/addons/kube-state-metrics/#github","title":"github","text":"<ul> <li>https://github.com/kubernetes/kube-state-metrics</li> <li>included by prometheus operator</li> </ul>","tags":["kubernetes"]},{"location":"EKS/addons/kube-state-metrics/#kube-state-metrics-vs-metrics-server","title":"kube-state-metrics vs. metrics-server","text":"<p>The\u00a0metrics-server\u00a0is a project that has been inspired by\u00a0Heapster\u00a0and is implemented to serve the goals of core metrics pipelines in\u00a0Kubernetes monitoring architecture. It is a cluster level component which periodically scrapes metrics from all Kubernetes nodes served by Kubelet through Metrics API. The metrics are aggregated, stored in memory and served in\u00a0Metrics API format. The metrics-server stores the latest values only and is not responsible for forwarding metrics to third-party destinations.</p> <p>kube-state-metrics (KSM) is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. (See examples in the Metrics section below.) It is not focused on the health of the individual Kubernetes components, but rather on the health of the various objects inside, such as deployments, nodes and pods.</p> <p>Having kube-state-metrics as a separate project also enables access to these metrics from monitoring systems such as Prometheus.</p>","tags":["kubernetes"]},{"location":"EKS/addons/metrics-server/","title":"metrics-server","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/metrics-server/#metrics-server","title":"metrics-server","text":"","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/metrics-server/#github","title":"github","text":"<ul> <li>https://github.com/kubernetes-sigs/metrics-server</li> </ul>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/metrics-server/#install","title":"install","text":"","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/metrics-server/#from-yaml","title":"from yaml","text":"<ul> <li>https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/metrics-server.html</li> </ul> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\nkubectl get deployment metrics-server -n kube-system\n</code></pre>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/metrics-server/#from-helm","title":"from helm","text":"<ul> <li>https://artifacthub.io/packages/helm/metrics-server/metrics-server</li> </ul> <pre><code>helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/\n\nhelm upgrade --install metrics-server metrics-server/metrics-server -n kube-system\n</code></pre>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/metrics-server/#sample","title":"sample","text":"<ul> <li>../kubernetes/k8s-hpa-horizontal-pod-autoscaler</li> </ul> <p>**</p>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/","title":"nginx-ingress-controller-community-ver","text":"<p>[!WARNING] This is a github note</p>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#nginx-ingress-controller-community-ver","title":"nginx-ingress-controller-community-ver","text":"","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#install","title":"install","text":"","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#install-with-eksdemo","title":"install with eksdemo","text":"<ul> <li>https://github.com/awslabs/eksdemo/blob/main/docs/install-ingress-nginx.md <pre><code>echo ${CLUSTER_NAME}\neksdemo install ingress-nginx -c ${CLUSTER_NAME} --namespace kube-system\n</code></pre></li> </ul>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#install-manually","title":"install manually","text":"<ul> <li>link <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install my-ing1 ingress-nginx/ingress-nginx\n\nkubectl annotate service/my-ing1-ingress-nginx-controller \\\n  service.beta.kubernetes.io/aws-load-balancer-internal=\"false\" \\\n  service.beta.kubernetes.io/aws-load-balancer-type=nlb \\\n  service.beta.kubernetes.io/aws-load-balancer-nlb-target-type=ip\n\n# need aws lbc \n# service.beta.kubernetes.io/aws-load-balancer-nlb-target-type=ip\n</code></pre></li> </ul>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#resources","title":"resources","text":"<pre><code>$ kubectl apply -f https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/apple.yaml \n$ kubectl apply -f https://raw.githubusercontent.com/cornellanthony/nlb-nginxIngress-eks/master/banana.yaml\n</code></pre>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#tls-certificate","title":"tls certificate","text":"<p>for nlb <pre><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls-n.key -out tls-n.crt -subj \"/CN=*elb.us-east-2.amazonaws.com/O=*.elb.us-east-2.amazonaws.com\"\n\nkubectl create secret tls tls-secret-n --key tls-n.key --cert tls-n.crt\n</code></pre></p> <p>for your domain</p> <ul> <li>create certificate for your domain</li> <li>create cname record in route53 to mapping your domain to nlb domain name</li> <li>using your domain name in ingress yaml definition</li> </ul>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#ingress","title":"ingress","text":"<p>for nlb <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  tls:\n\n  - hosts:\n    - '*.elb.us-east-2.amazonaws.com'\n    secretName: tls-secret-n\n  rules:\n  - host: '*.elb.us-east-2.amazonaws.com'\n    http:\n      paths:\n        - path: /apple\n          pathType: Prefix\n          backend:\n            service: \n              name: apple-service\n              port: \n                number: 5678\n        - path: /banana\n          pathType: Prefix\n          backend:\n            service: \n              name: banana-service\n              port: \n                number: 5678\n</code></pre> (refer: appendix)</p>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#refer","title":"refer","text":"<ul> <li>Using a Network Load Balancer with the NGINX Ingress Controller on Amazon EKS </li> <li>https://kubernetes.github.io/ingress-nginx/deploy/ <pre><code>k apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/aws/deploy.yaml\n</code></pre></li> </ul>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#appendix-for-clb","title":"appendix - for clb","text":"","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#certificate","title":"certificate","text":"<p>for clb <pre><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls-c.key -out tls-c.crt -subj \"/CN=*.us-east-2.elb.amazonaws.com/O=*.us-east-2.elb.amazonaws.com\"\n\nkubectl create secret tls tls-secret-c --key tls-c.key --cert tls-c.crt\n</code></pre></p>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#ingress_1","title":"ingress","text":"<p>for clb <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  tls:\n\n  - hosts:\n    - '*.us-east-2.elb.amazonaws.com'\n    secretName: tls-secret-c\n  rules:\n  - host: '*.us-east-2.elb.amazonaws.com'\n    http:\n      paths:\n        - path: /apple\n          pathType: Prefix\n          backend:\n            service: \n              name: apple-service\n              port: \n                number: 5678\n        - path: /banana\n          pathType: Prefix\n          backend:\n            service: \n              name: banana-service\n              port: \n                number: 5678\n</code></pre></p>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#appendix-ingress-nginx-output","title":"appendix - ingress-nginx-output","text":"<pre><code>LAST DEPLOYED: Mon Aug 15 01:17:43 2022\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe ingress-nginx controller has been installed.\nIt may take a few minutes for the LoadBalancer IP to be available.\nYou can watch the status by running 'kubectl --namespace default get services -o wide -w my-ing1-ingress-nginx-controller'\n\nAn example Ingress that makes use of the controller:\n  apiVersion: networking.k8s.io/v1\n  kind: Ingress\n  metadata:\n    name: example\n    namespace: foo\n  spec:\n    ingressClassName: nginx\n    rules:\n\n      - host: www.example.com\n        http:\n          paths:\n            - pathType: Prefix\n              backend:\n                service:\n                  name: exampleService\n                  port:\n                    number: 80\n              path: /\n    # This section is only required if TLS is to be enabled for the Ingress\n    tls:\n      - hosts:\n        - www.example.com\n        secretName: example-tls\n\nIf TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:\n\n  apiVersion: v1\n  kind: Secret\n  metadata:\n    name: example-tls\n    namespace: foo\n  data:\n    tls.crt: &lt;base64 encoded cert&gt;\n    tls.key: &lt;base64 encoded key&gt;\n  type: kubernetes.io/tls\n</code></pre>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-community-ver/#appendix-other-sample","title":"appendix - other sample","text":"<pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: game-2048-ns2\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: game-2048-ns2\n  name: deployment-2048\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app-2048\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app-2048\n    spec:\n      containers:\n\n      - image: public.ecr.aws/l6m2t8p7/docker-2048:latest\n        imagePullPolicy: Always\n        name: app-2048\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: game-2048-ns2\n  name: service-2048\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n  type: NodePort\n  selector:\n    app.kubernetes.io/name: app-2048\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  namespace: game-2048-ns2\n  name: ingress-2048\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - '*.elb.us-east-2.amazonaws.com'\n    secretName: tls-secret-n\n  rules:\n    - http:\n        paths:\n        - path: /abc(/|$)(.*)\n          pathType: Prefix\n          backend:\n            service:\n              name: service-2048\n              port:\n                number: 80\n</code></pre>","tags":["nginx","kubernetes/ingress","aws/container/eks"]},{"location":"EKS/addons/nginx-ingress-controller-nginx-ver/","title":"nginx-ingress-controller-nginx-ver","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["aws/container/eks","kubernetes/ingress","nginx"]},{"location":"EKS/addons/nginx-ingress-controller-nginx-ver/#nginx-ingress-controller-nginx-version","title":"nginx-ingress-controller-nginx-version","text":"","tags":["aws/container/eks","kubernetes/ingress","nginx"]},{"location":"EKS/addons/nginx-ingress-controller-nginx-ver/#install","title":"install","text":"<ul> <li>https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/</li> </ul> <p><pre><code>helm repo add nginx-stable https://helm.nginx.com/stable\nhelm repo update\nhelm install my-release nginx-stable/nginx-ingress\n\nkubectl annotate service/my-release-nginx-ingress \\\n  service.beta.kubernetes.io/aws-load-balancer-type=nlb\n  \u00a0\u00a0 \u00a0nginx.ingress.kubernetes.io/rewrite-target:\u00a0/:q\n\n\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp\n    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '60'\n    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: 'true'\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n</code></pre> or <pre><code>git clone https://github.com/nginxinc/kubernetes-ingress.git --branch v2.3.0\ncd kubernetes-ingress/deployments/helm-chart\nhelm install my-release .\n</code></pre></p>","tags":["aws/container/eks","kubernetes/ingress","nginx"]},{"location":"EKS/addons/nginx-ingress-controller-nginx-ver/#app-expose","title":"app expose","text":"","tags":["aws/container/eks","kubernetes/ingress","nginx"]},{"location":"EKS/addons/nginx-ingress-controller/","title":"nginx-ingress-controller","text":"<p>[!WARNING] This is a github note</p>","tags":["kubernetes/ingress"]},{"location":"EKS/addons/nginx-ingress-controller/#nginx-ingress-controller","title":"nginx-ingress-controller","text":"","tags":["kubernetes/ingress"]},{"location":"EKS/addons/nginx-ingress-controller/#understand-version","title":"understand version","text":"<ul> <li> <p>Community version\u00a0\u2013 Found in the\u00a0kubernetes/ingress-nginx\u00a0repo on GitHub, the community Ingress controller is based on NGINX Open Source with docs on\u00a0Kubernetes.io. It is maintained by the Kubernetes community with a\u00a0commitment from F5\u00a0NGINX\u00a0to help manage the project</p> </li> <li> <p>NGINX version\u00a0\u2013 Found in the\u00a0nginxinc/kubernetes-ingress\u00a0repo on GitHub, NGINX Ingress Controller is developed and maintained by F5\u00a0NGINX with docs on\u00a0docs.nginx.com. It is available in two editions:</p> <ul> <li>NGINX Open Source\u2011based (free and open source option)</li> <li>NGINX Plus-based\u00a0(commercial option)</li> </ul> </li> <li> <p>nginx-ingress-controller-community-ver </p> </li> <li>nginx-ingress-controller-nginx-ver</li> </ul>","tags":["kubernetes/ingress"]},{"location":"EKS/addons/nginx-ingress-controller/#refer","title":"refer","text":"<ul> <li>Exposing Kubernetes Applications Part 3 NGINX Ingress Controller </li> </ul>","tags":["kubernetes/ingress"]},{"location":"EKS/addons/nginx-ingress-controller/#compatibility","title":"compatibility","text":"<ul> <li>https://github.com/kubernetes/ingress-nginx#supported-versions-table</li> </ul>","tags":["kubernetes/ingress"]},{"location":"EKS/addons/pluto/","title":"pluto","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["kubernetes"]},{"location":"EKS/addons/pluto/#pluto-cmd","title":"pluto-cmd","text":"<ul> <li>scan</li> <li>install<ul> <li>asdf</li> <li>pluto</li> </ul> </li> </ul>","tags":["kubernetes"]},{"location":"EKS/addons/pluto/#scan","title":"scan","text":"<ul> <li>scan folder <pre><code>pluto detect-files -d .\n</code></pre></li> <li>scan helm <pre><code>pluto detect-helm -o wide\n</code></pre></li> </ul>","tags":["kubernetes"]},{"location":"EKS/addons/pluto/#install","title":"install","text":"","tags":["kubernetes"]},{"location":"EKS/addons/pluto/#asdf","title":"asdf","text":"<ul> <li>https://asdf-vm.com/guide/getting-started.html#_1-install-dependencies <pre><code>git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.11.3\necho '. \"$HOME/.asdf/asdf.sh\"' &gt;&gt; ~/.bash_profile\necho '. \"$HOME/.asdf/completions/asdf.bash\"' &gt;&gt; ~/.bash_profile\nsource ~/.bash_profile\n</code></pre></li> </ul>","tags":["kubernetes"]},{"location":"EKS/addons/pluto/#pluto","title":"pluto","text":"<ul> <li>https://pluto.docs.fairwinds.com/installation/#asdf <pre><code>asdf plugin-add pluto\nlastest_version=$(asdf list-all pluto |tail -n 1)\nasdf install pluto ${lastest_version}\nasdf local pluto ${lastest_version}\npluto version\n</code></pre></li> </ul>","tags":["kubernetes"]},{"location":"EKS/cluster/eks-cluster-addons-list/","title":"EKS Addons","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/cluster/eks-cluster-addons-list/#eks-addons","title":"EKS Addons","text":"","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/cluster/eks-cluster-addons-list/#list","title":"list","text":"<ul> <li> <p>\u6258\u7ba1\u96c6\u7fa4\u63d2\u4ef6</p> <ul> <li>../addons/eks-addons-coredns </li> <li>../addons/eks-addons-vpc-cni </li> <li>../addons/eks-addons-kube-proxy </li> </ul> </li> <li> <p>\u7b2c\u4e09\u65b9\u63d2\u4ef6</p> <ul> <li>../addons/aws-load-balancer-controller </li> <li>externaldns </li> <li>../addons/cert-manager </li> <li>ebs-csi-driver </li> <li>prometheus operator / grafana operator</li> <li>splunk-otel-collector </li> <li>../addons/nginx-ingress-controller </li> <li>../addons/metrics-server </li> <li>../addons/cluster-autoscaler </li> </ul> </li> <li> <p>\u5176\u4ed6\u63d2\u4ef6</p> <ul> <li>efs-csi-driver </li> <li>cloudwatch-agent / fluentbit </li> <li>kubernetes-dashboard </li> <li>tigera-operator for calico </li> <li>../addons/cni-metrics-helper </li> <li>../addons/kube-state-metrics </li> </ul> </li> </ul>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/cluster/eks-cluster-addons-list/#upgrade-addons-sample","title":"upgrade addons sample","text":"<ul> <li>https://aws.amazon.com/blogs/containers/amazon-eks-add-ons-preserve-customer-edits/</li> <li>https://aws.amazon.com/cn/blogs/containers/amazon-eks-add-ons-advanced-configuration/ <pre><code>CLUSTER_NAME=ekscluster1\nADDON_NAME=coredns\n\naws eks describe-addon \\\n--cluster-name ${CLUSTER_NAME} \\\n--addon-name ${ADDON_NAME} | grep status\n\naws eks update-addon \\\n--cluster-name ${CLUSTER_NAME} \\\n--addon-name ${ADDON_NAME} \\\n--addon-version v1.8.7-eksbuild.3 \\\n--resolve-conflicts PRESERVE\n</code></pre></li> </ul>","tags":["aws/container/eks","kubernetes"]},{"location":"EKS/cluster/eks-cluster-with-terraform/","title":"Create EKS Cluster with Terraform","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","terraform"]},{"location":"EKS/cluster/eks-cluster-with-terraform/#create-eks-cluster-with-terraform","title":"Create EKS Cluster with Terraform","text":"","tags":["aws/container/eks","terraform"]},{"location":"EKS/cluster/eks-cluster-with-terraform/#install-terraform","title":"install terraform","text":"<ul> <li>this step has been included in setup-cloud9-for-eks <pre><code>sudo yum install -y yum-utils shadow-utils\nsudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo\nsudo yum -y install terraform\n</code></pre></li> </ul>","tags":["aws/container/eks","terraform"]},{"location":"EKS/cluster/eks-cluster-with-terraform/#sample-create-3x-clusters-for-thanos-poc-","title":"sample-create-3x-clusters-for-thanos-poc-","text":"<ul> <li> <p>get terraform template  <pre><code>git clone https://github.com/panlm/eks-blueprints-clusters.git\ncd eks-blueprints-clusters/multi-cluster-thanos\n</code></pre></p> </li> <li> <p>execute function to create an existed host zone (../addons/externaldns-for-route53) <pre><code>DOMAIN_NAME=eks1224.aws.panlm.xyz\ncreate-host-zone -n ${DOMAIN_NAME}\n</code></pre></p> </li> <li> <p>need setup upstream domain registry from your labtop (git/git-mkdocs/CLI/awscli/route53-cmd)</p> </li> <li> <p>terraform.tfvars <pre><code>aws_region          = \"us-east-2\"\nenvironment_name    = \"thanos\"\ncluster_version     = \"1.27\"\nhosted_zone_name    = \"eks1206.aws.panlm.xyz\" # your Existing Hosted Zone\neks_admin_role_name = \"panlm\" # Additional role admin in the cluster \n</code></pre></p> </li> <li> <p>build environment <pre><code>cd environment\nterraform init\nterraform apply -auto-approve\n</code></pre></p> </li> <li> <p>create ekscluster1, following output to save kubeconfig file <pre><code>cd ekscluster1\nterraform init\nterraform apply -auto-approve\n</code></pre></p> </li> <li> <p>create ekscluster2 and ekscluster3 from their folder with same commands</p> </li> <li> <p>in each eks cluster, will install following addons by argocd. access argocd svc url with default password saved in aws secret manager </p> </li> </ul>","tags":["aws/container/eks","terraform"]},{"location":"EKS/cluster/eks-cluster-with-terraform/#internal-error","title":"internal error","text":"<ul> <li>re-run <code>terraform apply</code> if you got following errors <pre><code>\u2502       * Internal error occurred: failed calling webhook \"mservice.elbv2.k8s.aws\": failed to call webhook: Post \"https://aws-load-balancer-webhook-service.kube-system.svc:443/mutate-v1-service?timeout=10s\": no endpoints available for service \"aws-load-balancer-webhook-service\"\n\u2502 \n\u2502   with module.eks_cluster.module.eks_blueprints_addons.module.cert_manager.helm_release.this[0],\n\u2502   on .terraform/modules/eks_cluster.eks_blueprints_addons.cert_manager/main.tf line 9, in resource \"helm_release\" \"this\":\n\u2502    9: resource \"helm_release\" \"this\" {\n</code></pre></li> </ul>","tags":["aws/container/eks","terraform"]},{"location":"EKS/cluster/eks-cluster-with-terraform/#refer","title":"refer","text":"<ul> <li>eks-blueprints-blue-green-upgrade</li> <li>helm-in-terraform</li> </ul>","tags":["aws/container/eks","terraform"]},{"location":"EKS/cluster/eks-private-access-cluster/","title":"Create Private Only EKS Cluster","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#create-private-only-eks-cluster","title":"Create Private Only EKS Cluster","text":"","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#prep-bastion","title":"prep bastion","text":"<ul> <li>\u521b\u5efavpc\u548ccloud9 <ul> <li>../../cloud9/create-standard-vpc-for-lab-in-china-region or hugo link </li> <li>\u5982\u679c\u53ea\u9700\u8981\u521b\u5efa\u6258\u7ba1\u8282\u70b9\u7ec4\uff0c\u79c1\u6709\u5b50\u7f51\u8def\u7531\u8868\u53ef\u4ee5\u6ca1\u6709\u6307\u5411 nat \u7684\u8def\u7531</li> <li>\u5982\u679c\u9700\u8981\u521b\u5efa\u81ea\u7ba1\u8282\u70b9\u7ec4\uff0c\u79c1\u6709\u5b50\u7f51\u8def\u7531\u8868\u9700\u8981\u6709\u6307\u5411 nat \u7684\u8def\u7531\uff0c\u5426\u5219\u8282\u70b9\u52a0\u96c6\u7fa4\u5931\u8d25\uff0c\u6216\u8005\u63d0\u524d\u521b\u5efa eks endpoint\u3002 <pre><code>Connect timeout on endpoint URL: \"https://eks.us-east-1.amazonaws.com/clusters/ekscluster-name\"\nExited with error on line 351\n</code></pre></li> </ul> </li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#prep-cloud9","title":"prep cloud9","text":"<ul> <li> <p>\u5b89\u88c5\u5fc5\u8981\u7684\u8f6f\u4ef6 </p> <ul> <li>../../cloud9/setup-cloud9-for-eks or hugo link <pre><code>sudo yum -y install jq gettext bash-completion moreutils wget\n</code></pre></li> </ul> </li> <li> <p>\u521b\u5efa\u5b89\u5168\u7ec4 eks-shared-sg\uff0cinbound\u89c4\u5219\u662f\u81ea\u5df1 (needed if your cluster is private only mode ) <pre><code># export VPC_ID=vpc-xxxxxxxx\n# export AWS_REGION=cn-north-1\nAWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\nINST_ID=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.instanceId')\nVPC_ID=$(aws ec2 describe-instances --instance-ids ${INST_ID} --region ${AWS_REGION} |jq -r '.Reservations[0].Instances[0].VpcId')\n\nSG_NAME=eks-shared-sg\nSG_ID=$(aws ec2 describe-security-groups --region $AWS_REGION \\\n--filter Name=vpc-id,Values=$VPC_ID \\\n--query \"SecurityGroups[?GroupName == '\"${SG_NAME}\"'].GroupId\" \\\n--output text)\n\n# if SG does not existed, then create it\nif [[ -z ${SG_ID} ]]; then\nSG_ID=$(aws ec2 create-security-group \\\n  --description ${SG_NAME} \\\n  --group-name ${SG_NAME} \\\n  --vpc-id ${VPC_ID} \\\n  --query 'GroupId' \\\n  --output text )\naws ec2 authorize-security-group-ingress \\\n    --group-id ${SG_ID} \\\n    --protocol all \\\n    --source-group ${SG_ID}\nfi\n</code></pre></p> </li> <li> <p>assign security group to cloud9 instance <pre><code>SG_LIST=$(aws ec2 describe-instance-attribute --instance-id $INST_ID --attribute groupSet --query 'Groups[*].[GroupId]' --output text)\n\n# before\naws ec2 describe-instance-attribute --instance-id $INST_ID --attribute groupSet\n# assign\naws ec2 modify-instance-attribute --instance-id $INST_ID --groups $SG_LIST $SG_ID\n# after\naws ec2 describe-instance-attribute --instance-id $INST_ID --attribute groupSet\n</code></pre></p> </li> <li> <p>if you create private only cluster in vpc which you have created with public/private eks endpoint, using the Shared SG of the previous cluster</p> </li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#prep-config-","title":"prep-config-","text":"<ul> <li>\u521b\u5efa\u5b8c\u81ea\u5b9a\u4e49 vpc \u540e\uff0c\u76f4\u63a5\u6267\u884c\u4e0b\u9762\u4ee3\u7801 <pre><code>echo ${AWS_REGION}\nACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)\nAZS=($(aws ec2 describe-availability-zones --query 'AvailabilityZones[].ZoneName' --output text --region $AWS_REGION))\n\necho \"export VPC_ID=${VPC_ID}\" \necho \"export AWS_REGION=${AWS_REGION}\"\necho \"export AZS=(${AZS[@]})\"\n\n# output yaml format for vpc/subnet info\n( echo 'vpc:'\necho '  id:' ${VPC_ID}\necho '  subnets:'\necho '    private:'\nfor i in ${AZS[@]} ; do\n    subnetid=$(aws ec2 describe-subnets \\\n    --filter \"Name=availability-zone,Values=$i\" \"Name=vpc-id,Values=$VPC_ID\" \"Name=tag-key,Values=kubernetes.io/role/internal-elb\" \\\n    --query 'Subnets[?MapPublicIpOnLaunch==`false`].SubnetId' --output text)\n    if [[ ! -z $subnetid ]]; then\n        echo \"      ${i}:\"\n        echo -e \"        id: $subnetid\"\n    fi\ndone\necho '    public:'\nfor i in ${AZS[@]} ; do\n    subnetid=$(aws ec2 describe-subnets \\\n    --filter \"Name=availability-zone,Values=$i\" \"Name=vpc-id,Values=$VPC_ID\" \\\n    --query 'Subnets[?MapPublicIpOnLaunch==`true`].SubnetId' --output text)\n    if [[ ! -z $subnetid ]]; then\n        echo \"      ${i}:\"\n        echo -e \"        id: $subnetid\"\n    fi\ndone\nif [ ! -z $SG_ID ]; then\n    echo \"  sharedNodeSecurityGroup: $SG_ID\"\nfi )\n</code></pre></li> </ul> <p>^h86u1r</p> <ul> <li>output will be used in next step</li> <li>ensure you have no s3 endpoint in your target vpc <ul> <li>you could have ssm/ssmmessages endpoint</li> </ul> </li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#cluster-yaml","title":"cluster yaml","text":"<pre><code>touch cluster1.yaml\n</code></pre> <ul> <li>commercial region sample config<ul> <li>check here for china region sample config <pre><code>---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: ekscluster-privonly # MODIFY cluster name\n  region: \"us-east-2\" # MODIFY region\n  version: \"1.24\" # MODIFY version\n\n# full private cluster\nprivateCluster:\n  enabled: true \n  skipEndpointCreation: true # uncomment, if you create 2nd cluster in same vpc\n#   additionalEndpointServices:\n#   - \"autoscaling\"\n#   - \"logs\"\n#   - \"cloudformation\"\n\n# REPLACE THIS CODE BLOCK\nvpc:\n  subnets:\n    private:\n      us-east-2a:\n        id: subnet-xxxxxxxx\n      us-east-2b:\n        id: subnet-xxxxxxxx\n    public:\n      us-east-2a:\n        id: subnet-xxxxxxxx\n      us-east-2b:\n        id: subnet-xxxxxxxx\n  sharedNodeSecurityGroup: sg-xxxxxxxx\n\ncloudWatch:\n  clusterLogging:\n    enableTypes: [\"*\"]\n\n# secretsEncryption:\n#   keyARN: ${MASTER_ARN}\n\nmanagedNodeGroups:\n\n- name: mng1\n  minSize: 1\n  maxSize: 5\n  desiredCapacity: 1\n  instanceType: m5.large\n  ssh:\n    enableSsm: true\n  privateNetworking: true\n\nnodeGroups:\n\n- name: ng1\n  minSize: 1\n  maxSize: 5\n  desiredCapacity: 1\n  instanceType: m5.large\n  ssh:\n    enableSsm: true\n  privateNetworking: true\n  ami: ami-06a8057d9b6a06ee6\n  amiFamily: AmazonLinux2\n  overrideBootstrapCommand: |\n    #!/bin/bash\n    source /var/lib/cloud/scripts/eksctl/bootstrap.helper.sh\n    /etc/eks/bootstrap.sh ${CLUSTER_NAME} --container-runtime containerd --kubelet-extra-args \"--node-labels=${NODE_LABELS}\"\n\niam:\n  withOIDC: true\n\naddons:\n\n- name: vpc-cni \n  version: latest\n- name: coredns\n  version: latest # auto discovers the latest available\n- name: kube-proxy\n  version: latest\n</code></pre></li> </ul> </li> </ul> <pre><code>eksctl create cluster -f cluster1.yaml\n</code></pre> <pre><code># get optimized eks ami id for your version &amp; region\nEKS_VERSION=1.24\n# AWS_REGION=us-east-2\naws ssm get-parameter --name /aws/service/eks/optimized-ami/${EKS_VERSION}/amazon-linux-2/recommended/image_id --region ${AWS_REGION} --query \"Parameter.Value\" --output text\n</code></pre>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#endpoint","title":"endpoint","text":"<p>https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html#private-cluster-requirements</p> <ul> <li>\u8bbe\u7f6e <code>skipEndpointCreation: false</code>\uff0c\u96c6\u7fa4\u521b\u5efa\u5b8c\u6210\u540e\u5c06\u81ea\u52a8\u521b\u5efa\u4ee5\u4e0b endpoint\uff0c\u5e76\u4e14\u7ed1\u5b9a <code>sharedNodeSecurityGroup</code> \u4e2d\u6307\u5b9a\u7684\u5b89\u5168\u7ec4<ul> <li>logs</li> <li>s3 (gw)</li> <li>sts</li> <li>ecr.api</li> <li>ec2</li> <li>ecr.dkr</li> </ul> </li> <li>\u4f60\u53ef\u4ee5\u81ea\u5df1\u521b\u5efa\u4ee5\u4e0b endpoint\uff0c\u5e76\u4e14\u7ed1\u5b9a\u5b89\u5168\u7ec4\u5141\u8bb8 80 443 \u8bbf\u95ee<ul> <li>ssm</li> <li>ssmmessages</li> </ul> </li> <li>\u53e6\u5916\u8fd8\u9700\u8981\u63d0\u524d\u521b\u5efa\u4ee5\u4e0b endpoint\uff0c\u5e76\u4e14\u7ed1\u5b9a\u5b89\u5168\u7ec4<ul> <li>eks \uff08\u4f7f\u7528\u81ea\u7ba1\u8282\u70b9\u7ec4\u65f6\u9700\u8981\uff09</li> <li>elasticfilesystem \uff08\u4f7f\u7528 efs \u65f6\u9700\u8981\uff09</li> <li>elasticloadbalancing \uff08\u4f7f\u7528 aws lb controller\u65f6\u9700\u8981\uff09</li> <li>kms \uff08\u5f85\u9a8c\u8bc1\uff09</li> <li>ebs \uff08\u5f85\u9a8c\u8bc1\uff09</li> </ul> </li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#access-cluster","title":"access cluster","text":"<ul> <li>create-kubeconfig-manually</li> <li>recover-access-eks</li> <li>token-different</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#issue-about-kubectl","title":"issue about kubectl","text":"","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#solve-1","title":"solve 1","text":"<ul> <li>download aws-iam-authenticator, and then run write-kubeconfig command it will using aws-iam-authenticator instead of aws to create kubeconfig</li> <li><code>aws eks update-kubeconfig</code> default using aws</li> <li><code>eksctl utils write-kubeconfig</code> default using aws-iam-authenticator if you have installed</li> </ul> <pre><code>eksctl utils write-kubeconfig --cluster ekscluster1\n</code></pre>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#solve-2","title":"solve 2","text":"<ul> <li>check if null TOKEN variable <code>aws_session_token=</code> in your <code>~/.aws/credentials</code></li> <li>delete it</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#network-topo-preview","title":"network topo preview","text":"<ul> <li>TC-security-group-for-eks-deepdive</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-private-access-cluster/#reference","title":"reference","text":"<ul> <li>eks-public-access-cluster</li> <li>eks-nodegroup</li> <li>eksctl-sample-priv-addons</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-public-access-cluster-in-china-region/","title":"Create Public Access EKS Cluster in China Region","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","aws/china"]},{"location":"EKS/cluster/eks-public-access-cluster-in-china-region/#create-public-access-eks-cluster-in-china-region","title":"Create Public Access EKS Cluster in China Region","text":"<ol> <li>create cloud9</li> <li>create vpc </li> </ol> <ul> <li>refer: git/git-mkdocs/cloud9/create-standard-vpc-for-lab-in-china-region</li> </ul> <ol> <li>get vpc id </li> </ol> <ul> <li>refer: git/git-mkdocs/cloud9/create-standard-vpc-for-lab-in-china-region</li> </ul> <ol> <li>pre-config </li> </ol> <ul> <li>refer: eks-private-access-cluster</li> </ul> <ol> <li>cluster config <pre><code>---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: ekscluster131 # MODIFY cluster name\n  region: \"cn-north-1\" # MODIFY region\n  version: \"1.24\" # MODIFY version\n\n# REPLACE THIS CODE BLOCK\nvpc:\n  subnets:\n    private:\n      us-east-2a:\n        id: subnet-xxxxxxxx\n      us-east-2b:\n        id: subnet-xxxxxxxx\n    public:\n      us-east-2a:\n        id: subnet-xxxxxxxx\n      us-east-2b:\n        id: subnet-xxxxxxxx\n  sharedNodeSecurityGroup: sg-xxxxxxxx\n\ncloudWatch:\n  clusterLogging:\n    enableTypes: [\"*\"]\n\n# secretsEncryption:\n#   keyARN: ${MASTER_ARN}\n\nmanagedNodeGroups:\n\n- name: mng1\n  minSize: 2\n  maxSize: 5\n  desiredCapacity: 2\n  instanceType: m5.large\n  ssh:\n    enableSsm: true\n  privateNetworking: true\n\niam:\n  withOIDC: true\n\naddons:\n\n- name: vpc-cni \n  version: latest\n- name: coredns\n  version: latest # auto discovers the latest available\n- name: kube-proxy\n  version: latest\n</code></pre></li> </ol> <p>^8ir6w8</p>","tags":["aws/container/eks","aws/china"]},{"location":"EKS/cluster/eks-public-access-cluster/","title":"Create Public Access EKS Cluster","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-public-access-cluster/#create-public-access-eks-cluster","title":"Create Public Access EKS Cluster","text":"","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-public-access-cluster/#prep","title":"prep","text":"<ul> <li>do not need to create vpc in advance</li> <li>../../cloud9/setup-cloud9-for-eks or using your local environment</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-public-access-cluster/#create-cluster-from-scratch","title":"create cluster from scratch","text":"<ul> <li>don\u2019t put <code>subnets</code>/<code>sharedNodeSecurityGroup</code> in your <code>vpc</code> section. eksctl will create a clean vpc for you</li> <li>don\u2019t use <code>privateCluster</code> section, you could make cluster api server endpoint <code>public</code> or <code>public and private</code></li> <li>you still could put your group node in private subnet for security consideration</li> <li>recommend for most of POC environment</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-public-access-cluster/#create-eks-cluster","title":"create eks cluster","text":"<ul> <li> <p>\u5c06\u5728\u4e0b\u9762\u533a\u57df\u521b\u5efa EKS \u96c6\u7fa4 (prepare to create eks cluster) <pre><code>export AWS_PAGER=\"\"\nexport AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\nexport AWS_REGION=${AWS_DEFAULT_REGION}\n\nexport CLUSTER_NAME=ekscluster1\nexport EKS_VERSION=1.25\nCLUSTER_NUM=$(eksctl get cluster |wc -l)\nexport CIDR=\"10.25${CLUSTER_NUM}.0.0/16\"\n</code></pre></p> </li> <li> <p>\u6267\u884c\u4e0b\u9762\u4ee3\u7801\u521b\u5efa\u914d\u7f6e\u6587\u4ef6 (create eks cluster)</p> <ul> <li>\u6ce8\u610f\u96c6\u7fa4\u540d\u79f0</li> <li>\u6ce8\u610f\u4f7f\u7528\u7684 AZ \u7b26\u5408\u4f60\u6240\u5728\u7684\u533a\u57df <pre><code>AZS=($(aws ec2 describe-availability-zones \\\n--query 'AvailabilityZones[].ZoneName' --output text |awk '{print $1,$2}'))\nexport AZ0=${AZS[0]}\nexport AZ1=${AZS[1]}\n\ncat &gt;$$.yaml &lt;&lt;-'EOF'\n---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: \"${CLUSTER_NAME}\"\n  region: \"${AWS_REGION}\"\n  version: \"${EKS_VERSION}\"\n\navailabilityZones: [\"${AZ0}\", \"${AZ1}\"]\n\nvpc:\n  cidr: \"${CIDR}\"\n  clusterEndpoints:\n    privateAccess: true\n    publicAccess: true\n\ncloudWatch:\n  clusterLogging:\n    enableTypes: [\"*\"]\n\n# secretsEncryption:\n#   keyARN: ${MASTER_ARN}\n\nmanagedNodeGroups:\n\n- name: managed-ng\n  minSize: 1\n  maxSize: 5\n  desiredCapacity: 2\n  instanceType: m5.large\n  privateNetworking: true\n\naddons:\n\n- name: vpc-cni \n  version: latest\n- name: coredns\n  version: latest \n- name: kube-proxy\n  version: latest\n\niam:\n  withOIDC: true\n  serviceAccounts:\n\n  - metadata:\n      name: aws-load-balancer-controller\n      namespace: kube-system\n    wellKnownPolicies:\n      awsLoadBalancerController: true\n  - metadata:\n      name: ebs-csi-controller-sa\n      namespace: kube-system\n    wellKnownPolicies:\n      ebsCSIController: true\n  - metadata:\n      name: efs-csi-controller-sa\n      namespace: kube-system\n    wellKnownPolicies:\n      efsCSIController: true\n  - metadata:\n      name: cloudwatch-agent\n      namespace: amazon-cloudwatch\n    attachPolicyARNs:\n    - \"arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\"\n  - metadata:\n      name: fluent-bit\n      namespace: amazon-cloudwatch\n    attachPolicyARNs:\n    - \"arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\"\nEOF\ncat $$.yaml |envsubst '$CLUSTER_NAME $AWS_REGION $AZ0 $AZ1 $EKS_VERSION $CIDR ' &gt; cluster-${CLUSTER_NAME}.yaml\n</code></pre></li> </ul> </li> <li> <p>\u521b\u5efa\u96c6\u7fa4\uff0c\u9884\u8ba1\u9700\u8981 20 \u5206\u949f (wait about 20 mins) <pre><code>eksctl create cluster -f cluster-${CLUSTER_NAME}.yaml\n</code></pre></p> </li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-public-access-cluster/#get-newest-ami","title":"get-newest-ami","text":"<ul> <li> <p>get newest ami id for your self-managed node group, for GPU or Graviton instance (link) <pre><code>echo ${AWS_REGION}\necho ${EKS_VERSION}\n\nexport AMI=$(aws ssm get-parameter --name /aws/service/eks/optimized-ami/${EKS_VERSION}/amazon-linux-2/recommended/image_id --region ${AWS_REGION} --query \"Parameter.Value\" --output text)\n\ncat  &lt;&lt;-'EOF' |envsubst '$AMI' |tee -a cluster-${CLUSTER_NAME}.yaml\nnodeGroups:\n\n- name: ng1\n  minSize: 1\n  maxSize: 5\n  desiredCapacity: 1\n  instanceType: m5.large\n  ssh:\n    enableSsm: true\n  privateNetworking: true\n  ami: \"${AMI}\"\n  amiFamily: AmazonLinux2\n  overrideBootstrapCommand: |\n    #!/bin/bash\n    source /var/lib/cloud/scripts/eksctl/bootstrap.helper.sh\n    /etc/eks/bootstrap.sh ${CLUSTER_NAME} --container-runtime containerd --kubelet-extra-args \"--node-labels=${NODE_LABELS} --max-pods=110\"\nEOF\n</code></pre></p> </li> <li> <p>\u6258\u7ba1\u8282\u70b9\u6ca1\u6709 <code>/var/lib/cloud/scripts/eksctl/bootstrap.helper.sh</code> \u811a\u672c\uff0c\u5bfc\u81f4\u5f53\u4f7f\u7528\u5b9a\u5236 ami \u5e76\u4e14 extra args \u4e2d NODE_LABELS \u53c2\u6570\u4e22\u5931\u3002(refer link)</p> </li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-public-access-cluster/#access-eks-cluster-from-web-console","title":"access eks cluster from web console","text":"<ul> <li>\u5c06\u5b9e\u9a8c\u73af\u5883\u5bf9\u5e94\u7684 <code>TeamRole</code> \u89d2\u8272\u4f5c\u4e3a\u96c6\u7fa4\u7ba1\u7406\u5458\uff0c\u65b9\u4fbf\u4f7f\u7528 web \u9875\u9762\u67e5\u770b eks \u96c6\u7fa4 <pre><code>echo ${CLUSTER_NAME}\n\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nAWS_REGION=us-east-2\n\nfor i in TeamRole WSOpsRole WSParticipantRole WSAdminRole ; do\neksctl create iamidentitymapping \\\n  --cluster ${CLUSTER_NAME} \\\n  --arn arn:aws:iam::${ACCOUNT_ID}:role/${i} \\\n  --username cluster-admin \\\n  --group system:masters \\\n  --region ${AWS_REGION}\ndone\n</code></pre></li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-public-access-cluster/#create-cluster-in-existed-vpc","title":"create cluster in existed VPC","text":"<ul> <li>get target vpc id</li> <li>create SG (../../CLI/awscli/ec2-cmd)<ul> <li>or using existed cluster\u2019s shared SG (see chapter refer)</li> </ul> </li> <li>get vpc info (eks-private-access-cluster)</li> <li>cluster yaml <pre><code>---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: ekscluster2 # MODIFY cluster name\n  region: \"us-east-2\" # MODIFY region\n  version: \"1.25\" # MODIFY version\n\n# REPLACE THIS CODE BLOCK\nvpc:\n  subnets:\n    private:\n      us-east-2a:\n        id: subnet-xxxxxxxx\n      us-east-2b:\n        id: subnet-xxxxxxxx\n    public:\n      us-east-2a:\n        id: subnet-xxxxxxxx\n      us-east-2b:\n        id: subnet-xxxxxxxx\n  sharedNodeSecurityGroup: sg-xxxxxxxx\n  clusterEndpoints:\n    privateAccess: true\n    publicAccess: true\n\ncloudWatch:\n  clusterLogging:\n    enableTypes: [\"*\"]\n\n# secretsEncryption:\n#   keyARN: ${MASTER_ARN}\n\nmanagedNodeGroups:\n\n- name: mng1\n  minSize: 1\n  maxSize: 5\n  desiredCapacity: 2\n  instanceType: m5.large\n  ssh:\n    enableSsm: true\n  privateNetworking: true\n\niam:\n  withOIDC: true\n\naddons:\n\n- name: vpc-cni \n  version: latest\n- name: coredns\n  version: latest # auto discovers the latest available\n- name: kube-proxy\n  version: latest\n</code></pre></li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-public-access-cluster/#refer","title":"refer","text":"<ul> <li>TC-security-group-for-eks-deepdive</li> <li>eks-private-access-cluster</li> <li>eks-nodegroup</li> <li>eksctl-default-tags-on-subnet</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-upgrade-procedure/","title":"EKS Upgrade Procedure","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-upgrade-procedure/#eks-upgrade-procedure","title":"EKS Upgrade Procedure","text":"","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-upgrade-procedure/#workshop","title":"workshop","text":"<ul> <li>\u4e2d\u6587\u5347\u7ea7 workshop</li> <li>eks-upgrade-lab<ul> <li>https://eks-upgrades-workshop.netlify.app/</li> </ul> </li> <li>Workshop </li> <li>Accelerate software development lifecycles with GitOps</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-upgrade-procedure/#_1","title":"\u6d41\u7a0b","text":"<p>1: \u68c0\u67e5\u5e94\u7528\u914d\u7f6e\u6587\u4ef6\u517c\u5bb9\u6027</p> <ul> <li>kube-no-trouble </li> <li>pluto </li> <li>eksup </li> <li>\u68c0\u67e5\u7b2c\u4e09\u65b9\u63d2\u4ef6 (eks-cluster-addons-list)</li> </ul> <p>2: \u5347\u7ea7\u6838\u5fc3addon \uff08\u5982\u679c\u76ee\u6807\u7248\u672c\u548c addon \u6709\u517c\u5bb9\u95ee\u9898\u5219\u5148\u5347\u7ea7\uff0c\u5426\u5219\u5728\u5347\u7ea7\u5b8c\u7ba1\u7406\u8282\u70b9\u540e\u5347\u7ea7\uff09</p> <ul> <li>coredns: <ul> <li>\u6258\u7ba1dns addon (managed-coredns)</li> <li>\u81ea\u7ba1dns addon (self-managed-coredns)</li> </ul> </li> <li>aws-node: upgrade-vpc-cni </li> <li>kube-proxy: eks-addons-kube-proxy</li> </ul> <p>3: \u5347\u7ea7eks\u63a7\u5236\u5e73\u9762</p> <p>4: \u5347\u7ea7eks\u7ba1\u7406\u8282\u70b9</p> <ul> <li>\u6258\u7ba1\u8282\u70b9\u7684\u66f4\u65b0 LINK <ul> <li>ssm-document-eks-node-upgrade </li> </ul> </li> <li>\u81ea\u7ba1\u8282\u70b9\u7684\u66f4\u65b0 LINK </li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-upgrade-procedure/#others","title":"others","text":"<ul> <li>mm-eks-upgrade-workshop-walkley</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-upgrade-procedure/#deck","title":"deck","text":"","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-upgrade-procedure/#docs-history","title":"docs history","text":"<ul> <li>for release 1.22 <ul> <li>https://github.com/awsdocs/amazon-eks-user-guide/blob/cb60bb7b2b78220f2f8809bbd640ec4d0fbcb5eb/doc_source/kubernetes-versions.md</li> </ul> </li> <li>for release 1.21 and before<ul> <li>https://github.com/awsdocs/amazon-eks-user-guide/blob/a7e7162191efbfdb256ffeb4ec26757c7f3cc7b3/doc_source/kubernetes-versions.md</li> </ul> </li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-upgrade-procedure/#refer","title":"refer","text":"<ul> <li>Amazon EKS \u96c6\u7fa4\u5347\u7ea7\u6307\u5357 </li> <li>amazon-eks-\u7248\u672c\u7ba1\u7406\u7b56\u7565\u4e0e\u5347\u7ea7\u6d41\u7a0b </li> <li>Automate Amazon EKS upgrades with infrastructure as code </li> <li>GCR Resilience Series - EKS Resilience</li> <li>https://kubernetes.io/releases/version-skew-policy/</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/cluster/eks-upgrade-procedure/#_2","title":"\u53c2\u8003\u6587\u6863","text":"<ul> <li>Kubernetes\u5b98\u65b9\u6587\u6863: Kubernetes Release Cycle</li> <li>Kubernetes\u5b98\u65b9\u6587\u6863: Kubernetes Deprecation Policy</li> <li>Kubernetes\u535a\u5ba2: Increasing the Kubernetes Support Window to One Year</li> <li>AWS\u535a\u5ba2: Planning Kubernetes Upgrades with Amazon EKS</li> <li>AWS\u535a\u5ba2: Making Cluster Updates Easy with Amazon EKS</li> <li>AWS\u5b98\u65b9\u6587\u6863: Amazon EKS Kubernetes versions</li> <li>AWS\u5b98\u65b9\u6587\u6863: Updating a cluster</li> <li>EKS\u6700\u4f73\u5b9e\u8df5\u624b\u518c: Handling Cluster Upgrades</li> </ul>","tags":["aws/container/eks"]},{"location":"EKS/kubernetes/k8s-hpa-horizontal-pod-autoscaler/","title":"horizontal pod autoscaler","text":"<p>[!WARNING] This is a github note</p>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-hpa-horizontal-pod-autoscaler/#hpa-horizontal-pod-autoscaler","title":"hpa-horizontal-pod-autoscaler","text":"","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-hpa-horizontal-pod-autoscaler/#sample","title":"sample","text":"<ul> <li>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</li> </ul>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-hpa-horizontal-pod-autoscaler/#v1","title":"v1","text":"<ul> <li>https://user-images.githubusercontent.com/16036481/72983523-c4634f00-3de1-11ea-9a46-fa5229580d06.jpg</li> </ul>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-hpa-horizontal-pod-autoscaler/#v2","title":"v2","text":"<pre><code>k autoscale sts thanos-receive-cluster3-tmp -n thanos --cpu-percent=60   --min=1  --max=10 --dry-run=client -oyaml &gt; ~/environment/hpa.yaml\n</code></pre> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  creationTimestamp: null\n  name: thanos-receive-cluster3-tmp\nspec:\n  maxReplicas: 10\n  minReplicas: 1\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: StatefulSet\n    name: thanos-receive-cluster3-tmp\n  metrics:\n\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 70\n</code></pre>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-hpa-horizontal-pod-autoscaler/#resource","title":"resource","text":"<ul> <li>doc</li> <li>workshop</li> <li>sysctl</li> </ul>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-topology-spread-constraints/","title":"k8s-topology-spread-constraints","text":"","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-topology-spread-constraints/#link","title":"link","text":"<ul> <li>https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/</li> <li>https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/</li> </ul>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-topology-spread-constraints/#_1","title":"\u96c6\u7fa4\u7ea7\u522b\u7684\u9ed8\u8ba4\u7ea6\u675f","text":"","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-topology-spread-constraints/#_2","title":"\u5185\u7f6e\u9ed8\u8ba4\u7ea6\u675f","text":"<pre><code>defaultConstraints:\n\n  - maxSkew: 3\n    topologyKey: \"kubernetes.io/hostname\"\n    whenUnsatisfiable: ScheduleAnyway\n  - maxSkew: 5\n    topologyKey: \"topology.kubernetes.io/zone\"\n    whenUnsatisfiable: ScheduleAnyway\n</code></pre>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-topology-spread-constraints/#sample","title":"sample","text":"<ul> <li>6 node in 3 az</li> <li>1.25</li> </ul>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-topology-spread-constraints/#nginx-deployment","title":"nginx deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n  name: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      topologySpreadConstraints:\n\n      - maxSkew: 1\n        topologyKey: \"kubernetes.io/hostname\"\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            app: nginx\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\nstatus: {}\n</code></pre>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-topology-spread-constraints/#pod","title":"pod","text":"<pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: mypod\n  labels:\n    app: nginx\nspec:\n  topologySpreadConstraints:\n\n  - maxSkew: 1\n    # topologyKey: \"kubernetes.io/hostname\"\n    topologyKey: \"topology.kubernetes.io/zone\"\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        app: nginx\n  containers:\n  - name: pause\n    image: registry.k8s.io/pause:3.1\n</code></pre>","tags":["kubernetes"]},{"location":"EKS/kubernetes/k8s-topology-spread-constraints/#result","title":"result","text":"<ul> <li> <p>for zone </p> </li> <li> <p>for hostname check up topology in previous diagram</p> </li> </ul>","tags":["kubernetes"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/","title":"appmesh-workshop-eks","text":"<pre><code>title: This is a github note\n</code></pre>","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/#appmesh-workshop-eks","title":"appmesh-workshop-eks","text":"<p>https://www.eksworkshop.com/intermediate/330_app_mesh/deploy_dj_app/clone_repo/</p>","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/#install-appmesh-controller","title":"install appmesh-controller","text":"<ul> <li>doc</li> <li>workshop</li> </ul> <pre><code>CLUSTER_NAME=ekscluster4\nexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nexport AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\n\n# Create the namespace\nkubectl create ns appmesh-system\n\n# Install the App Mesh CRDs\nkubectl apply -k \"https://github.com/aws/eks-charts/stable/appmesh-controller/crds?ref=master\"\n\n# Create your OIDC identity provider for the cluster\neksctl utils associate-iam-oidc-provider \\\n--region=${AWS_DEFAULT_REGION} \\\n--cluster ${CLUSTER_NAME} --approve\n\n# Download the IAM policy for AWS App Mesh Kubernetes Controller\ncurl -o controller-iam-policy.json https://raw.githubusercontent.com/aws/aws-app-mesh-controller-for-k8s/master/config/iam/controller-iam-policy.json\n\n# Create an IAM policy called AWSAppMeshK8sControllerIAMPolicy\naws iam create-policy \\\n--policy-name AWSAppMeshK8sControllerIAMPolicy \\\n--policy-document file://controller-iam-policy.json\n\n# Create an IAM role for the appmesh-controller service account\neksctl create iamserviceaccount --cluster ${CLUSTER_NAME} \\\n--namespace appmesh-system \\\n--name appmesh-controller \\\n--attach-policy-arn arn:aws:iam::${AWS_ACCOUNT_ID}:policy/AWSAppMeshK8sControllerIAMPolicy,arn:aws:iam::aws:policy/AWSCloudMapFullAccess,arn:aws:iam::aws:policy/AWSAppMeshFullAccess  \\\n--override-existing-serviceaccounts --approve\n\n# install using helm\nhelm repo add eks https://aws.github.io/eks-charts\nhelm repo update\nhelm upgrade -i appmesh-controller eks/appmesh-controller \\\n    --namespace appmesh-system \\\n    --set region=${AWS_DEFAULT_REGION} \\\n    --set serviceAccount.create=false \\\n    --set serviceAccount.name=appmesh-controller\n\n#    --set tracing.enabled=true \\\n#    --set tracing.provider=x-ray\n\n# check version\nkubectl get deployment appmesh-controller -n appmesh-system -o json |jq -r \".spec.template.spec.containers[].image\" | cut -f2 -d ':'\n# check crds\nkubectl get crds | grep appmesh\n# check pods\nkubectl -n appmesh-system get all          \n</code></pre>","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/#ensure-node-role","title":"ensure node role","text":"<ul> <li>ensure node role has <code>AWSAppMeshEnvoyAccess</code> policy</li> </ul> <p>../../../CLI/linux/eksctl</p>","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/#flagger","title":"flagger","text":"<p>automated-canary-deployment-using-flagger</p>","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/#others","title":"others","text":"","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/#appmesh-on-eks","title":"appmesh on eks","text":"<p>link</p> <pre><code>git clone https://github.com/aws/aws-app-mesh-examples.git\ncd aws-app-mesh-examples/walkthroughs/eks/\n</code></pre>","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/#github-repo","title":"github repo","text":"<pre><code>git clone https://github.com/aws/aws-app-mesh-examples.git\n</code></pre>","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/#howto-k8s-http2","title":"howto-k8s-http2","text":"<p>link</p> <pre><code>CLUSTER_NAME=ekscluster1\n\nexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nexport AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\n\nexport VPC_ID=$(aws eks describe-cluster \\\n  --name ${CLUSTER_NAME} \\\n  --query \"cluster.resourcesVpcConfig.vpcId\" --output text )\n</code></pre> <pre><code>cd aws-app-mesh-examples/walkthroughs/howto-k8s-http2/\n./deploy\n</code></pre>","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/appmesh-workshop-eks/#refer","title":"refer","text":"<p>automated-canary-deployment-using-flagger</p>","tags":["aws/container/appmesh","aws/container/eks"]},{"location":"EKS/solutions/appmesh/automated-canary-deployment-using-flagger/","title":"automated-canary-deployment-using-flagger","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","aws/container/appmesh","flagger"]},{"location":"EKS/solutions/appmesh/automated-canary-deployment-using-flagger/#flagger-lab","title":"flagger lab","text":"","tags":["aws/container/eks","aws/container/appmesh","flagger"]},{"location":"EKS/solutions/appmesh/automated-canary-deployment-using-flagger/#install","title":"install","text":"<p>appmesh-workshop-eks</p>","tags":["aws/container/eks","aws/container/appmesh","flagger"]},{"location":"EKS/solutions/appmesh/automated-canary-deployment-using-flagger/#metrics-server-and-prometheus","title":"metrics-server and prometheus","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\nhelm upgrade -i appmesh-prometheus eks/appmesh-prometheus \\\n--namespace appmesh-system \\\n--set serviceAccount.create=false \\\n--set serviceAccount.name=appmesh-controller\n</code></pre>","tags":["aws/container/eks","aws/container/appmesh","flagger"]},{"location":"EKS/solutions/appmesh/automated-canary-deployment-using-flagger/#flagger","title":"flagger","text":"<pre><code>helm repo add flagger https://flagger.app\nhelm upgrade -i flagger flagger/flagger \\\n--namespace=appmesh-system \\\n--set meshProvider=appmesh:v1beta2 \\\n--set metricsServer=http://appmesh-prometheus:9090 \\\n--set serviceAccount.create=false \\\n--set serviceAccount.name=appmesh-controller\n\nkubectl get po -n appmesh-system | grep flagger\n</code></pre>","tags":["aws/container/eks","aws/container/appmesh","flagger"]},{"location":"EKS/solutions/appmesh/automated-canary-deployment-using-flagger/#automated-canary-deployments","title":"Automated Canary Deployments","text":"<p>Step 1. Create a mesh.</p> <ul> <li>must have the 2nd label </li> <li>need the 1st label for unique selection</li> </ul> <pre><code>set -u\nUNIQ_STR=$(date +%H%M)\nMESH_NAME=mesh${UNIQ_STR}\nenvsubst &gt;mesh.yaml &lt;&lt;-EOF\napiVersion: appmesh.k8s.aws/v1beta2\nkind: Mesh\nmetadata:\n  name: ${MESH_NAME}\nspec:\n  namespaceSelector:\n    matchLabels:\n      mash: ${MESH_NAME}\n      appmesh.k8s.aws/sidecarInjectorWebhook: enabled\nEOF\nkubectl apply -f mesh.yaml\n</code></pre> <p>Step 2.\u00a0Create a new namespace with App Mesh sidecar injection enabled. <pre><code>NS_NAME=namespace${UNIQ_STR}\n# NS_NAME=${MESH_NAME}\nenvsubst &gt;namespace.yaml &lt;&lt;-EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ${NS_NAME}\n  labels:\n    mash: ${MESH_NAME}\n    appmesh.k8s.aws/sidecarInjectorWebhook: enabled\nEOF\nkubectl apply -f namespace.yaml\n</code></pre></p> <p>Step 3.\u00a0Create a Kubernetes Deployment object.</p> <ul> <li>../../../CLI/linux/docker-cmd</li> </ul> <pre><code>IMAGE_URL=694242712155.dkr.ecr.us-east-2.amazonaws.com/sample/colorapp:v1\nenvsubst &gt;deployment.yaml &lt;&lt;-EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp\n  namespace: ${NS_NAME}\nspec:\n  minReadySeconds: 3\n  revisionHistoryLimit: 5\n  progressDeadlineSeconds: 60\n  strategy:\n    rollingUpdate:\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: webapp\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: \"true\"\n      labels:\n        app: webapp\n    spec:\n      containers:\n\n      - name: webapp\n        image: ${IMAGE_URL}\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          successThreshold: 3\n        ports:\n        - name: http\n          containerPort: 80\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\nEOF\nkubectl apply -f deployment.yaml\n</code></pre> <pre><code>kubectl get deploy -n ${NS_NAME}\n</code></pre> <p>Step 4.\u00a0Deploy a canary object. <pre><code>envsubst &gt;canary.yaml &lt;&lt;-EOF\napiVersion: flagger.app/v1beta1\nkind: Canary\nmetadata:\n  name: webapp\n  namespace: ${NS_NAME}\nspec:\n  # App Mesh API reference\n  provider: appmesh:v1beta2\n  # deployment reference\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: webapp\n  # the maximum time in seconds for the canary deployment\n  # to make progress before it is rollback (default 600s)\n  progressDeadlineSeconds: 60\n  service:\n    # container port\n    port: 80\n    targetPort: 80\n    # App Mesh ingress timeout (optional)\n    timeout: 15s\n    # App Mesh retry policy (optional)\n    retries:\n      attempts: 3\n      perTryTimeout: 5s\n      retryOn: \"gateway-error,client-error,stream-error\"\n    # App Mesh URI settings\n    match:\n\n      - uri:\n          prefix: /\n    rewrite:\n      uri: /\n  # define the canary analysis timing and KPIs\n  analysis:\n    # schedule interval (default 60s)\n    interval: 1m\n    # max number of failed metric checks before rollback\n    threshold: 5\n    # max traffic percentage routed to canary\n    # percentage (0-100)\n    maxWeight: 50\n    # canary increment step\n    # percentage (0-100)\n    stepWeight: 10\n    # App Mesh Prometheus checks\n    metrics:\n    - name: request-success-rate\n      # minimum req success rate (non 5xx responses)\n      # percentage (0-100)\n      thresholdRange:\n        min: 99\n      interval: 1m\n    - name: request-duration\n      # maximum req duration P99\n      # milliseconds\n      thresholdRange:\n        max: 500\n      interval: 30s\nEOF\nkubectl apply -f canary.yaml\n</code></pre></p> <pre><code>kubectl get canary -n ${NS_NAME} -w\n</code></pre> <p>Step 5.\u00a0To expose the webapp application outside the mesh, create an App Mesh gateway. <pre><code>helm upgrade -i appmesh-gateway-${UNIQ_STR} eks/appmesh-gateway \\\n--namespace ${NS_NAME}\n</code></pre></p> <p>Step 6.\u00a0Create a gateway route that points to the webapp virtual service. <pre><code>envsubst &gt;gatewayroute.yaml &lt;&lt;-EOF\napiVersion: appmesh.k8s.aws/v1beta2\nkind: GatewayRoute\nmetadata:\n  name: webapp\n  namespace: ${NS_NAME}\nspec:\n  httpRoute:\n    match:\n      prefix: \"/\"\n    action:\n      target:\n        virtualService:\n          virtualServiceRef:\n            name: webapp\nEOF\nkubectl apply -f gatewayroute.yaml\n</code></pre></p> <p>get URL <pre><code>URL=\"http://$(kubectl -n ${NS_NAME} get svc/appmesh-gateway-${UNIQ_STR} -ojson | jq -r \".status.loadBalancer.ingress[].hostname\")\"\necho ${URL}\n</code></pre></p>","tags":["aws/container/eks","aws/container/appmesh","flagger"]},{"location":"EKS/solutions/appmesh/automated-canary-deployment-using-flagger/#automated-canary-promotion","title":"Automated Canary Promotion","text":"<pre><code>IMAGE_URL_V2=${IMAGE_URL%:*}:v2\necho ${IMAGE_URL_V2}\n\nkubectl -n ${NS_NAME} set image deployment/webapp webapp=${IMAGE_URL_V2}\nkubectl get canaries -A -w\n\nkubectl describe canary webapp -n ${NS_NAME}\n</code></pre>","tags":["aws/container/eks","aws/container/appmesh","flagger"]},{"location":"EKS/solutions/appmesh/automated-canary-deployment-using-flagger/#refer","title":"refer","text":"<ul> <li>https://docs.flagger.app/</li> <li>https://aws.amazon.com/cn/blogs/containers/progressive-delivery-using-aws-app-mesh-and-flagger/</li> </ul>","tags":["aws/container/eks","aws/container/appmesh","flagger"]},{"location":"EKS/solutions/gitops/argocd-lab/","title":"argocd","text":"<p>[!WARNING] This is a github note</p>","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#argocd-lab","title":"argocd-lab","text":"","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#install","title":"install","text":"<ul> <li>https://github.com/argoproj/argo-cd</li> </ul> <pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.4.8/manifests/install.yaml\n\n# install argocd binary\nsudo curl --silent --location -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/v2.4.8/argocd-linux-amd64\nsudo chmod +x /usr/local/bin/argocd\n\n# connect to argo server\nkubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\nexport ARGOCD_SERVER=`kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'`\necho $ARGOCD_SERVER\nexport ARGO_PWD=`kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d`\nargocd login $ARGOCD_SERVER --username admin --password $ARGO_PWD --insecure\n</code></pre>","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#add-helm","title":"add helm","text":"<pre><code>argocd repo add https://aws.github.io/eks-charts --type helm --name aws-eks-charts --project default\n</code></pre>","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#install-aws-lb-controller","title":"install aws-lb-controller","text":"","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#with-argocd-ui","title":"with argocd UI","text":"<pre><code>cluster_name=ekscluster1\nexport AWS_DEFAULT_REGION=us-east-2\n\ncurl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json\npolicy_name=AWSLoadBalancerControllerIAMPolicy-$RANDOM\npolicy_arn=$(aws iam create-policy \\\n  --policy-name ${policy_name}  \\\n  --policy-document file://iam_policy.json \\\n  --query 'Policy.Arn' \\\n  --output text)\n\neksctl create iamserviceaccount \\\n  --cluster=${cluster_name} \\\n  --namespace=kube-system \\\n  --name=aws-load-balancer-controller \\\n  --role-name=${policy_name} \\\n  --attach-policy-arn=${policy_arn} \\\n  --override-existing-serviceaccounts \\\n  --approve\n</code></pre> <p>\u6700\u9ad8\u53ef\u9009\u7248\u672c chart version 1.4.3 </p>","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#with-argocd-cli","title":"with argocd cli","text":"<pre><code>cat &gt;aws-lb-controller.yaml &lt;&lt;-EOF\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: aws-lb-controller\nspec:\n  destination:\n    name: ''\n    namespace: kube-system\n    server: 'https://kubernetes.default.svc'\n  source:\n    path: ''\n    repoURL: 'https://aws.github.io/eks-charts'\n    targetRevision: 1.4.3\n    chart: aws-load-balancer-controller\n    helm:\n      parameters:\n\n        - name: clusterName\n          value: ekscluster1\n        - name: serviceAccount.create\n          value: 'false'\n        - name: serviceAccount.name\n          value: aws-load-balancer-controller\n  project: default\n  syncPolicy:\n    automated:\n      prune: false\n      selfHeal: false\nEOF\n\nkubectl apply -f aws-lb-controller.yaml -n argocd\n</code></pre>","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#install-echoserver-with-argocd","title":"install echoserver with argocd","text":"<pre><code># argocd app create echoserver --repo https://github.com/kubernetes-sigs/aws-load-balancer-controller.git --path docs/examples/echoservice --dest-server https://kubernetes.default.svc --dest-namespace echoserver\n\nargocd app create echoserver --repo https://github.com/panlm/aws-eks-example.git --path echoserver --dest-server https://kubernetes.default.svc --dest-namespace echoserver\nargocd app sync apps\n</code></pre> <p>clone to your github </p> <ul> <li>modify ingress file to update hostname</li> <li>modify deployment file to increase replicas</li> </ul>","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#install-2048-game","title":"install 2048 game","text":"<pre><code>argocd app create game2048 --repo https://github.com/panlm/aws-eks-example.git --path 2048 --dest-server https://kubernetes.default.svc --dest-namespace game2048\nargocd app sync apps\n</code></pre>","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#install-prometheus-with-customized-values","title":"install prometheus with customized values","text":"<ul> <li> <p>argocd-sample-prometheus-operator</p> </li> <li> <p>need customized values put on github or define in argocd Application yaml</p> </li> </ul>","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/argocd-lab/#refer","title":"refer","text":"<ul> <li>argocd-cmd</li> </ul>","tags":["gitops/argo"]},{"location":"EKS/solutions/gitops/flux-lab/","title":"flux","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/gitops/flux-lab/#flux-lab","title":"flux-lab","text":"","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/gitops/flux-lab/#bootstrap-v2","title":"bootstrap v2","text":"<pre><code>export GITHUB_TOKEN=ghp_xxxxxx\nflux check --pre\nflux bootstrap github \\\n    --owner=panlm \\\n    --repository=aws-eks-config \\\n    --branch=main \\\n    --personal \\\n    --path=clusters/ekscluster2\n</code></pre>","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/gitops/flux-lab/#another-sample","title":"another sample","text":"<pre><code>flux bootstrap github \\\n    --owner=panlm \\\n    --repository=eks-cluster-upgrades-workshop \\\n    --branch=main \\\n    --personal \\\n    --path=gitops/clusters/cluster-demo\n</code></pre> <pre><code>      owner: \"panlm\"\n      repository: \"eks-cluster-upgrades-workshop\"\n      private: \"true\"\n      branch: \"main\"\n      namespace: \"flux-system\"\n      path: \"gitops/clusters/cluster-demo\"\n</code></pre>","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/gitops/flux-lab/#lab","title":"lab","text":"<pre><code>flux get all\n\n# before delete ns\nflux uninstall --dry-run -n flux-system\n</code></pre>","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/gitops/flux-lab/#generate-yaml","title":"generate yaml","text":"<p><code>cat value.yaml</code> <pre><code>clusterName: ekscluster3\nserviceAccount:\n  create: false\n  name: aws-load-balancer-controller\n</code></pre></p> <pre><code>flux create source helm ww-gitops \\\n  --url=https://aws.github.io/eks-charts \\\n  --export &gt; a.yaml\n\nflux create helmrelease aws-load-balancer-controller \\\n  --source=HelmRepository/ww-gitops \\\n  --chart=aws-load-balancer-controller \\\n  --chart-version 1.4.4 --values value.yaml --export &gt; c.yaml\n</code></pre>","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/gitops/flux-lab/#helm-v1-alternative","title":"helm v1 (alternative)","text":"<pre><code>CLUSTER_NAME=ekscluster1\nkubectl create ns flux\nhelm repo add fluxcd https://charts.fluxcd.io \n\nhelm upgrade -i flux fluxcd/flux \\\n--set git.url=git@github.com:panlm/aws-eks-config \\\n--set git.branch=main \\\n--set git.path=clusters/${CLUSTER_NAME} \\\n--namespace flux\n\nhelm upgrade -i helm-operator fluxcd/helm-operator \\\n--set helm.versions=v3 \\\n--set git.ssh.secretName=flux-git-deploy \\\n--set git.branch=main \\\n--namespace flux\n\nkubectl -n flux logs deployment/flux | grep identity.pub | cut -d '\"' -f2\n# put public key to repo's `Deploy keys`\n</code></pre>","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/gitops/flux-lab/#dependencies","title":"dependencies","text":"","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/gitops/flux-lab/#others","title":"others","text":"<p>https://www.eksworkshop.com/intermediate/260_weave_flux/</p> <p>got error: <pre><code>[Container] 2022/04/06 07:19:56 Running command curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -  \n\n/codebuild/output/tmp/script.sh: 4: /codebuild/output/tmp/script.sh: sudo: not found\n</code></pre></p> <p>try to find <code>build details</code> in <code>image-codepipeline</code> <code>build project</code> modify <code>buildspec</code> remove <code>sudo</code> before <code>apt-key add</code></p>","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/gitops/flux-lab/#workshop","title":"workshop","text":"<ul> <li>Accelerate software development lifecycles with GitOps</li> </ul>","tags":["aws/container/eks","gitops/weaveworks/flux"]},{"location":"EKS/solutions/logging/POC-loki-for-logging/","title":"Using Loki for Logging","text":"<p>[!WARNING] This is a github note</p>","tags":["grafana/loki"]},{"location":"EKS/solutions/logging/POC-loki-for-logging/#loki","title":"loki","text":"","tags":["grafana/loki"]},{"location":"EKS/solutions/logging/POC-loki-for-logging/#diagram","title":"diagram","text":"","tags":["grafana/loki"]},{"location":"EKS/solutions/logging/POC-loki-for-logging/#walkthrough","title":"walkthrough","text":"<ul> <li> <p>install with helm <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm upgrade -i -f minio.yaml loki grafana/loki -n loki\nhelm upgrade -i promtail grafana/promtail -n ${NAMESPACE}\n\n# list all repo\nhelm search repo grafana\n</code></pre></p> </li> <li> <p>create sa for loki git/git-mkdocs/CLI/linux/eksctl <pre><code>CLUSTER_NAME_1=ekscluster1\nNAMESPACE=loki-stack\nLOKI_SA_NAME=loki-sa\nAWS_DEFAULT_REGION=us-east-2\nLOKI_BUCKET_NAME=store-loki-eks1217\ncreate-iamserviceaccount -s ${LOKI_SA_NAME} -c ${CLUSTER_NAME_1} -n ${NAMESPACE} -r 0\n</code></pre></p> </li> <li>values yaml file for install <pre><code>echo ${AWS_DEFAULT_REGION}\nenvsubst &gt; loki-1.yaml &lt;&lt;-EOF\nloki:\n  auth_enabled: false\n  storage:\n    bucketNames:\n      chunks: ${LOKI_BUCKET_NAME}\n      ruler: ${LOKI_BUCKET_NAME}\n      admin: ${LOKI_BUCKET_NAME}\n    type: s3\n    s3:\n      region: ${AWS_DEFAULT_REGION}\n      insecure: false\n      s3ForcePathStyle: false\nserviceAccount:\n  create: false\n  name: ${LOKI_SA_NAME}\nwrite:\n  replicas: 2\nread:\n  replicas: 1\ngateway: \n  enabled: true\n  replicas: 2\nEOF\n\nhelm upgrade -i -f loki-1.yaml loki grafana/loki -n ${NAMESPACE}\nhelm upgrade -i promtail grafana/promtail -n ${NAMESPACE}\n</code></pre></li> </ul>","tags":["grafana/loki"]},{"location":"EKS/solutions/logging/POC-loki-for-logging/#dashboard-","title":"dashboard-","text":"<ul> <li>12019 - quick search - good for try<ul> <li>change to <code>{namespace=\"$namespace\", pod=~\"$pod\"} |~ \"$search\"</code> in Logs Panel definition</li> </ul> </li> <li>loki-stack will have some predefined dashboard - https://artifacthub.io/packages/helm/grafana/loki-stack </li> </ul>","tags":["grafana/loki"]},{"location":"EKS/solutions/logging/POC-loki-for-logging/#sample","title":"sample","text":"<pre><code>loki:\n  auth_enabled: false\nminio:\n  enabled: true\n</code></pre>","tags":["grafana/loki"]},{"location":"EKS/solutions/logging/cloudwatch-to-firehose-python/","title":"cloudwatch-to-firehose-python","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/serverless/lambda","aws/mgmt/cloudwatch","aws/analytics/kinesis/firehose","python"]},{"location":"EKS/solutions/logging/cloudwatch-to-firehose-python/#cloudwatch-to-firehose-python","title":"cloudwatch-to-firehose-python","text":"","tags":["aws/serverless/lambda","aws/mgmt/cloudwatch","aws/analytics/kinesis/firehose","python"]},{"location":"EKS/solutions/logging/cloudwatch-to-firehose-python/#create","title":"create","text":"<p>source from here</p> <ol> <li>create from blueprint <code>Process CloudWatch logs sent to Kinesis Firehose</code> <pre><code>Process CloudWatch logs sent to Kinesis Firehose\n\nAn Amazon Kinesis Firehose stream processor that extracts individual log events from records sent by Cloudwatch Logs subscription filters.\n\npython3.8\u00a0\u00b7\u00a0kinesis-firehose\u00a0\u00b7\u00a0cloudwatch-logs\u00a0\u00b7\u00a0splunk\n</code></pre></li> </ol>","tags":["aws/serverless/lambda","aws/mgmt/cloudwatch","aws/analytics/kinesis/firehose","python"]},{"location":"EKS/solutions/logging/cloudwatch-to-firehose-python/#revision","title":"revision","text":"<p>download lambda_function.py</p>","tags":["aws/serverless/lambda","aws/mgmt/cloudwatch","aws/analytics/kinesis/firehose","python"]},{"location":"EKS/solutions/logging/cloudwatch-to-firehose-python/#3-party-python-lib","title":"3-party python lib","text":"<p>download from package.zip</p>","tags":["aws/serverless/lambda","aws/mgmt/cloudwatch","aws/analytics/kinesis/firehose","python"]},{"location":"EKS/solutions/logging/cloudwatch-to-firehose-python/#layer-version","title":"layer version","text":"<ul> <li>../../../CLI/awscli/lambda-cmd</li> </ul>","tags":["aws/serverless/lambda","aws/mgmt/cloudwatch","aws/analytics/kinesis/firehose","python"]},{"location":"EKS/solutions/logging/export-cloudwatch-log-group-to-s3/","title":"Export Cloudwatch Log Group to S3","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/mgmt/cloudwatch","aws/storage/s3"]},{"location":"EKS/solutions/logging/export-cloudwatch-log-group-to-s3/#export-cloudwatch-log-group-to-s3","title":"Export Cloudwatch Log Group to S3","text":"<ul> <li> <p>https://docs.aws.amazon.com/zh_cn/AmazonCloudWatch/latest/logs/S3ExportTasks.html#S3Permissions</p> </li> <li> <p>\u5bfc\u51fa\u65e5\u5fd7\u683c\u5f0f\u6587\u4ef6\u7c7b\u4f3c\uff1a </p> </li> </ul>","tags":["aws/mgmt/cloudwatch","aws/storage/s3"]},{"location":"EKS/solutions/logging/export-cloudwatch-log-group-to-s3/#create-bucket-and-policy","title":"create bucket and policy","text":"<pre><code>bucket_name=my-exported-logs-2358-$RANDOM\naws_region=us-east-2\nenvsubst &gt;policy.json &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": \"s3:GetBucketAcl\",\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::${bucket_name}\",\n            \"Principal\": { \"Service\": \"logs.${aws_region}.amazonaws.com\" }\n        },\n        {\n            \"Action\": \"s3:PutObject\" ,\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::${bucket_name}/*\",\n            \"Condition\": { \"StringEquals\": { \"s3:x-amz-acl\": \"bucket-owner-full-control\" } },\n            \"Principal\": { \"Service\": \"logs.${aws_region}.amazonaws.com\" }\n        }\n    ]\n}\nEOF\n\naws s3api create-bucket --bucket ${bucket_name} \\\n--create-bucket-configuration LocationConstraint=${aws_region} \\\n--region ${aws_region}\n\n# will overwrite existed policy attached on s3 bucket !\naws s3api put-bucket-policy --bucket ${bucket_name} --policy file://policy.json\n</code></pre>","tags":["aws/mgmt/cloudwatch","aws/storage/s3"]},{"location":"EKS/solutions/logging/export-cloudwatch-log-group-to-s3/#export-task","title":"export task","text":"<pre><code>loggroup_name=\"/aws/eks/ekscluster1/cluster\"\nbegin_time=$(date --date \"2 days ago\" +%s)\nend_time=$(date --date \"1 days ago\" +%s)\n\nsuffix=$(date +%Y%m%d-%H%M%S)\n#for i in kube-apiserver-audit kube-apiserver kube-scheduler authenticator kube-controller-manager cloud-controller-manager ; do\nfor i in kube-apiserver-audit ; do\n  task_name=${i}-${suffix}\n  # create export task\n  aws logs create-export-task --task-name ${task_name} \\\n    --log-group-name ${loggroup_name} \\\n    --log-stream-name-prefix ${i} \\\n    --from ${begin_time}000 --to ${end_time}000 \\\n    --destination ${bucket_name} \\\n    --destination-prefix ${i} |tee /tmp/$$\n  task_id=$(cat /tmp/$$ |jq -r '.taskId')\n  while true; do\n    # describe task\n    aws logs describe-export-tasks \\\n      --task-id ${task_id} |tee /tmp/$$.task\n    task_status=$(cat /tmp/$$.task |jq -r '.exportTasks[0].status.code')\n    if [[ ${task_status} == \"COMPLETED\" ]]; then\n      break\n    fi\n    sleep 10\n  done\ndone\n</code></pre>","tags":["aws/mgmt/cloudwatch","aws/storage/s3"]},{"location":"EKS/solutions/logging/export-cloudwatch-log-group-to-s3/#remaining-issues","title":"remaining issues","text":"<ul> <li>\u9700\u8981\u4fdd\u5b58\u5bfc\u51fa\u7684\u65f6\u95f4\u70b9\uff0c\u4ee5\u4fbf\u4e8e\u4e0b\u6b21\u4ece\u6539\u65f6\u95f4\u70b9\u7ee7\u7eed</li> <li>\u5bfc\u51fa\u540e\u5c06\u5728\u6307\u5b9a\u7684 prefix \u4e2d\uff0c\u521b\u5efa task id \u4e3a folder \uff0c\u6309\u7167\u6240\u6709 log stream name \u4f5c\u4e3a\u4e0b\u4e00\u5c42\u7684 folder \uff0c\u65e5\u5fd7\u6587\u4ef6\u4e3a gz \u538b\u7f29\u683c\u5f0f</li> <li>\u53ef\u4ee5\u7701\u7565 <code>log-stream-name-prefix</code> \u53c2\u6570\uff0c\u5bfc\u51fa\u6240\u6709\u65e5\u5fd7\uff0c\u8003\u8651\u65e5\u5fd7\u5982\u4f55\u8fdb\u884c\u4e8c\u6b21\u5904\u7406</li> <li>k8s\u63a7\u5236\u65e5\u5fd7\u5305\u542b\u7c7b\u4f3c\u524d\u7f00\uff0c\u6bd4\u5982 <code>kube-apiserver-audit-xxx</code> \u548c <code>kube-apiserver-xxxx</code> \uff0c\u5e76\u4e14\u524d\u8005\u662fjson\u683c\u5f0f\u65e5\u5fd7\uff0c\u540e\u8005\u662f\u884c\u65e5\u5fd7\uff0c\u5982\u4f55\u8fdb\u884c\u533a\u5206\u5bfc\u51fa\u5230\u4e0d\u540c prefix \u8def\u5f84\uff0c\u6216\u8005\u5bfc\u51fa\u540e\u5982\u4f55\u8fdb\u884c\u4e8c\u6b21\u5904\u7406\uff0c\u53c2\u8003\uff1a stream-k8s-control-panel-logs-to-s3 (link or hugo)</li> <li>\u5bfc\u51fa\u6d88\u606f\u683c\u5f0f\u95ee\u9898\u5f85\u89e3\u51b3 athena-sample-query </li> </ul>","tags":["aws/mgmt/cloudwatch","aws/storage/s3"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/","title":"Stream EKS Control Panel Logs to S3","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#stream-eks-control-panel-logs-to-s3","title":"Stream EKS Control Panel Logs to S3","text":"","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#background","title":"background","text":"<p>\u76ee\u524d eks \u63a7\u5236\u5e73\u9762\u65e5\u5fd7\u53ea\u652f\u6301\u53d1\u9001\u5230 cloudwatch\uff0c\u4e14\u5728\u540c\u4e00\u4e2a log group \u4e2d\u6709 5 \u79cd\u7c7b\u578b 6\u79cd\u524d\u7f00\u7684 log stream \u7684\u65e5\u5fd7\uff0c\u4e0d\u5229\u4e8e\u7edf\u4e00\u67e5\u8be2\u3002\u4e14\u53ea\u6709 audit \u65e5\u5fd7\u662f json \u683c\u5f0f\u5176\u4ed6\u5747\u662f\u5355\u884c\u65e5\u5fd7\uff0c\u4e14\u5b57\u6bb5\u5404\u4e0d\u76f8\u540c\u3002</p> <ul> <li>kube-apiserver-audit</li> <li>kube-apiserver</li> <li>kube-scheduler</li> <li>authenticator</li> <li>kube-controller-manager</li> <li>cloud-controller-manager</li> </ul>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#requirement","title":"requirement","text":"<p>\u5ba2\u6237\u9700\u6c42\uff1a</p> <ol> <li>\u7b80\u5355 - \u5df2\u6709 splunk \u65e5\u5fd7\u5e73\u53f0\uff0c\u4e0d\u5e0c\u671b\u4f7f\u7528 opensearch \u7b49\u5176\u4ed6\u65e5\u5fd7\u5e73\u53f0\uff0c\u4fdd\u8bc1\u8fd0\u7ef4\u7b80\u5316</li> <li>\u5b9e\u65f6 - \u9700\u8981\u6709\u65b9\u6cd5\u5c06\u65e5\u5fd7\u8fd1\u5b9e\u65f6\u5730\u53d1\u9001\u5230S3\uff0c\u53ef\u4ee5\u901a\u8fc7 splunk \u8fdb\u884c\u67e5\u8be2\u548c\u5b9e\u65f6\u544a\u8b66\u3002export cloudwatch \u65e5\u5fd7\u7684\u65b9\u5f0f\uff0c\u5b9e\u65f6\u6027\u65e0\u6cd5\u6ee1\u8db3\uff0c\u4e14\u540c\u6837\u9700\u8981\u989d\u5916\u5b9e\u73b0 export \u7aef\u70b9\u7eed\u5bfc\u7684\u95ee\u9898</li> <li>\u4e8c\u6b21\u5904\u7406 - \u672a\u6765\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u65e5\u5fd7\u8fdb\u884c\u67e5\u8be2\u53ca\u5173\u952e\u5b57\u6bb5\u63d0\u53d6\u65b9\u4fbf\u8fdb\u884c\u5206\u6790\u548c\u544a\u8b66</li> <li>\u6210\u672c\u548c\u5b89\u5168 - \u6210\u672c\u63a7\u5236\uff0c\u9ad8\u5b89\u5168\u6027\uff0c\u652f\u6301\u591a\u8d26\u53f7</li> </ol>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#architecture","title":"architecture","text":"","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#walkthrough","title":"walkthrough","text":"","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#eks-cluster","title":"eks cluster","text":"<ul> <li>need an EKS cluster and enable log to cloudwatch</li> </ul>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#s3","title":"s3","text":"<ul> <li>\u521b\u5efas3\u6876</li> </ul> <pre><code>export AWS_PAGER=\"\"\nexport AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\nexport AWS_DEFAULT_REGION=${AWS_REGION}\nexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity \\\n--query \"Account\" --output text)\n\nUNIQ=$(date +%Y%m%d%H%M)\nBUCKET_NAME=centrallog-$UNIQ\nS3_PREFIX=\"CentralizedAccountLogs\"\naws s3 mb s3://${BUCKET_NAME}\n\nATHENA_BUCKET_NAME=athena-$UNIQ\naws s3 mb s3://${ATHENA_BUCKET_NAME}\n</code></pre>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#lambda","title":"lambda","text":"<ul> <li>\u521b\u5efa\u51fd\u6570\u6240\u9700\u89d2\u8272</li> <li>\u4e0b\u8f7d\u5b9a\u5236 lambda \u4ee3\u7801 \u548c layer \uff08\u53c2\u89c1cloudwatch-to-firehose-python\uff09<ul> <li>lambda_function.py </li> <li>package.zip </li> </ul> </li> <li>\u521b\u5efa\u51fd\u6570\u5e76\u83b7\u53d6arn</li> </ul> <pre><code>echo ${AWS_REGION} ${UNIQ}\nLAMBDA_NAME=firehose-lambda-${UNIQ}\n\nif [[ ${AWS_REGION%%-*} == \"cn\" ]]; then\n  ARN_PREFIX=\"aws-cn\"\nelse\n  ARN_PREFIX=\"aws\"\nfi\n\n# create lambda role\nLAMBDA_ROLE_NAME=${LAMBDA_NAME}-role-${UNIQ}\necho '{\"Version\": \"2012-10-17\",\"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]}' |tee lambda-role-trust-policy.json\naws iam create-role --role-name ${LAMBDA_ROLE_NAME} --assume-role-policy-document file://lambda-role-trust-policy.json\n\nenvsubst &gt;lambda-role-policy.json &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"firehose:PutRecord\",\n                \"firehose:PutRecordBatch\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:${ARN_PREFIX}:firehose:${AWS_REGION}:${AWS_ACCOUNT_ID}:deliverystream/*\"\n        }\n    ]\n}\nEOF\n\nLAMBDA_POLICY_ARN=$(aws iam create-policy \\\n--policy-name ${LAMBDA_ROLE_NAME} \\\n--policy-document file://lambda-role-policy.json |jq -r '.Policy.Arn')\n\naws iam attach-role-policy --role-name ${LAMBDA_ROLE_NAME} --policy-arn ${LAMBDA_POLICY_ARN}\naws iam attach-role-policy --role-name ${LAMBDA_ROLE_NAME} --policy-arn arn:${ARN_PREFIX}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\naws iam attach-role-policy --role-name ${LAMBDA_ROLE_NAME} --policy-arn arn:${ARN_PREFIX}:iam::aws:policy/service-role/AWSLambdaKinesisExecutionRole\nLAMBDA_ROLE_ARN=$(aws iam get-role --role-name ${LAMBDA_ROLE_NAME} |jq -r '.Role.Arn')\n\n# download code and create lambda\n# wget -O lambda_function.py https://github.com/panlm/aws-labs/raw/main/eks-cloudwatch-log-firehose-s3/lambda_function.py\nzip lambda_function.zip ./lambda_function.py\n\nsleep 10\naws lambda create-function \\\n    --function-name ${LAMBDA_NAME} \\\n    --runtime python3.8 \\\n    --timeout 60 \\\n    --zip-file fileb://lambda_function.zip \\\n    --handler lambda_function.lambda_handler \\\n    --role ${LAMBDA_ROLE_ARN}\n\nLAMBDA_ARN=$(aws lambda get-function \\\n    --function-name ${LAMBDA_NAME} \\\n    --query 'Configuration.FunctionArn' --output text)\n\n# download package and create lambda layer\n# wget -O package.zip https://github.com/panlm/aws-labs/raw/main/eks-cloudwatch-log-firehose-s3/package.zip\n\naws lambda publish-layer-version \\\n    --layer-name layer_flatten_json \\\n    --description \"flatten_json\" \\\n    --zip-file fileb://package.zip \\\n    --compatible-runtimes python3.8\nLAYER_ARN=$(aws lambda list-layer-versions \\\n    --layer-name layer_flatten_json \\\n    --query 'LayerVersions[0].LayerVersionArn' --output text)\n\n# add layer to lambda\naws lambda update-function-configuration --function-name ${LAMBDA_NAME} \\\n--layers ${LAYER_ARN}\n</code></pre>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#firehose","title":"firehose","text":"<ul> <li>\u521b\u5efafirehose\u6240\u9700\u89d2\u8272</li> <li>\u521b\u5efafirehose\u5e76\u4e14\u5173\u8054 transform \u51fd\u6570</li> <li>\u83b7\u53d6arn</li> </ul> <pre><code>FIREHOSE_NAME=firehose-${UNIQ}\nFIREHOSE_ROLE_NAME=${FIREHOSE_NAME}-role-${UNIQ}\necho \"LAMBDA_NAME:\"${LAMBDA_NAME} \"LAMBDA_ARN:\"${LAMBDA_ARN} \"AWS_REGION:\"${AWS_REGION} \"ARN_PREFIX:\"${ARN_PREFIX} \"AWS_ACCOUNT_ID:\"${AWS_ACCOUNT_ID} \"BUCKET_NAME:\"${BUCKET_NAME} \"S3_PREFIX:\"${S3_PREFIX}\n\necho '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"firehose.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}' |tee firehose-role-trust-policy.json\naws iam create-role --role-name ${FIREHOSE_ROLE_NAME} \\\n  --assume-role-policy-document file://firehose-role-trust-policy.json\n\nenvsubst &gt;firehose-role-policy.json &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:AbortMultipartUpload\",\n                \"s3:GetBucketLocation\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"arn:${ARN_PREFIX}:s3:::${BUCKET_NAME}\",\n                \"arn:${ARN_PREFIX}:s3:::${BUCKET_NAME}/*\"\n            ]\n        },\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\",\n                \"lambda:GetFunctionConfiguration\"\n            ],\n            \"Resource\": \"arn:${ARN_PREFIX}:lambda:${AWS_REGION}:${AWS_ACCOUNT_ID}:function:%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%\"\n        },\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:${ARN_PREFIX}:logs:${AWS_REGION}:${AWS_ACCOUNT_ID}:log-group:/aws/kinesisfirehose/${FIREHOSE_NAME}:log-stream:*\",\n                \"arn:${ARN_PREFIX}:logs:${AWS_REGION}:${AWS_ACCOUNT_ID}:log-group:%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%:log-stream:*\"\n            ]\n        }\n    ]\n}\nEOF\n\nsed -i '/:function:/s/%FIREHOSE_POLICY_TEMPLATE_PLACEHOLDER%/'\"${LAMBDA_NAME}\"':$LATEST/' firehose-role-policy.json\n\nFIREHOSE_POLICY_ARN=$(aws iam create-policy \\\n--policy-name ${FIREHOSE_ROLE_NAME} \\\n--policy-document file://firehose-role-policy.json |jq -r '.Policy.Arn')\n\naws iam attach-role-policy \\\n--role-name ${FIREHOSE_ROLE_NAME} \\\n--policy-arn ${FIREHOSE_POLICY_ARN}\naws iam list-attached-role-policies --role-name ${FIREHOSE_ROLE_NAME}\n\nFIREHOSE_ROLE_ARN=$(aws iam get-role --role-name ${FIREHOSE_ROLE_NAME} |jq -r '.Role.Arn')\n\nsleep 10\naws firehose create-delivery-stream \\\n--delivery-stream-name ${FIREHOSE_NAME} \\\n--delivery-stream-type \"DirectPut\" \\\n--extended-s3-destination-configuration \"RoleARN=${FIREHOSE_ROLE_ARN},BucketARN=arn:aws:s3:::${BUCKET_NAME},Prefix=${S3_PREFIX}/,ErrorOutputPrefix=${S3_PREFIX}_failed/,BufferingHints={SizeInMBs=2,IntervalInSeconds=120},CompressionFormat=GZIP,EncryptionConfiguration={NoEncryptionConfig=NoEncryption},CloudWatchLoggingOptions={Enabled=true,LogGroupName=${FIREHOSE_ROLE_NAME},LogStreamName=${FIREHOSE_ROLE_NAME}},ProcessingConfiguration={Enabled=true,Processors=[{Type=Lambda,Parameters=[{ParameterName=LambdaArn,ParameterValue=${LAMBDA_ARN}:\\$LATEST},{ParameterName=BufferSizeInMBs,ParameterValue=1},{ParameterName=BufferIntervalInSeconds,ParameterValue=60}]}]}\"\n\n# no lambda for data transform\n# --s3-destination-configuration \"RoleARN=${role_arn},BucketARN=arn:aws:s3:::${bucket_name},Prefix=CentralizedAccountLogs/,ErrorOutputPrefix=CentralizedAccountLogs_failed/,BufferingHints={SizeInMBs=1,IntervalInSeconds=60},CompressionFormat=UNCOMPRESSED,EncryptionConfiguration={NoEncryptionConfig=NoEncryption},CloudWatchLoggingOptions={Enabled=true,LogGroupName=${role_name},LogStreamName=${role_name}}\"\n\nFIREHOSE_ARN=$(aws firehose describe-delivery-stream \\\n--delivery-stream-name ${FIREHOSE_NAME} \\\n--query \"DeliveryStreamDescription.DeliveryStreamARN\" --output text)\n\nwhile true ; do\n  FIREHOSE_STATUS=$(aws firehose describe-delivery-stream --delivery-stream-name ${FIREHOSE_NAME} --query \"DeliveryStreamDescription.DeliveryStreamStatus\" --output text)\n  echo ${FIREHOSE_STATUS}\n  if [[ ${FIREHOSE_STATUS} == \"ACTIVE\" ]]; then\n    break\n  fi\n  sleep 10\ndone\n</code></pre>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#cloudwatch","title":"cloudwatch","text":"<ul> <li>\u521b\u5efacwl\u6240\u9700\u89d2\u8272\u6765\u8bbf\u95eefirehose</li> </ul> <pre><code>CWL_ROLE_NAME=cwl-firehose-role-$UNIQ\necho \"AWS_REGION:\"${AWS_REGION} \"AWS_ACCOUNT_ID:\"${AWS_ACCOUNT_ID} \"ARN_PREFIX:\"${ARN_PREFIX}\n\nif [[ ${AWS_REGION%%-*} == \"cn\" ]]; then\n  DN_SUFFIX=\"com.cn\"\nelse\n  DN_SUFFIX=\"com\"\nfi\n\nenvsubst &gt; cwl-role-trust-policy.json &lt;&lt;-EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"logs.${AWS_REGION}.amazonaws.${DN_SUFFIX}\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nEOF\naws iam create-role --role-name ${CWL_ROLE_NAME} \\\n  --assume-role-policy-document file://cwl-role-trust-policy.json\n\nenvsubst &gt; cwl-role-policy.json &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"firehose:PutRecord\",\n                \"firehose:PutRecordBatch\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:${ARN_PREFIX}:firehose:${AWS_REGION}:${AWS_ACCOUNT_ID}:deliverystream/*\"\n        }\n    ]\n}\nEOF\nCWL_POLICY_ARN=$(aws iam create-policy \\\n--policy-name ${CWL_ROLE_NAME} \\\n--policy-document file://cwl-role-policy.json |jq -r '.Policy.Arn')\naws iam attach-role-policy --role-name ${CWL_ROLE_NAME} \\\n  --policy-arn ${CWL_POLICY_ARN}\naws iam list-attached-role-policies --role-name ${CWL_ROLE_NAME}\n\nCWL_ROLE_ARN=$(aws iam get-role --role-name ${CWL_ROLE_NAME} |jq -r '.Role.Arn')\n</code></pre> <ul> <li>\u6ce8\u518c firehose \u5230 eks \u96c6\u7fa4\u7684 log group \u4e0a</li> </ul> <pre><code>CLUSTER_NAME=ekscluster1\nLOG_GROUP_NAME=/aws/eks/${CLUSTER_NAME}/cluster\n\necho ${CWL_ROLE_ARN} ${FIREHOSE_ARN}\n\naws logs create-log-group \\\n--log-group-name ${LOG_GROUP_NAME}\n\naws logs put-subscription-filter \\\n--log-group-name ${LOG_GROUP_NAME} \\\n--filter-name \"to-firehose\" \\\n--filter-pattern \"\" \\\n--destination-arn ${FIREHOSE_ARN} \\\n--role-arn ${CWL_ROLE_ARN}\n</code></pre>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#glue","title":"glue","text":"<ul> <li>create database in glue catalog <code>testdb</code></li> <li>create crawler</li> </ul>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#using-cli","title":"using cli","text":"<pre><code>database_name=testdb\ns3_uri=\"${bucket_name}/${s3_prefix}/\"\n\ncrawler_role_name=AWSGlueServiceRole-$RANDOM\naws iam create-role --role-name ${crawler_role_name} --assume-role-policy-document '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"glue.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}'\naws iam attach-role-policy --role-name ${crawler_role_name} --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\n\nenvsubst &gt; crawler-role-policy.json &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::${s3_uri}*\"\n            ]\n        }\n    ]\n}\nEOF\ncrawler_policy_arn=$(aws iam create-policy \\\n--policy-name ${crawler_role_name} \\\n--policy-document file://crawler-role-policy.json |jq -r '.Policy.Arn')\naws iam attach-role-policy --role-name ${crawler_role_name} \\\n  --policy-arn ${crawler_policy_arn}\ncrawler_role_arn=$(aws iam get-role --role-name ${crawler_role_name} |jq -r '.Role.Arn')\n\naws glue create-database \\\n    --database-input '{\"Name\":\"'\"${database_name}\"'\"}'\n\nsleep 10\ncrawler_name=c1-$RANDOM\naws glue create-crawler --name ${crawler_name} \\\n--role ${crawler_role_arn} \\\n--database-name ${database_name} \\\n--targets '{\n  \"S3Targets\": [\n    {\n      \"Path\": \"s3://'\"${s3_uri}\"'\"\n    }\n  ]\n}'\n\n# run crawler later\n# aws glue start-crawler --name ${crawler_name}\n</code></pre>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#using-ui","title":"using ui","text":"","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#databrew","title":"databrew","text":"","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#using-cli_1","title":"using cli","text":"<ul> <li>download recipe first<ul> <li>cwl-recipe.json <pre><code>databrew_name=cwl-$RANDOM\ndatabrew_output=parquet-$RANDOM\n\n# \"Json={MultiLine=false}\": json line, not json document\naws databrew create-dataset \\\n--name ${databrew_name} \\\n--format JSON \\\n--format-options \"Json={MultiLine=false}\" \\\n--input \"S3InputDefinition={Bucket=${bucket_name},Key=${s3_prefix}/}\"\n\n# wget -O cwl-recipe.json 'https://github.com/panlm/aws-labs/raw/main/eks-cloudwatch-log-firehose-s3/cwl-recipe.json'\naws databrew create-recipe \\\n--name ${databrew_name} \\\n--steps file://cwl-recipe.json\n\naws databrew publish-recipe \\\n--name ${databrew_name}\n\ndatabrew_role_name=AWSGlueDataBrewServiceRole-$RANDOM\naws iam create-role --role-name ${databrew_role_name} --assume-role-policy-document '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"databrew.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}'\naws iam attach-role-policy --role-name ${databrew_role_name} --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueDataBrewServiceRole\naws iam attach-role-policy --role-name ${databrew_role_name} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\ndatabrew_role_arn=$(aws iam get-role --role-name ${databrew_role_name} |jq -r '.Role.Arn')\n\nsleep 10\n# this command will failed when bucket is empty\n# aws s3 ls --recursive s3://${bucket_name}/${s3_prefix}/\naws databrew create-recipe-job \\\n--dataset-name ${databrew_name} \\\n--name ${databrew_name} \\\n--recipe-reference 'Name='\"${databrew_name}\"',RecipeVersion=1.0' \\\n--outputs \"Format=PARQUET,Location={Bucket=${bucket_name},Key=${databrew_output}/},Overwrite=true\" \\\n--role-arn ${databrew_role_arn}\n\naws databrew create-schedule \\\n--cron-expression 'Cron(*/15 * * * ? *)' \\\n--name per15min-${databrew_name} \\\n--job-names ${databrew_name}\n</code></pre></li> </ul> </li> </ul> <p>create another crawler and query with athena <pre><code>s3_uri_2=\"${bucket_name}/${databrew_output}/\"\n\ncrawler_role_name_2=AWSGlueServiceRole-$RANDOM\naws iam create-role --role-name ${crawler_role_name_2} --assume-role-policy-document '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"glue.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}'\naws iam attach-role-policy --role-name ${crawler_role_name_2} --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\n\nenvsubst &gt; crawler-role-policy.json &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::${s3_uri_2}*\"\n            ]\n        }\n    ]\n}\nEOF\ncrawler_policy_arn_2=$(aws iam create-policy \\\n--policy-name ${crawler_role_name_2} \\\n--policy-document file://crawler-role-policy.json |jq -r '.Policy.Arn')\naws iam attach-role-policy --role-name ${crawler_role_name_2} \\\n  --policy-arn ${crawler_policy_arn_2}\ncrawler_role_arn_2=$(aws iam get-role --role-name ${crawler_role_name_2} |jq -r '.Role.Arn')\n\nsleep 10\ncrawler_name_2=parquet-${crawler_name}\naws glue create-crawler --name ${crawler_name_2} \\\n--role ${crawler_role_arn_2} \\\n--database-name ${database_name} \\\n--targets '{\n  \"S3Targets\": [\n    {\n      \"Path\": \"s3://'\"${s3_uri_2}\"'\"\n    }\n  ]\n}'\n\n# run crawler later\n# awtart-crawler --name ${crawler_name_2}\n</code></pre></p> <p>check databrew job status <pre><code>aws databrew start-job-run --name ${databrew_name}\naws databrew list-job-runs --name ${databrew_name}\n</code></pre></p> <p>if job status is <code>SUCCEEDED</code>, start crawler to create catalog in glue <pre><code>aws glue start-crawler --name ${crawler_name}\naws glue start-crawler --name ${crawler_name_2}\n</code></pre></p>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#using-ui_1","title":"using ui","text":"<ul> <li> <p>import recipe</p> <ul> <li>download </li> </ul> </li> <li> <p>create dataset from s3 or glue catalog </p> </li> <li> <p>create project from this dataset and using existed recipe </p> </li> </ul> <p></p> <ul> <li>create job as you need</li> </ul>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#athena","title":"athena","text":"<p>this table is created by firehose </p> <p>this table is created by databrew job </p>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#conclusion","title":"conclusion","text":"<p>\u6ee1\u8db3\u5ba2\u6237\u9700\u6c42\uff0c\u57fa\u4e8e\u76ee\u524ds3\u4e2d\u4fdd\u5b58\u7684\u539f\u59cb\u6570\u636e\uff0c\u5e76\u4e14\u53ef\u4ee5\u8fdb\u884c\u5b57\u6bb5\u62c6\u5206\u7b49\u4e8c\u6b21\u5904\u7406\uff0c\u672a\u6765\u53ef\u4ee5\u4f7f\u7528aws databrew\u8fdb\u884c\u66f4\u590d\u6742\u7684\u5904\u7406</p>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#alternative","title":"alternative","text":"<ul> <li>export-cloudwatch-log-group-to-s3 (link or hugo)</li> </ul>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/logging/stream-k8s-control-panel-logs-to-s3/#reference","title":"reference","text":"<ul> <li>https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/<ul> <li>https://github.com/aws-samples/amazon-cloudwatch-log-centralizer</li> </ul> </li> <li>https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-firehose-cloudwatch-logs/</li> <li>https://www.chaossearch.io/blog/cloudwatch2s3-an-easy-way-to-get-your-logs-to-aws-s3</li> <li>eks-control-panel-log-cwl-firehose-opensearch</li> <li>cloudwatch-firehose-splunk</li> <li>eks-loggroup-description</li> <li>Subscription filters with Amazon Kinesis Data Firehose<ul> <li>https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample</li> <li>https://docs.amazonaws.cn/en_us/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample</li> </ul> </li> <li>Validated IAM Service Principal List<ul> <li>https://gist.github.com/shortjared/4c1e3fe52bdfa47522cfe5b41e5d6f22</li> <li>https://github.com/henrysher/aws-china-iam-service-principal-list</li> <li>https://github.com/aws/aws-cdk/issues/1282#issuecomment-444513107</li> </ul> </li> </ul>","tags":["aws/container/eks","aws/analytics/kinesis/firehose","aws/storage/s3","aws/analytics/glue/databrew"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/","title":"Building Prometheus HA Architect with Thanos","text":"<p>[!WARNING] This is a github note</p>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#thanos-prometheus","title":"\u4f7f\u7528 Thanos \u6269\u5c55 Prometheus \u9ad8\u53ef\u7528\u76d1\u63a7\u67b6\u6784","text":"","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#_1","title":"\u67b6\u6784\u63cf\u8ff0","text":"<p>Prometheus \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u76d1\u63a7\u548c\u62a5\u8b66\u5de5\u5177\uff0c\u4e13\u4e3a\u5bb9\u5668\u5316\u548c\u4e91\u539f\u751f\u67b6\u6784\u7684\u8bbe\u8ba1\uff0c\u901a\u8fc7\u57fa\u4e8e HTTP \u7684 Pull \u6a21\u5f0f\u91c7\u96c6\u65f6\u5e8f\u6570\u636e\uff0c\u63d0\u4f9b\u529f\u80fd\u5f3a\u5927\u7684\u67e5\u8be2\u8bed\u8a00 PromQL\uff0c\u5e76\u53ef\u89c6\u5316\u5448\u73b0\u76d1\u63a7\u6307\u6807\u4e0e\u751f\u6210\u62a5\u8b66\u4fe1\u606f\u3002\u5ba2\u6237\u666e\u904d\u91c7\u7528\u5176\u7528\u4e8e Kubernetes \u7684\u76d1\u63a7\u4f53\u7cfb\u5efa\u8bbe\u3002Amazon \u4e5f\u5728 2021 \u5e74 9 \u6708\u6b63\u5f0f\u53d1\u5e03\u4e86\u6258\u7ba1\u7684 Prometheus \u670d\u52a1\uff08Amazon Managed Service for Prometheus\uff09\u7b80\u5316\u5ba2\u6237\u90e8\u7f72\u548c\u4f7f\u7528\u3002\u5e76\u4e14\u5728 2023 \u5e74 11 \u6708\u9488\u5bf9 EKS \u53d1\u5e03\u4e86\u6258\u7ba1\u7684 Prometheus \u670d\u52a1\u7684\u65e0\u4ee3\u7406\u91c7\u96c6\u529f\u80fd\uff08\u65b0\u95fb\u7a3f\uff09\uff0c\u8fdb\u4e00\u6b65\u65b9\u4fbf\u5ba2\u6237\u65e0\u9700\u63d0\u524d\u89c4\u5212\uff0c\u4ece\u800c\u53ef\u4ee5\u5f00\u7bb1\u5373\u7528\u7684\u4f7f\u7528 Prometheus \u7684\u76f8\u5173\u7ec4\u4ef6\u3002</p> <p>\u622a\u6b62\u672c\u6587\u53d1\u5e03\u4e4b\u65e5\uff0c\u5728\u4e9a\u9a6c\u900a\u4e2d\u56fd\u533a\u57df\u6682\u65f6\u6ca1\u6709\u53d1\u5e03\u6258\u7ba1\u7684 Prometheus \u670d\u52a1\u3002\u56e0\u6b64\u9488\u5bf9\u4e2d\u56fd\u5ba2\u6237\u5982\u4f55\u90e8\u7f72\u4e00\u5957 Prometheus \u76d1\u63a7\u5e73\u53f0\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5bfc\u53ef\u4ee5\u5e2e\u52a9\u5ba2\u6237\u5feb\u901f\u4f7f\u7528 Prometheus\uff0c\u4ece\u800c\u5c06\u7cbe\u529b\u4e13\u6ce8\u4e8e\u4e1a\u52a1\u9700\u6c42\u3002</p> <p>\u72ec\u7acb Kubernetes \u96c6\u7fa4\u901a\u5e38\u4f7f\u7528 Prometheus Operator \u90e8\u7f72\u6240\u6709\u76f8\u5173\u7ec4\u4ef6\u5305\u62ec Alert Manager\u3001Grafana \u7b49\u3002\u8fd9\u79cd\u72ec\u7acb\u90e8\u7f72\u7684\u76d1\u63a7\u67b6\u6784\u4f18\u70b9\u662f\u90e8\u7f72\u65b9\u4fbf\uff0c\u6570\u636e\u6301\u4e45\u5316\u4f7f\u7528 EBS \u4e5f\u53ef\u4ee5\u6ee1\u8db3\u5927\u90e8\u5206\u573a\u666f\u4e0b\u67e5\u8be2\u6027\u80fd\u7684\u8981\u6c42\uff0c\u4f46\u7f3a\u70b9\u4e5f\u663e\u800c\u6613\u89c1\uff0c\u5373\u65e0\u6cd5\u4fdd\u5b58\u592a\u957f\u65f6\u95f4\u7684\u5386\u53f2\u6570\u636e\u3002\u800c\u4e14\u5f53\u5ba2\u6237\u73af\u5883\u4e2d\u96c6\u7fa4\u6570\u91cf\u8f83\u591a\uff0c\u76d1\u63a7\u5e73\u53f0\u81ea\u8eab\u7684\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u8981\u6c42\u8f83\u9ad8\uff0c\u540c\u65f6\u5e0c\u671b\u63d0\u4f9b\u5168\u5c40\u67e5\u8be2\u65f6\uff0c\u7ba1\u7406\u548c\u7ef4\u62a4\u7684\u5de5\u4f5c\u91cf\u4e5f\u968f\u4e4b\u589e\u52a0\u3002</p> <p>Thanos\u662f\u4e00\u5957\u5f00\u6e90\u7ec4\u4ef6\uff0c\u6784\u5efa\u5728 Prometheus \u4e4b\u4e0a\uff0c\u7528\u4ee5\u89e3\u51b3 Prometheus \u5728\u591a\u96c6\u7fa4\u5927\u89c4\u6a21\u73af\u5883\u4e0b\u7684\u9ad8\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002\u5f53\u9700\u8981\u505a\u5386\u53f2\u6027\u80fd\u6570\u636e\u5206\u6790\uff0c\u6216\u8005\u4f7f\u7528 Prometheus \u8fdb\u884c\u6210\u672c\u5206\u6790\u7684\u573a\u666f\u90fd\u4f1a\u4f9d\u8d56\u4e8e\u8f83\u957f\u65f6\u95f4\u7684\u5386\u53f2\u6570\u636e\u3002Thanos \u4e3b\u8981\u901a\u8fc7\u63a5\u6536\u5e76\u5b58\u50a8 Prometheus \u7684\u591a\u96c6\u7fa4\u6570\u636e\u526f\u672c\uff0c\u5e76\u63d0\u4f9b\u5168\u5c40\u67e5\u8be2\u548c\u4e00\u81f4\u6027\u6570\u636e\u8bbf\u95ee\u63a5\u53e3\u7684\u65b9\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e8e Prometheus \u7684\u53ef\u9760\u6027\u3001\u4e00\u81f4\u6027\u548c\u53ef\u7528\u6027\u4fdd\u969c\uff0c\u4ece\u800c\u89e3\u51b3\u4e86 Prometheus \u5355\u96c6\u7fa4\u5728\u5b58\u50a8\u3001\u67e5\u8be2\u5386\u53f2\u6570\u636e\u548c\u5907\u4efd\u7b49\u65b9\u9762\u7684\u6269\u5c55\u6027\u6311\u6218\u3002</p> <p>\u5728\u8ba8\u8bba\u57fa\u4e8e Thanos \u7684\u5404\u79cd Prometheus \u76d1\u63a7\u67b6\u6784\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u4e86\u89e3\u4e0b Thanos \u53ca\u5176\u5e38\u7528\u7684\u7ec4\u4ef6\uff0c\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u53c2\u8003 thanos.io \u3002</p> <ul> <li>Sidecar\uff08\u8fb9\u8f66\uff09\uff1a\u8fd0\u884c\u5728 Prometheus \u7684 Pod \u4e2d\uff0c\u8bfb\u53d6\u5176\u6570\u636e\u4ee5\u4f9b\u67e5\u8be2\u548c/\u6216\u4e0a\u4f20\u5230\u4e91\u5b58\u50a8\u3002</li> <li>Store\uff08\u5b58\u50a8\u7f51\u5173\uff09\uff1a\u7528\u4e8e\u4ece\u5bf9\u8c61\u5b58\u50a8\u6876\uff08\u4f8b\u5982\uff1aAWS S3\uff09\u4e0a\u67e5\u8be2\u6570\u636e\u3002</li> <li>Compactor\uff08\u538b\u7f29\u5668)\uff1a\u5bf9\u5b58\u50a8\u5728\u5bf9\u8c61\u5b58\u50a8\u6876\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u538b\u7f29\u3001\u805a\u5408\u5386\u53f2\u6570\u636e\u4ee5\u51cf\u5c0f\u91c7\u6837\u7cbe\u5ea6\u5e76\u957f\u4e45\u4fdd\u7559\u3002</li> <li>Receive\uff08\u63a5\u6536\u5668\uff09\uff1a\u63a5\u6536\u6765\u81ea Prometheus \u8fdc\u7a0b\u5199\u5165\u65e5\u5fd7\u7684\u6570\u636e\uff0c\u5e76\u5c06\u5176\u4e0a\u4f20\u5230\u5bf9\u8c61\u5b58\u50a8\u3002</li> <li>Ruler\uff08\u89c4\u5219\u5668\uff09\uff1a\u9488\u5bf9 Thanos \u4e2d\u7684\u6570\u636e\u8bc4\u4f30\u8bb0\u5f55\u548c\u8b66\u62a5\u89c4\u5219\u3002</li> <li>Query\uff08\u67e5\u8be2\u5668\uff09\uff1a\u5b9e\u73b0 Prometheus \u7684 v1 API\uff0c\u67e5\u8be2\u5e76\u6c47\u603b\u6765\u81ea\u5e95\u5c42\u7ec4\u4ef6\u7684\u6570\u636e\u3002\u5c06\u6240\u6709\u6570\u636e\u6e90\u6dfb\u52a0\u4e3a Query \u7684 Endpoint\uff0c\u5305\u62ec Sidecar\u3001 Store\u3001 Receive \u7b49\u3002</li> <li>Query Frontend\uff08\u67e5\u8be2\u524d\u7aef\uff09\uff1a\u5b9e\u73b0 Prometheus \u7684 v1 API\uff0c\u5c06\u5176\u4ee3\u7406\u7ed9\u67e5\u8be2\u5668\uff0c\u540c\u65f6\u7f13\u5b58\u54cd\u5e94\uff0c\u5e76\u53ef\u4ee5\u62c6\u5206\u67e5\u8be2\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002</li> </ul> <p>\u7b2c\u4e00\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0b\u56fe\u84dd\u8272\u548c\u7eff\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a</p> <ul> <li>\u88ab\u76d1\u63a7\u96c6\u7fa4\uff08Observee\uff09\u90e8\u7f72 Prometheus \u4e14\u542f\u7528 Thanos \u7684 Sidecar \u65b9\u5f0f\u5c06\u76d1\u63a7\u7684\u5386\u53f2\u6570\u636e\u5b9a\u671f\u5f52\u6863\u5230 S3\uff0c\u901a\u8fc7\u90e8\u7f72 Thanos Store \u7ec4\u4ef6\u67e5\u8be2\u5386\u53f2\u6570\u636e\uff08\u4e0b\u56fe\u4e2d Store \u7ec4\u4ef6\u90e8\u7f72\u5728\u76d1\u63a7\u96c6\u7fa4\u4e2d\uff09\uff0c\u88ab\u76d1\u63a7\u96c6\u7fa4\u4e2d\u4e0d\u542f\u7528 Grafana \u7ec4\u4ef6\uff1b</li> <li>\u76d1\u63a7\u96c6\u7fa4\uff08Observer\uff09\u9664\u4e86\u90e8\u7f72 Prometheus \u4e4b\u5916\uff0c\u5c06\u7edf\u4e00\u90e8\u7f72 Grafana \u4f5c\u4e3a Dashboard \u5c55\u793a\u3002</li> </ul> <p>\u7b2c\u4e8c\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0b\u56fe\u7ea2\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09:</p> <ul> <li>\u88ab\u76d1\u63a7\u96c6\u7fa4\uff08Observee\uff09\u9664\u4e86\u542f\u7528 Thanos Sidecar \u4e4b\u5916\uff0c\u8fd8\u542f\u7528\u4e86 Prometheus \u7684 Remote Write \u529f\u80fd\uff0c\u5c06\u672a\u5f52\u6863\u7684\u6570\u636e\u4ee5 WAL \u65b9\u5f0f\u8fdc\u7a0b\u4f20\u8f93\u5230\u90e8\u7f72\u5728\u76d1\u63a7\u96c6\u7fa4\uff08Observer\uff09\u4e0a\u7684 Thanos Receive\uff0c\u4ee5\u4fdd\u8bc1\u6570\u636e\u7684\u5197\u4f59\u5ea6\u3002 Thanos Receive \u540c\u6837\u53ef\u4ee5\u5c06\u5386\u53f2\u76d1\u63a7\u6570\u636e\u5f52\u6863\u5230 S3 \u4e0a\uff0c\u4e14\u652f\u6301\u88ab Thanos Query \u76f4\u63a5\u67e5\u8be2\uff0c\u540c\u65f6\u907f\u514d\u76f4\u63a5\u67e5\u8be2 Sidecar \u800c\u7ed9\u88ab\u76d1\u63a7\u96c6\u7fa4\u5e26\u6765\u989d\u5916\u7684\u6027\u80fd\u635f\u8017\u3002</li> </ul> <p>\u7b2c\u4e09\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0b\u56fe\u9ec4\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a</p> <ul> <li>\u5728\u591a\u96c6\u7fa4\u76d1\u63a7\u573a\u666f\u4e0b\uff0c\u4e00\u822c\u4f1a\u5728\u6bcf\u4e2a\u96c6\u7fa4\u90e8\u7f72\u72ec\u7acb\u7684 Prometheus \u7ec4\u4ef6\u3002Prometheus \u63d0\u4f9b Agent Mode \u9488\u5bf9\u8fd9\u6837\u7684\u573a\u666f\u53ef\u4ee5\u6700\u5c0f\u5316\u8d44\u6e90\u5360\u7528\uff0c\u76f4\u63a5\u542f\u7528 Remote Write \u529f\u80fd\u5c06\u76d1\u63a7\u6570\u636e\u96c6\u4e2d\u4fdd\u5b58 \uff08\u53ef\u4ee5\u662f\u53e6\u4e00\u4e2a Prometheus \u96c6\u7fa4\uff0c\u6216\u8005 Thanos Receive \u7ec4\u4ef6\uff09\u3002\u5728 AWS \u4e0a\u53ef\u4ee5\u4f7f\u7528\u6258\u7ba1\u7684 Prometheus \u670d\u52a1\u4f5c\u4e3a\u96c6\u4e2d\u76d1\u63a7\u6570\u636e\u6301\u4e45\u5316\uff0c\u63d0\u4f9b\u6700\u597d\u7684\u6027\u80fd\u548c\u6700\u4f4e\u7684\u7ef4\u62a4\u6210\u672c\u3002</li> </ul> <p></p> <p>\u4ee5\u4e0b\u603b\u7ed3\u4e86\u57fa\u4e8e Thanos \u7684\u5404\u79cd Prometheus \u76d1\u63a7\u67b6\u6784\u6240\u9002\u5408\u7684\u573a\u666f\uff1a \u7b2c\u4e00\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0a\u56fe\u84dd\u8272\u548c\u7eff\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a\u9002\u5408\u7edd\u5927\u90e8\u5206\u751f\u4ea7\u73af\u5883\uff0c\u5c24\u5176\u5728\u4e9a\u9a6c\u900a\u4e2d\u56fd\u533a\u57df\u6ca1\u6709\u6258\u7ba1 Prometheus \u670d\u52a1\uff0c\u6b64\u7c7b\u67b6\u6784\u4e5f\u662f\u5ba2\u6237\u9996\u9009\u3002</p> <ul> <li>\u4f18\u70b9<ul> <li>\u67b6\u6784\u7b80\u5355</li> <li>\u53ea\u6709\u4e00\u4efd\u76d1\u63a7\u6570\u636e\uff0c\u6700\u5c0f\u5316\u5b58\u50a8\u6210\u672c\u548c\u5176\u4ed6\u8d44\u6e90\u5f00\u9500</li> </ul> </li> <li>\u7f3a\u70b9 <ul> <li>\u901a\u8fc7 Sidecar \u67e5\u8be2\u5b9e\u65f6\u76d1\u63a7\u6570\u636e\u65f6\uff0c\u5c06\u7ed9\u88ab\u76d1\u63a7\u96c6\u7fa4\u5e26\u6765\u989d\u5916\u6027\u80fd\u635f\u8017</li> </ul> </li> </ul> <p>\u7b2c\u4e8c\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0a\u56fe\u7ea2\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a\u5728 Thanos 0.19 \u7248\u672c\u4e4b\u524d\uff0cSidecar \u8fd8\u6ca1\u6709\u5b9e\u73b0 StoreAPI \u65f6\uff0c\u53ea\u80fd\u901a\u8fc7 Receive \u67e5\u8be2\u6700\u65b0\u7684\u6027\u80fd\u6570\u636e\u3002\u9002\u5408\u9700\u8981\u591a\u526f\u672c\u76d1\u63a7\u6570\u636e\u7684\u7279\u6b8a\u573a\u666f\u3002</p> <ul> <li>\u4f18\u70b9<ul> <li>\u76d1\u63a7\u6570\u636e\u5197\u4f59\uff0c\u53ef\u4ee5\u4f7f\u7528 Compactor \u5bf9\u6570\u636e\u8fdb\u884c\u538b\u7f29\u3001\u805a\u5408\u5386\u53f2\u6570\u636e\u4ee5\u51cf\u5c11\u5b58\u50a8\u6210\u672c</li> <li>\u76f4\u63a5\u4ece Thanos Receive \u67e5\u8be2\u5b9e\u65f6\u6027\u80fd\u6570\u636e\uff0c\u5bf9\u88ab\u76d1\u63a7\u96c6\u7fa4\u6ca1\u6709\u989d\u5916\u6027\u80fd\u635f\u8017</li> </ul> </li> <li>\u7f3a\u70b9<ul> <li>\u67b6\u6784\u590d\u6742\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u5bf9\u5e94\u4e00\u7ec4 Thanos Receive\uff0c\u5efa\u8bae\u914d\u7f6e\u526f\u672c\u6570\u91cf\u548c\u8d44\u6e90\u7b49\u4e0e\u6e90\u96c6\u7fa4\u7684 Prometheus \u526f\u672c\u6570\u91cf\u548c\u8d44\u6e90\u76f8\u540c</li> </ul> </li> </ul> <p>\u7b2c\u4e09\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0a\u56fe\u9ec4\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a\u6700\u5927\u7a0b\u5ea6\u51cf\u5c11\u5bf9\u4e8e\u88ab\u76d1\u63a7\u96c6\u7fa4\u7684\u8d44\u6e90\u5360\u7528\uff0c\u53ef\u4ee5\u4f7f\u7528 Prometheus \u7684 Agent Mode\uff1a</p> <ul> <li>\u4f18\u70b9<ul> <li>\u67b6\u6784\u7b80\u5355\uff0c\u4f7f\u7528 Agent Mode \u51e0\u4e4e\u65e0\u72b6\u6001\uff0c\u53ef\u4ee5\u4f7f\u7528\u9664 Stateful \u4e4b\u5916\u7684\u5176\u4ed6 Deployment\uff0c\u672c\u5730\u5b58\u50a8\u9700\u6c42\u4f4e\uff08\u9664\u975e\u8fdc\u7a0b Endpoint \u4e0d\u53ef\u7528\u65f6\uff0c\u672c\u5730\u7f13\u5b58\u6570\u636e\u4ee5\u91cd\u8bd5\uff09</li> <li>\u53ef\u5b9e\u73b0\u96c6\u4e2d\u544a\u8b66 - \u544a\u8b66\u5c06\u901a\u8fc7 Thanos Ruler \u5b9a\u4e49\uff0c\u901a\u8fc7 Thanos Query \u67e5\u8be2 Receive \u5e76\u53d1\u9001\u5230\u76d1\u63a7\u96c6\u7fa4\u7684 Alert Manager \u5b9e\u73b0</li> </ul> </li> <li>\u7f3a\u70b9 <ul> <li>\u65e0\u76d1\u63a7\u6570\u636e\u5197\u4f59\uff0c\u67d0\u4e9b\u7ec4\u4ef6\u5c06\u65e0\u6cd5\u5728 Agent Mode \u4e0b\u542f\u7528\uff0c\u4f8b\u5982\uff1aSidecar\u3001Alert\u3001Rules \uff08\u53c2\u89c1\u6587\u6863\uff09</li> </ul> </li> </ul> <p>\u5728\u652f\u6301\u6258\u7ba1 Prometheus \u670d\u52a1\u7684\u4e9a\u9a6c\u900a\u533a\u57df\uff08\u6587\u6863\uff09\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u6258\u7ba1\u670d\u52a1\u66ff\u4ee3\u7b2c\u4e09\u79cd\u76d1\u63a7\u67b6\u6784\uff0c\u5b9e\u73b0\u5b8c\u5168\u5f00\u7bb1\u5373\u7528\uff0c\u540c\u65f6\u907f\u514d\u7ba1\u7406 Thanos \u7ec4\u4ef6\uff0c\u53ea\u9700\u8981\u90e8\u7f72 Grafana \u5373\u53ef\u3002</p>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#walkthrough","title":"walkthrough","text":"<p>Prometheus Operator \u63d0\u4f9b Kubernetes \u539f\u751f\u90e8\u7f72\u548c\u7ba1\u7406 Prometheus \u53ca\u76f8\u5173\u76d1\u63a7\u7ec4\u4ef6\u7684\u529f\u80fd\u3002\u8be5\u9879\u76ee\u7684\u76ee\u7684\u662f\u7b80\u5316\u548c\u81ea\u52a8\u914d\u7f6e Kubernetes \u96c6\u7fa4\u57fa\u4e8e Prometheus \u7684\u76d1\u63a7\u5806\u6808\u3002\u672c\u5b9e\u9a8c\u57fa\u4e8e Prometheus Operator \u90e8\u7f72\u4f5c\u4e3a\u57fa\u7840\uff0c\u5e76\u901a\u8fc7 values.yaml \u53c2\u6570\u6587\u4ef6\u5b9a\u5236\uff0c\u8be6\u7ec6\u4fe1\u606f\u53c2\u89c1\uff08Github\uff09\u3002\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u521b\u5efa 3 \u4e2a EKS \u96c6\u7fa4\uff0c\u5206\u522b\u5bf9\u5e94\u4e0a\u56fe\u4e2d\u7684\u84dd\u8272\u3001\u7ea2\u8272\u3001\u9ec4\u8272\u96c6\u7fa4\u9a8c\u8bc1 Thanos \u76f8\u5173\u914d\u7f6e\u3002 </p> <p>\u672c\u5b9e\u9a8c\u4e2d\u5c06\u4f7f\u7528 Terraform \u5feb\u901f\u521b\u5efa EKS \u96c6\u7fa4\uff0c\u5e76\u4e14\u81ea\u52a8\u90e8\u7f72\u4e0a\u56fe\u4e2d\u76f8\u5173\u7684 Prometheus \u76d1\u63a7\u67b6\u6784\uff08Github\uff09\uff0c\u548cThanos \u76f8\u5173\u7ec4\u4ef6\uff08Github\uff09\uff0c\u5e26\u5927\u5bb6\u4e86\u89e3 Thanos \u7684\u914d\u7f6e\u548c\u5de5\u4f5c\u539f\u7406\u3002</p>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#prometheus-on-eks","title":"Prometheus on EKS","text":"<ul> <li>\u9996\u5148\u6211\u4eec\u5c06\u5148\u521b\u5efa 3 \u4e2a EKS \u96c6\u7fa4, <code>ekscluster1</code> \u662f\u76d1\u63a7\u96c6\u7fa4\uff08Observer\uff09, <code>ekscluster2</code>  \u548c <code>ekscluster3</code> \u662f\u88ab\u76d1\u63a7\u96c6\u7fa4\uff08Observee\uff09 </li> <li>\u672c\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u4e86\u9884\u8bbe\u7684\u5b50\u57df\u540d\u7528\u4e8e\u7b80\u5316\u670d\u52a1\u4e4b\u95f4\u7684\u8bbf\u95ee\u548c\u5bf9\u5916\u66b4\u9732\u3002\u9700\u8981\u63d0\u524d\u5728 Route53 \u4e2d\u521b\u5efa\u8be5\u5b50\u57df\u540d\uff08\u590d\u5236\u94fe\u63a5\u4e2d\u7684\u51fd\u6570\u5e76\u7c98\u8d34\u5230\u547d\u4ee4\u884c\uff09 <pre><code>PARENT_DOMAIN_NAME=eks0103.aws.panlm.xyz\ncreate-hosted-zone -n ${PARENT_DOMAIN_NAME}\n</code></pre></li> <li>\u7136\u540e\u5728\u4e0a\u6e38 Route53 \u4e2d\u521b\u5efa\u5bf9\u5e94\u7684 NS \u8bb0\u5f55\uff08\u590d\u5236\u94fe\u63a5\u4e2d\u7684\u51fd\u6570\u5e76\u7c98\u8d34\u5230\u547d\u4ee4\u884c\uff09\uff0c\u6b64\u5904\u79bb\u5f00 Cloud9 \u7a97\u53e3\uff0c\u6216\u8005\u786e\u4fdd\u547d\u4ee4\u884c\u6709\u4e0a\u6e38 Route53 \u76f8\u5e94\u7684\u6743\u9650 <pre><code>PARENT_DOMAIN_NAME=eks0103.panlm.xyz\nNS=\"copy NS records from previous output and paste here\"\ncreate-ns-record -n $PARENT_DOMAIN_NAME -s \"$NS\" # double quote is mandortory\n</code></pre></li> <li>\u521b\u5efa\u672c\u5b9e\u9a8c\u7684\u76ee\u5f55\uff0c\u540e\u7eed\u5c06\u5728\u6b64\u8def\u5f84\u4e0b\u514b\u9686 2 \u4e2a REPO\uff0c\u5206\u522b\u7528\u4e8e\u521b\u5efa EKS \u96c6\u7fa4\u7684 Terraform \u4ee3\u7801\u548c Thanos \u914d\u7f6e\u793a\u4f8b <pre><code>LAB_HOME=~/environment/lab-thanos\nmkdir ${LAB_HOME}\n</code></pre></li> <li>\u5148\u83b7\u53d6 Thanos \u914d\u7f6e\u6a21\u677f <pre><code>cd ${LAB_HOME}\ngit clone https://github.com/panlm/thanos-example.git\ncd thanos-example\n</code></pre></li> <li> <p>\u4f7f\u7528\u4e0b\u9762\u53c2\u6570\u8fdb\u884c\u521b\u5efa\u672c\u5b9e\u9a8c\u4f7f\u7528\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u4f4d\u4e8e <code>POC</code> \u76ee\u5f55\u4e0b <pre><code>CLUSTER_NAME_1=ekscluster1\nCLUSTER_NAME_2=ekscluster2\nCLUSTER_NAME_3=ekscluster3\nDOMAIN_NAME=thanos.${PARENT_DOMAIN_NAME} # this domain will be created by terraform\nTHANOS_BUCKET_NAME=thanos-store-$(TZ=EAT-8 date +%Y%m%d-%H%M%S)\nAWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\nexport CLUSTER_NAME_1 CLUSTER_NAME_2 CLUSTER_NAME_3 DOMAIN_NAME THANOS_BUCKET_NAME AWS_DEFAULT_REGION\n\naws s3 mb s3://${THANOS_BUCKET_NAME}\n\nmkdir POC\ncd POC-template\nfind ./ -type d -name \"[a-z]*\" -exec mkdir ../POC/{} \\;\n\nfind ./ -type f -name \"*\" |while read filename ; do\n  cat $filename |envsubst '$CLUSTER_NAME_1 $CLUSTER_NAME_2 $CLUSTER_NAME_3 $DOMAIN_NAME $THANOS_BUCKET_NAME $AWS_DEFAULT_REGION' &gt; ../POC/$filename\ndone\n</code></pre></p> </li> <li> <p>\u83b7\u53d6 Terraform \u4ee3\u7801\u5f00\u59cb\u521b\u5efa\u73af\u5883\uff08\u53c2\u8003\u6587\u6863\uff09 <pre><code>cd ${LAB_HOME}\ngit clone https://github.com/panlm/eks-blueprints-clusters.git\ncd eks-blueprints-clusters/multi-cluster-thanos\n</code></pre></p> </li> <li>\u4fee\u6539 <code>terraform.tfvars</code> \u914d\u7f6e\u5982\u4e0b\uff0c\u8bf7\u6309\u7167 <pre><code>aws_region          = \"us-east-2\"\nenvironment_name    = \"thanos\" # sub-domain will be created by terraform\ncluster_version     = \"1.27\"\nhosted_zone_name    = \"eks0103.aws.panlm.xyz\" # your existing hosted zone\neks_admin_role_name = \"\" # Additional role admin in the cluster \n</code></pre></li> <li>\u521b\u5efa\u72ec\u7acb\u7684\u73af\u5883\u7528\u4e8e\u672c\u5b9e\u9a8c\uff0c\u5305\u62ec VPC\uff0cSecret\uff0c\u73af\u5883\u7279\u5b9a\u7684\u5b50\u57df\u540d\u7b49 <pre><code>cd environment\nterraform init\nterraform apply -auto-approve\n</code></pre></li> <li>\u521b\u5efa ekscluster1\uff08\u76ee\u5f55\u540d\u5373\u4e3a EKS \u96c6\u7fa4\u540d\uff09\uff0c\u5e76\u6839\u636e\u547d\u4ee4\u884c\u8f93\u51fa\u4fdd\u5b58 kubeconfig \u914d\u7f6e\u3002\u521b\u5efa\u96c6\u7fa4\u540e\u5c06\u81ea\u52a8\u68c0\u67e5 <code>thanos-example/POC</code> \u8def\u5f84\u4e0b\u76f8\u5173\u6587\u4ef6\u5b58\u5728\uff0c\u5219\u5b89\u88c5 Prometheus \u548c Thanos \u7ec4\u4ef6\u3002 <pre><code>cd ../ekscluster1\nterraform init\nterraform apply -auto-approve\n</code></pre></li> <li>\u6309\u7167\u4e0a\u8ff0\u64cd\u4f5c\u5206\u522b\u8fdb\u5165\u5176\u4ed6\u4e24\u4e2a\u76ee\u5f55\u521b\u5efa ekscluster2 \u548c ekscluster3\uff0c\u53ef\u4ee5\u901a\u8fc7\u5176\u4ed6 terminal \u540c\u65f6\u64cd\u4f5c\u3002</li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#thanos","title":"Thanos \u7ec4\u4ef6","text":"<p>\u6240\u6709 Thanos \u76f8\u5173\u7684\u7ec4\u4ef6\u5c06\u90e8\u7f72\u5728\u88ab\u76d1\u63a7\u96c6\u7fa4\uff08ekscluster1\uff09\u3002\u672c\u5b9e\u9a8c\u4f7f\u7528 Terraform \u8c03\u7528 <code>thanos-values</code> \u76ee\u5f55\u4e0b\u81ea\u5b9a\u4e49\u914d\u7f6e\u90e8\u7f72\u73af\u5883\uff0c\u540c\u65f6\u63d0\u4f9b <code>thanos-yaml</code> \u76ee\u5f55\u4e0b\u914d\u7f6e\u5b9e\u4f8b\u4f9b\u53c2\u8003\u3002</p>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#store","title":"Store","text":"<p>Store \u7ec4\u4ef6\u5c06\u53ea\u7528\u4e8e\u8bbf\u95ee S3 \u4e0a\u7684\u5386\u53f2\u6570\u636e\uff0c\u6bcf\u4e2a Store \u4f7f\u7528\u72ec\u7acb\u7684 secret \u914d\u7f6e\uff0c\u4e14\u5bf9\u5e94\u4e00\u4e2a\u88ab\u76d1\u63a7\u96c6\u7fa4\u3002</p> <ul> <li>\u67e5\u770b Terraform \u81ea\u52a8\u521b\u5efa\u7684 s3 \u7684 secret \u914d\u7f6e\u548c service account <pre><code>kubectl get secret -n thanos\nkubectl get sa -n thanos\nkubectl get po -n thanos |grep store\n</code></pre></li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#query-query-frontend","title":"Query \u548c Query Frontend","text":"<p>Query Frontend \u5bf9\u5916\u63d0\u4f9b\u4e0e Prometheus \u517c\u5bb9\u7684 API\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f5c\u4e3a Prometheus \u7c7b\u578b\u7684\u6570\u636e\u6e90\u6dfb\u52a0\u5230 Grafana \u4e2d\uff1bQuery \u662f\u65e0\u72b6\u6001\u4e14\u53ef\u4ee5\u6a2a\u5411\u6269\u5c55\uff0c\u5bf9\u5916\u63d0\u4f9b Prometheus \u517c\u5bb9\u7684 API \u4f9b\u5916\u90e8\u5de5\u5177\u67e5\u8be2\uff0c\u4f8b\u5982 Grafana\uff0c\u672c\u5b9e\u9a8c\u4e2d Query Frontend \u5b9e\u73b0\u5c06\u67e5\u8be2\u5206\u7247\uff08split\uff09\uff0c\u4f7f\u7528 Query \u7ec4\u4ef6\u63d0\u4f9b\u67e5\u8be2\u7684\u6a2a\u5411\u6269\u5c55\u80fd\u529b</p> <ul> <li><code>query-frontend-deployment</code> \u6587\u4ef6\u4e2d\u6307\u5b9a\u4e86\u5982\u4e0b\u5206\u5272\u67e5\u8be2\u7684\u53c2\u6570\uff0c\u5bf9\u5916\u57df\u540d\u5b9a\u4e49\u5728 <code>query-frontend-service</code> \u6587\u4ef6\u4e2d <pre><code>        - --query-range.split-interval=4h\n        - --labels.split-interval=4h\n</code></pre></li> <li>Query \u7ec4\u4ef6\u914d\u7f6e\u4e86\u67e5\u8be2\u76ee\u6807\u7aef\u70b9\uff0c\u5305\u62ec Sidecar\uff0cReceive\uff0cStore \u7b49\uff0c\u672c\u5b9e\u9a8c\u5747\u901a\u8fc7\u57df\u540d\u8bbf\u95ee</li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#receive","title":"Receive","text":"<ul> <li>\u90e8\u7f72\u72ec\u7acb\u7684 Thanos Receive \u7ec4\u4ef6\uff0c\u5206\u522b\u5bf9\u5e94 ekscluster2 \u548c ekscluster3\u3002\u5efa\u8bae\u5206\u914d Receive \u7ec4\u4ef6\u7684\u8d44\u6e90\u548c\u6570\u91cf\u9700\u8981\u548c\u88ab\u76d1\u63a7\u96c6\u7fa4\u4e2d Prometheus \u91c7\u96c6\u7ec4\u4ef6\u7684\u8d44\u6e90\u548c\u6570\u91cf\u76f8\u5f53\u3002</li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#grafana","title":"Grafana","text":"<p>\u4f7f\u7528 Prometheus Operator \u90e8\u7f72 Grafana \u5c06\u81ea\u5e26\u4e00\u4e9b\u5e38\u7528\u7684 Dashboard\uff0c\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c\u7b80\u5355\u914d\u7f6e\u5b9e\u73b0\u591a\u96c6\u7fa4\u6570\u636e\u67e5\u8be2\u3002</p>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#dashboard","title":"\u67e5\u770b\u591a\u96c6\u7fa4 Dashboard","text":"<ul> <li>\u8bbf\u95ee Grafana \u57df\u540d\uff0c\u53ef\u4ee5\u901a\u8fc7 <code>thanos-example/POC/prometheus/values-ekscluster1-1.yaml</code> \u4e2d\u67e5\u770b</li> <li>\u4fee\u6539 Grafana \u9ed8\u8ba4\u5bc6\u7801 <code>prom-operator</code></li> <li>\u6dfb\u52a0 Thanos Query Frontend \u4f5c\u4e3a Prometheus  \u7c7b\u578b\u7684\u6570\u636e\u6e90<ul> <li>\u5b9a\u4e49\u6570\u636e\u6e90\u540d\u79f0 <code>thanoslab</code>\uff0c\u5e76\u8bbe\u7f6e\u4e3a\u9ed8\u8ba4</li> <li>\u76f4\u63a5\u4f7f\u7528 Kubernetes \u5185\u90e8\u57df\u540d: http://thanos-query-frontend.thanos.svc.cluster.local:9090</li> <li>\u6216\u8005\u4e0a\u6587\u63d0\u5230\u7684 Query Frontend Service \u7ed1\u5b9a\u7684\u57df\u540d\u8bbf\u95ee</li> </ul> </li> <li>\u67e5\u770b\u9884\u7f6e\u7684 dashboard\uff1a <code>Kubernetes / Compute Resources / Multi-Cluster</code> </li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#dashboard_1","title":"\u67e5\u770b\u5176\u4ed6 Dashboard","text":"<p>\u901a\u8fc7\u7b80\u5355\u4fee\u6539\u5373\u53ef\u5c06\u5176\u4ed6\u9884\u7f6e dashboard \u4fee\u6539\u4e3a\u652f\u6301\u591a\u96c6\u7fa4\u67e5\u8be2\uff1a</p> <ul> <li>\u6253\u5f00\u9884\u7f6e\u7684 dashboard\uff1a<code>Kubernetes / Compute Resources / Namespace (Pods)</code></li> <li>\u70b9\u51fb\u9f7f\u8f6e\u56fe\u6807\u8fdb\u5165 Dashboard Settings \u754c\u9762</li> <li>\u5728 Variable \u4e2d \u627e\u5230 <code>cluster</code> \u53d8\u91cf\uff0c\u6309\u7167\u4e0b\u9762\u622a\u56fe\u4fee\u6539<ul> <li>Show on dashboard \u8bbe\u7f6e\u4e3a <code>Label and value</code></li> <li>Data source \u8bbe\u7f6e\u4e3a <code>thanoslab</code> </li> </ul> </li> <li>\u70b9\u51fb Apply \u4fdd\u5b58\u53d8\u91cf\u4fee\u6539</li> <li>\u70b9\u51fb Save As \u4fdd\u5b58 Dashboard \u5373\u53ef</li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#thanos-query-frontend","title":"\u67e5\u770b Thanos Query Frontend \u6570\u636e\u6e90\u63a5\u53e3","text":"<ul> <li>\u6253\u5f00 Frontend Service \u7684\u5916\u90e8\u57df\u540d: <code>thanos-query-frontend.${DOMAIN_NAME}</code><ul> <li>Receive \u8868\u4e2d\uff0cMin Time \u6709\u503c\u8868\u793a Prometheus \u4f7f\u7528 Remote Write \u5c06\u6570\u636e\u5199\u5165 Receive \u6210\u529f</li> <li>Prometheus \u4e2d\u6536\u5230\u6027\u80fd\u6307\u6807\u6570\u636e\u4ee5 WAL \u5f62\u5f0f\u4fdd\u5b58\u5728\u5185\u5b58\u5e76\u6301\u4e45\u5316\u5230\u78c1\u76d8\uff0c\u542f\u7528 Sidecar \u540e\uff0c\u81ea\u52a8\u5c06 <code>min-block-duration</code> \u548c <code>max-block-duration</code> \u8bbe\u7f6e\u6210 2\u5c0f\u65f6\u4e14\u65e0\u6cd5\u66f4\u6539\uff0c\u5373\u6bcf\u4e2a\u6570\u636e\u5757\u6587\u4ef6\u4fdd\u5b582\u5c0f\u65f6\u7684\u6027\u80fd\u6307\u6807\u3002\u540c\u65f6 Prometheus \u8bbe\u7f6e <code>retention</code> \u65f6\u95f4\u548c <code>retentionSize</code> \u5927\u5c0f\uff0c\u672a\u5230 <code>retention</code> \u524d\uff0cSidecar \u8868\u4e2d\uff0cMin Time \u5c06\u663e\u793a\u4e3a <code>-</code>\uff0c\u5230 <code>retention</code> \u540e\uff0c\u6570\u636e\u7531 Sidecar \u5199\u5165 S3\uff0c\u6b64\u65f6\u4f1a\u663e\u793a\u6700\u65e9\u6570\u636e\u7684\u65f6\u95f4\u6233</li> <li>Store \u8868\u4e2d\uff0cMin Time \u6709\u503c\u8868\u793a\u6570\u636e\u88ab\u5199\u5165 S3\uff0c\u4e14\u663e\u793a\u6700\u65e9\u6570\u636e\u7684\u65f6\u95f4\u6233</li> </ul> </li> <li>\u6570\u636e\u901a\u8fc7 Label \u6807\u6ce8\u6765\u6e90\u7684\u96c6\u7fa4\u540d\u79f0\u4ee5\u53ca\u91c7\u96c6\u6570\u636e\u7684 Pod \u540d\u79f0</li> <li>\u672c\u5b9e\u9a8c\u5728 Prometheus \u4e2d\u8bbe\u7f6e\u4e86 3 \u4e2a <code>ExternalLabel</code>\uff0c\u5305\u62ec <code>cluster</code>\u3001<code>cluster_name</code>\u3001<code>origin_prometheus</code> </li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#_2","title":"\u6e05\u7406\u73af\u5883","text":"<ul> <li>\u6309\u7167\u4e0b\u9762\u6b65\u9aa4\u6e05\u7406\u96c6\u7fa4\u76f8\u5173\u5185\u5bb9 <pre><code>cd ${LAB_HOME}\ncd eks-blueprints-clusters/multi-cluster-thanos\nfor i in ekscluster1 ekscluster2 ekscluster3 ; do\n    cd $i &amp;&amp; terraform destroy -auto-approve &amp;&amp; cd ..\ndone\ncd environment\nterraform destroy -auto-approve\n</code></pre></li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#_3","title":"\u7ed3\u8bba","text":"<p>Prometheus \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u76d1\u63a7\u548c\u62a5\u8b66\u5de5\u5177\uff0c\u4e13\u4e3a\u5bb9\u5668\u5316\u548c\u4e91\u539f\u751f\u67b6\u6784\u7684\u8bbe\u8ba1\u3002\u5ba2\u6237\u666e\u904d\u91c7\u7528\u5176\u7528\u4e8e Kubernetes \u7684\u76d1\u63a7\u4f53\u7cfb\u5efa\u8bbe\u3002Thanos\u662f\u4e00\u5957\u5f00\u6e90\u7ec4\u4ef6\uff0c\u6784\u5efa\u5728 Prometheus \u4e4b\u4e0a\uff0c\u7528\u4ee5\u89e3\u51b3 Prometheus \u5728\u591a\u96c6\u7fa4\u5927\u89c4\u6a21\u73af\u5883\u4e0b\u7684\u9ad8\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002\u5f53\u9700\u8981\u505a\u5386\u53f2\u6027\u80fd\u6570\u636e\u5206\u6790\uff0c\u6216\u8005\u4f7f\u7528 Prometheus \u8fdb\u884c\u6210\u672c\u5206\u6790\u7684\u573a\u666f\u90fd\u4f1a\u4f9d\u8d56\u4e8e\u8f83\u957f\u65f6\u95f4\u7684\u5386\u53f2\u6570\u636e\u3002Thanos \u4e3b\u8981\u901a\u8fc7\u63a5\u6536\u5e76\u5b58\u50a8 Prometheus \u7684\u591a\u96c6\u7fa4\u6570\u636e\u526f\u672c\uff0c\u5e76\u63d0\u4f9b\u5168\u5c40\u67e5\u8be2\u548c\u4e00\u81f4\u6027\u6570\u636e\u8bbf\u95ee\u63a5\u53e3\u7684\u65b9\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e8e Prometheus \u7684\u53ef\u9760\u6027\u3001\u4e00\u81f4\u6027\u548c\u53ef\u7528\u6027\u4fdd\u969c\uff0c\u4ece\u800c\u89e3\u51b3\u4e86 Prometheus \u5355\u96c6\u7fa4\u5728\u5b58\u50a8\u3001\u67e5\u8be2\u5386\u53f2\u6570\u636e\u548c\u5907\u4efd\u7b49\u65b9\u9762\u7684\u6269\u5c55\u6027\u6311\u6218\u3002</p>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#_4","title":"\u53c2\u8003\u94fe\u63a5","text":"<ul> <li>https://observability.thomasriley.co.uk/prometheus/using-thanos/high-availability/</li> <li>https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/high-availability.md</li> <li>https://medium.com/@kakashiliu/deploy-prometheus-operator-with-thanos-60210eff172b</li> <li>https://particule.io/en/blog/thanos-monitoring/</li> <li>https://blog.csdn.net/kingu_crimson/article/details/123840099</li> <li>thanos </li> <li>prometheus</li> <li>prometheus</li> <li>https://github.com/terraform-aws-modules/terraform-aws-eks/issues/2009</li> <li>prometheus-agent</li> <li>https://p8s.io/docs/operator/install/</li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos/#_5","title":"\u6587\u6863\u7248\u672c","text":"<ul> <li>POC-prometheus-ha-architect-with-thanos-manually</li> </ul>","tags":["kubernetes","aws/container/eks","prometheus"]},{"location":"EKS/solutions/monitor/eks-container-insights/","title":"EKS Container Insights","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","aws/mgmt/cloudwatch"]},{"location":"EKS/solutions/monitor/eks-container-insights/#eks-container-insights","title":"EKS Container Insights","text":"","tags":["aws/container/eks","aws/mgmt/cloudwatch"]},{"location":"EKS/solutions/monitor/eks-container-insights/#install","title":"install","text":"","tags":["aws/container/eks","aws/mgmt/cloudwatch"]},{"location":"EKS/solutions/monitor/eks-container-insights/#using-addons","title":"using addons","text":"<ul> <li>https://docs.amazonaws.cn/en_us/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-addon.html</li> <li>it consists: <ul> <li>../../addons/aws-for-fluent-bit</li> <li>cloudwatch agent</li> </ul> </li> </ul>","tags":["aws/container/eks","aws/mgmt/cloudwatch"]},{"location":"EKS/solutions/monitor/eks-container-insights/#from-cli","title":"from CLI","text":"<ol> <li>replace 2 service accounts with CloudWatchAgentServerPolicy <pre><code>CLUSTER_NAME=ekscluster1\nexport AWS_DEFAULT_REGION=us-east-2\neksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} --approve\n</code></pre></li> </ol> <pre><code>eksctl create iamserviceaccount \\\n    --name cloudwatch-agent \\\n    --namespace amazon-cloudwatch \\\n    --cluster ${CLUSTER_NAME} \\\n    --attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \\\n    --approve \\\n    --override-existing-serviceaccounts\n\neksctl create iamserviceaccount \\\n    --name fluent-bit \\\n    --namespace amazon-cloudwatch \\\n    --cluster ${CLUSTER_NAME} \\\n    --attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \\\n    --approve \\\n    --override-existing-serviceaccounts\n</code></pre> <ol> <li>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-quickstart.html <pre><code>FluentBitHttpPort='2020'\nFluentBitReadFromHead='On'\n[[ ${FluentBitReadFromHead} = 'On' ]] &amp;&amp; FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On'\n[[ -z ${FluentBitHttpPort} ]] &amp;&amp; FluentBitHttpServer='Off' || FluentBitHttpServer='On'\n\noutput=cwqs-1.yaml\ncurl -o $output https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml \n\n# no more this line from 230819\n# sed -i 's;amazon/cloudwatch-agent;public.ecr.aws/cloudwatch-agent/cloudwatch-agent;' $output\n\n#sed -i 's;amazon/aws-for-fluent-bit:2.10.0;public.ecr.aws/aws-observability/aws-for-fluent-bit:2.28.0;' $output\n\ncat $output | sed 's/{{cluster_name}}/'${CLUSTER_NAME}'/;s/{{region_name}}/'${AWS_DEFAULT_REGION}'/;s/{{http_server_toggle}}/\"'${FluentBitHttpServer}'\"/;s/{{http_server_port}}/\"'${FluentBitHttpPort}'\"/;s/{{read_from_head}}/\"'${FluentBitReadFromHead}'\"/;s/{{read_from_tail}}/\"'${FluentBitReadFromTail}'\"/' | kubectl apply -f - \n\nk get po -n amazon-cloudwatch\n</code></pre></li> </ol> <p>if you do 2 before 1, than need</p> <ul> <li>delete pods which use these service account</li> <li>check cloudtrail for \u201cAccessDeny\u201d events</li> </ul>","tags":["aws/container/eks","aws/mgmt/cloudwatch"]},{"location":"EKS/solutions/monitor/eks-container-insights/#check-pod-deployment-log","title":"check pod / deployment log","text":"<ul> <li>https://www.eksworkshop.com/intermediate/250_cloudwatch_container_insights/viewlogs/</li> </ul>","tags":["aws/container/eks","aws/mgmt/cloudwatch"]},{"location":"EKS/solutions/monitor/eks-container-insights/#blog","title":"blog","text":"<ul> <li>https://aws.amazon.com/blogs/containers/diving-into-container-insights-cost-optimizations-for-amazon-eks/</li> </ul>","tags":["aws/container/eks","aws/mgmt/cloudwatch"]},{"location":"EKS/solutions/monitor/enable-prometheus-in-cloudwatch/","title":"enable-prometheus-in-cloudwatch","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","aws/mgmt/cloudwatch","prometheus"]},{"location":"EKS/solutions/monitor/enable-prometheus-in-cloudwatch/#enable-prometheus-in-cloudwatch","title":"enable-prometheus-in-cloudwatch","text":"","tags":["aws/container/eks","aws/mgmt/cloudwatch","prometheus"]},{"location":"EKS/solutions/monitor/enable-prometheus-in-cloudwatch/#enable","title":"enable","text":"<pre><code>CLUSTER_NAME=ekscluster1\neksctl create iamserviceaccount \\\n  --cluster ${CLUSTER_NAME} \\\n  --namespace amazon-cloudwatch \\\n  --name cwagent-prometheus \\\n  --attach-policy-arn  arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \\\n  --override-existing-serviceaccounts \\\n  --approve\n\noutput=prom.yaml\ncurl -o $output 'https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/service/cwagent-prometheus/prometheus-eks.yaml'\nsed -i 's;amazon/cloudwatch-agent;public.ecr.aws/cloudwatch-agent/cloudwatch-agent;' $output\nkubectl apply -f $output\n</code></pre>","tags":["aws/container/eks","aws/mgmt/cloudwatch","prometheus"]},{"location":"EKS/solutions/monitor/enable-prometheus-in-cloudwatch/#reference","title":"reference","text":"<ul> <li>https://www.eksworkshop.com/advanced/330_servicemesh_using_appmesh/add_nodegroup_fargate/cloudwatch_setup/#enable-prometheus-metrics-in-cloudwatch</li> </ul>","tags":["aws/container/eks","aws/mgmt/cloudwatch","prometheus"]},{"location":"EKS/solutions/monitor/install-grafana-on-beanstalk/","title":"Install Grafana on Beanstalk","text":"<p>[!WARNING] This is a github note</p>","tags":["grafana"]},{"location":"EKS/solutions/monitor/install-grafana-on-beanstalk/#install-grafana-on-beanstalk","title":"Install Grafana on Beanstalk","text":"","tags":["grafana"]},{"location":"EKS/solutions/monitor/install-grafana-on-beanstalk/#summary","title":"summary","text":"<p>Grafana \u4f5c\u4e3a\u5f00\u6e90\u8f6f\u4ef6\u53ef\u4ee5\u81ea\u7531\u90e8\u7f72\uff0c1\uff09\u5728 ec2 \u4e2d\u624b\u52a8\u90e8\u7f72\uff0c\u9664\u4e86 OS \u548c\u8f6f\u4ef6\u5c42\u9762\u914d\u7f6e\u4e4b\u5916\uff0c\u4e0d\u53ef\u907f\u514d\u9700\u8981\u4e00\u4e9b\u989d\u5916\u7684\u914d\u7f6e\u5de5\u4f5c\uff0c\u5305\u62ec vpc \u3001 elb \u5b89\u5168\u7ec4\u7b49\uff1b2\uff09\u5f53\u7136\u4e5f\u6709\u4e9b\u89e3\u51b3\u65b9\u6848\u5c06 Grafana \u90e8\u7f72\u5728 eks \u96c6\u7fa4\u4e0a\u4ee5\u89e3\u51b3\u9ad8\u53ef\u7528\u6027\u95ee\u9898\uff0c\u4f46\u5982\u679c\u53ea\u662f\u5e0c\u671b\u7b80\u5355\u4f7f\u7528\u53ef\u80fd\u4f1a\u6709\u70b9\u6740\u9e21\u7528\u725b\u5200\u7684\u611f\u89c9\uff1b3\uff09\u672c\u6587\u63cf\u8ff0\u4e86\u7528 aws elastic beanstalk \u63d0\u4f9b\u4e00\u7ad9\u5f0f\u5feb\u901f\u90e8\u7f72 Grafana\uff0c\u5e76\u4e14\u4f7f\u7528 efs \u5b9e\u73b0 grafana\u5185\u90e8\u914d\u7f6e\u7684\u6301\u4e45\u5316</p>","tags":["grafana"]},{"location":"EKS/solutions/monitor/install-grafana-on-beanstalk/#grafana-container-on-beanstalk","title":"grafana container on beanstalk","text":"<ul> <li>need efs for storage persistent <pre><code>CLUSTER_NAME=efs0225\nAWS_REGION=us-east-2\n</code></pre></li> </ul> <p>refer: git/git-mkdocs/CLI/awscli/efs-cmd</p> <p>mount nfs to instance (link)</p> <pre><code>echo ${FILESYSTEM_ID}\n</code></pre> <ul> <li> <p>install beanstalk (link) <pre><code>git clone https://github.com/aws/aws-elastic-beanstalk-cli-setup.git\npython ./aws-elastic-beanstalk-cli-setup/scripts/ebcli_installer.py\n# export env variable as instructions\n</code></pre></p> </li> <li> <p>deploy with prebuild image <pre><code>mkdir example-prebuild\ncd example-prebuild\ncat &gt;docker-compose.yml &lt;&lt;-EOF\nversion: \"3.8\"\n\nservices:\n  grafana:\n    image: grafana/grafana-oss\n    volumes:\n\n      - \"/efsmnt/grafana/var-lib-grafana:/var/lib/grafana\"\n      - \"/efsmnt/grafana/var-log-grafana:/var/log/grafana\"\n    ports:\n      - \"80:3000\"\n    restart: always\nEOF\n\nmkdir .ebextensions\n</code></pre> if you just use one pod, reference appendix</p> </li> <li> <p>customize config <code>.ebextensions/efs-mount.config</code> <pre><code>wget -O /tmp/efs-mount.config 'https://github.com/awsdocs/elastic-beanstalk-samples/raw/main/configuration-files/aws-provided/instance-configuration/storage-efs-mountfilesystem.config'\n# regexp different between egrep &amp; sed\n# using '\\s*' to instead will be better\ncat /tmp/efs-mount.config |egrep '^\\s+FILE_SYSTEM_ID: '\nsed -i '/^\\s\\+FILE_SYSTEM_ID: /s/:.*$/: '\"${FILESYSTEM_ID}\"'/' /tmp/efs-mount.config\ncp /tmp/efs-mount.config .ebextensions/\n</code></pre> another sample (link)</p> </li> <li> <p>prep elastic beanstalk <pre><code>APP_NAME=myapp$(date +%m%d)\nENV_NAME=${APP_NAME}-env$(date +%H%M)\neb init -p docker ${APP_NAME} --region ${AWS_REGION}\neb local run --port 3000\neb create ${ENV_NAME}\n\n## after modify config\n## create a new app version\neb appversion -c\n\n## deploy using new appver\neb create --version app-230224_123456789012 app2-env-3\n\n## swap ?\neb swap\n</code></pre></p> </li> <li> <p>others parameters <pre><code>cfg:default.paths.data=/var/lib/grafana cfg:default.paths.logs=/var/log/grafana cfg:default.paths.plugins=/var/lib/grafana/plugins cfg:default.paths.provisioning=/etc/grafana/provisioning\n</code></pre></p> </li> </ul>","tags":["grafana"]},{"location":"EKS/solutions/monitor/install-grafana-on-beanstalk/#grafana-in-ec2","title":"grafana in ec2","text":"<ul> <li> <p>install  <pre><code>cat &lt;&lt;-EOF |sudo tee /etc/yum.repos.d/grafana.repo\n[grafana]\nname=grafana\nbaseurl=https://rpm.grafana.com\nrepo_gpgcheck=1\nenabled=1\ngpgcheck=1\ngpgkey=https://rpm.grafana.com/gpg.key\nsslverify=1\nsslcacert=/etc/pki/tls/certs/ca-bundle.crt\nexclude=*beta*\nEOF\nsudo yum install -y grafana\nsudo systemctl status grafana-server\n</code></pre></p> </li> <li> <p>prep elb</p> <ul> <li>alb + http3000 port </li> </ul> </li> </ul>","tags":["grafana"]},{"location":"EKS/solutions/monitor/install-grafana-on-beanstalk/#grafana-on-eks","title":"grafana on eks","text":"<ul> <li>https://aws-quickstart.github.io/quickstart-eks-grafana/</li> </ul>","tags":["grafana"]},{"location":"EKS/solutions/monitor/install-grafana-on-beanstalk/#helm","title":"helm","text":"<pre><code>helm repo add grafana https://grafana.github.io/helm-charts\n</code></pre>","tags":["grafana"]},{"location":"EKS/solutions/monitor/install-grafana-on-beanstalk/#appendix-single-pod-in-beanstalk","title":"appendix - single pod in beanstalk","text":"<pre><code>cat &gt;Dockerrun.aws.json &lt;&lt;-EOF\n{\n  \"AWSEBDockerrunVersion\": \"1\",\n  \"Image\": {\n    \"Name\": \"grafana/grafana-oss\"\n  },\n  \"Ports\": [\n    {\n      \"ContainerPort\": 3000,\n      \"HostPort\": 3000\n    }\n  ],\n  \"Logging\": \"/usr/local/glassfish5/glassfish/domains/domain1/logs\"\n}\nEOF\n</code></pre>","tags":["grafana"]},{"location":"EKS/solutions/monitor/install-prometheus-grafana/","title":"install-prometheus-grafana-on-eks","text":"<p>[!WARNING] This is a github note</p>","tags":["grafana","prometheus","aws/container/eks"]},{"location":"EKS/solutions/monitor/install-prometheus-grafana/#install-prometheus-grafana","title":"install-prometheus-grafana","text":"","tags":["grafana","prometheus","aws/container/eks"]},{"location":"EKS/solutions/monitor/install-prometheus-grafana/#prep","title":"prep","text":"<ul> <li>../../addons/ebs-for-eks </li> </ul>","tags":["grafana","prometheus","aws/container/eks"]},{"location":"EKS/solutions/monitor/install-prometheus-grafana/#prefer-install-prometheus-operator","title":"(Prefer) install-prometheus-operator","text":"<ul> <li>https://blog.devgenius.io/step-by-step-guide-to-setting-up-prometheus-operator-in-your-kubernetes-cluster-7167a8228877</li> </ul> <pre><code>DEPLOY_NAME=prom-operator-run-abc\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\nkubectl create namespace monitoring\nhelm install ${DEPLOY_NAME} prometheus-community/kube-prometheus-stack --namespace monitoring\n</code></pre>","tags":["grafana","prometheus","aws/container/eks"]},{"location":"EKS/solutions/monitor/install-prometheus-grafana/#forward-to-local","title":"forward to local","text":"<ul> <li> <p>port forward to cloud9 <pre><code>k get svc/${DEPLOY_NAME}-grafana\nk port-forward svc/${DEPLOY_NAME}-grafana 3000:80 --address='0.0.0.0'\n</code></pre></p> </li> <li> <p>access from your laptop <pre><code># you need AKSK environment variables\nINST_ID=&lt;cloud9_inst_id&gt;\naws ssm start-session --target ${INST_ID} --document-name AWS-StartPortForwardingSession --parameters '{\"localPortNumber\":[\"3000\"],\"portNumber\":[\"3000\"]}'\n\n###\n# access local 3000 with \n# admin / prom-operator\n###\n</code></pre></p> </li> </ul>","tags":["grafana","prometheus","aws/container/eks"]},{"location":"EKS/solutions/monitor/install-prometheus-grafana/#install-standalone-prometheus","title":"install standalone prometheus","text":"<pre><code>CLUSTER_NAME=ekscluster4\nDEPLOY_NAME=prom-operator-${CLUSTER_NAME}\nNAMESPACE_NAME=monitoring\n\n# enable grafana and typical prometheus\nenvsubst &gt;values-${CLUSTER_NAME}-1.yaml &lt;&lt;-EOF\ngrafana:\n  enabled: false\nprometheus:\n  prometheusSpec:\n    additionalArgs: \n\n    - name: storage.tsdb.min-block-duration\n      value: 5m\n    - name: storage.tsdb.max-block-duration\n      value: 5m\n    replicas: 2\n    retention: 730h # one month\n    retentionSize: \"100GB\"\n    ruleSelectorNilUsesHelmValues: false\n    serviceMonitorSelectorNilUsesHelmValues: false\n    podMonitorSelectorNilUsesHelmValues: false\n    topologySpreadConstraints: \n    - maxSkew: 1\n      topologyKey: topology.kubernetes.io/zone\n      whenUnsatisfiable: DoNotSchedule\n      labelSelector:\n        matchLabels:\n          app: prometheus\n    # additionalScrapeConfigsSecret: \n    #   enabled: true\n    #   name: additional-scrape-configs\n    #   key: avalanche-additional.yaml\n    storageSpec: \n      volumeClaimTemplate:\n        spec:\n          storageClassName: ${STORAGECLASS_NAME}\n          accessModes: [\"ReadWriteOnce\"]\n          resources:\n            requests:\n              storage: 200Gi\n        selector: {}\n    remoteWrite: \n    - url: http://k8s-thanos-thanosre-xxx.elb.us-west-2.amazonaws.com:19291/api/v1/receive\n    externalLabels: \n      cluster: \"${CLUSTER_NAME}\"\n      cluster_name: \"${CLUSTER_NAME}\"\n      origin_prometheus: \"${CLUSTER_NAME}\"\nEOF\n\nhelm upgrade -i -f values-${CLUSTER_NAME}-1.yaml ${DEPLOY_NAME} prometheus-community/kube-prometheus-stack --namespace ${NAMESPACE_NAME} --create-namespace\n</code></pre> <ul> <li>https://github.com/prometheus-operator/prometheus-operator/issues/2918 External labels are only attached when data is communicated to the outside so you will see them in:</li> <li>Outgoing alerts (although we do automatically drop the \u201cprometheus_replica\u201d label, so alerts are unique and can be deduplicated by Alertmanager)</li> <li>Remote-read endpoint</li> <li>Remote-write</li> <li>/federate endpoint</li> </ul>","tags":["grafana","prometheus","aws/container/eks"]},{"location":"EKS/solutions/monitor/install-prometheus-grafana/#install-with-thanos","title":"install with thanos","text":"<ul> <li>refer: TC-prometheus-ha-architect-with-thanos</li> </ul>","tags":["grafana","prometheus","aws/container/eks"]},{"location":"EKS/solutions/monitor/install-prometheus-grafana/#optional-install-prometheus-and-grafana","title":"(Optional) install prometheus and grafana","text":"<ul> <li> <p>install prometheus <pre><code>kubectl create namespace prometheus\n\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n\nhelm install prometheus prometheus-community/prometheus \\\n    --namespace prometheus \\\n    --set alertmanager.persistentVolume.storageClass=\"gp2\" \\\n    --set server.persistentVolume.storageClass=\"gp2\"\n\nkubectl get all -n prometheus\n</code></pre></p> </li> <li> <p>install grafana <pre><code>cat &gt; ./grafana.yaml &lt;&lt;-EOF\ndatasources:\n  datasources.yaml:\n    apiVersion: 1\n    datasources:\n    - name: Prometheus\n      type: prometheus\n      url: http://prometheus-server.prometheus.svc.cluster.local\n      access: proxy\n      isDefault: true\nEOF\n\nhelm repo add grafana https://grafana.github.io/helm-charts\nkubectl create namespace grafana\nhelm install grafana grafana/grafana \\\n    --namespace grafana \\\n    --set persistence.storageClassName=\"gp2\" \\\n    --set persistence.enabled=true \\\n    --set adminPassword='EKS!sAWSome' \\\n    --values ./grafana.yaml \\\n    --set service.type=LoadBalancer\nkubectl get all -n grafana\n</code></pre></p> </li> <li> <p>dashboard</p> <ul> <li>cluster monitoring dashboard: 3119</li> <li>pod monitoring dashboard: 6417</li> <li>some other dashboard in grafana</li> </ul> </li> </ul>","tags":["grafana","prometheus","aws/container/eks"]},{"location":"EKS/solutions/monitor/install-prometheus-grafana/#refer","title":"refer","text":"<ul> <li>https://archive.eksworkshop.com/intermediate/240_monitoring/prereqs/</li> <li>External labels are only attached when data is communicated to the outside, including Outgoing alerts, remote write, remote read endpoint, <code>/federate</code> endpoint, etc.<ul> <li>https://github.com/prometheus-operator/prometheus-operator/issues/2918</li> </ul> </li> </ul>","tags":["grafana","prometheus","aws/container/eks"]},{"location":"cloud9/create-standard-vpc-for-lab-in-china-region/","title":"create standard vpc for lab in china region","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/network/vpc","aws/mgmt/cloudformation"]},{"location":"cloud9/create-standard-vpc-for-lab-in-china-region/#create-standard-vpc-for-lab-in-china-region","title":"create standard vpc for lab in china region","text":"","tags":["aws/network/vpc","aws/mgmt/cloudformation"]},{"location":"cloud9/create-standard-vpc-for-lab-in-china-region/#using-cloudformation-template-","title":"using-cloudformation-template-","text":"<ul> <li>search <code>cloud9</code> in marketplace, and launch instance from it</li> <li>assign <code>AdministratorAccess</code> to instance profile</li> </ul> <pre><code>AWS_REGION=cn-north-1\nexport AWS_DEFAULT_REGION=${AWS_REGION}\nUNIQ_STR=$(date +%Y%m%d-%H%M%S)\nBUCKET_NAME=$(aws s3 mb s3://panlm-${UNIQ_STR} |awk '{print $2}')\n\nwget -O aws-vpc.template.yaml https://github.com/panlm/panlm.github.io/raw/main/content/100-eks-infra/110-eks-cluster/aws-vpc.template.yaml\naws s3 cp aws-vpc.template.yaml s3://${BUCKET_NAME}/\n\n# first 2 AZs\n# separator `\\,` is necessary for ParameterValue in cloudformation\nTWOAZS=($(aws ec2 describe-availability-zones --query 'AvailabilityZones[].ZoneName' --output text |xargs -n 1 |sed -n '1,2p' |xargs |sed 's/ /\\\\,/g'))\n</code></pre> <p>(option 1) if you create vpc in china region, you could put your existed tgw id here for attach automatically. <pre><code># new vpc will connect with TGW, if TGW existed\nTGW_ID=tgw-0ec1b74b7d8dcea74\nTGW_NUMBER=$(aws ec2 describe-transit-gateways \\\n--filter Name=transit-gateway-id,Values=${TGW_ID} \\\n|jq -r '.TransitGateways | length')\nif [[ ${TGW_NUMBER} -eq 1 ]]; then\n  TGW_ATTACH=true\nelse\n  TGW_ATTACH=false\nfi\n# do not create public subnet &amp; igw\nCREATE_PUB_SUB=false\n</code></pre></p> <p>(option 2) you could create vpc without tgw <pre><code>TGW_ATTACH=false\nCREATE_PUB_SUB=true\n</code></pre></p> <p>create your vpc with specific CIDR <pre><code>echo ${CIDR:=10.129}\nexport AWS_PAGER=\"\"\n\nSTACK_NAME=aws-vpc-${CIDR##*.}-${UNIQ_STR}\n# global region: amazonaws.com\n# china region: amazonaws.com.cn\nif [[ ${AWS_REGION%%-*} == \"cn\" ]]; then\n  SUFFIX=\".cn\"\nelse\n  SUFFIX=\"\"\nfi\naws cloudformation create-stack --stack-name ${STACK_NAME} \\\n  --parameters ParameterKey=AvailabilityZones,ParameterValue=\"${TWOAZS}\" \\\n  ParameterKey=VPCCIDR,ParameterValue=\"${CIDR}.0.0/16\" \\\n  ParameterKey=NumberOfAZs,ParameterValue=2 \\\n  ParameterKey=PublicSubnet1CIDR,ParameterValue=\"${CIDR}.128.0/24\" \\\n  ParameterKey=PublicSubnet2CIDR,ParameterValue=\"${CIDR}.129.0/24\" \\\n  ParameterKey=PublicSubnet3CIDR,ParameterValue=\"${CIDR}.130.0/24\" \\\n  ParameterKey=PublicSubnet4CIDR,ParameterValue=\"${CIDR}.131.0/24\" \\\n  ParameterKey=PrivateSubnet1ACIDR,ParameterValue=\"${CIDR}.0.0/19\" \\\n  ParameterKey=PrivateSubnet2ACIDR,ParameterValue=\"${CIDR}.32.0/19\" \\\n  ParameterKey=PrivateSubnet3ACIDR,ParameterValue=\"${CIDR}.64.0/19\" \\\n  ParameterKey=PrivateSubnet4ACIDR,ParameterValue=\"${CIDR}.96.0/19\" \\\n  ParameterKey=CreateTgwSubnets,ParameterValue=\"true\" \\\n  ParameterKey=TgwSubnet1CIDR,ParameterValue=\"${CIDR}.132.0/24\" \\\n  ParameterKey=TgwSubnet2CIDR,ParameterValue=\"${CIDR}.133.0/24\" \\\n  ParameterKey=TgwSubnet3CIDR,ParameterValue=\"${CIDR}.134.0/24\" \\\n  ParameterKey=TgwSubnet4CIDR,ParameterValue=\"${CIDR}.135.0/24\" \\\n  ParameterKey=CreateTgwAttachment,ParameterValue=\"${TGW_ATTACH}\" \\\n  ParameterKey=TransitGatewayId,ParameterValue=\"${TGW_ID}\" \\\n  ParameterKey=CreatePublicSubnets,ParameterValue=\"${CREATE_PUB_SUB}\" \\\n  ParameterKey=CreatePrivateSubnets,ParameterValue=\"true\" \\\n  ParameterKey=CreateNATGateways,ParameterValue=\"false\" \\\n  --template-url https://${BUCKET_NAME}.s3.${AWS_REGION}.amazonaws.com${SUFFIX}/aws-vpc.template.yaml \\\n  --region ${AWS_REGION}\n\n# until get CREATE_COMPLETE\nwhile true ; do\n  status=$(aws cloudformation --region ${AWS_REGION} describe-stacks --stack-name ${STACK_NAME} --query 'Stacks[0].StackStatus' --output text)\n  echo ${status}\n  if [[ ${status} == 'CREATE_IN_PROGRESS' ]]; then\n    sleep 30\n  else\n    break\n  fi\ndone\n</code></pre></p>","tags":["aws/network/vpc","aws/mgmt/cloudformation"]},{"location":"cloud9/create-standard-vpc-for-lab-in-china-region/#get-vpc-id-","title":"get-vpc-id-","text":"<pre><code>VPC_ID=$(aws cloudformation --region ${AWS_REGION} describe-stacks --stack-name ${STACK_NAME} --query 'Stacks[0].Outputs[?OutputKey==`VPCID`].OutputValue' --output text)\n\n# PublicSubnet1ID=$(aws cloudformation --region ${AWS_REGION} describe-stacks --stack-name ${STACK_NAME} --query 'Stacks[0].Outputs[?OutputKey==`PublicSubnet1ID`].OutputValue' --output text)\n</code></pre>","tags":["aws/network/vpc","aws/mgmt/cloudformation"]},{"location":"cloud9/create-standard-vpc-for-lab-in-china-region/#option-create-cloud9-in-target-subnet","title":"(option) create cloud9 in target subnet","text":"<ul> <li>refer: setup-cloud9-for-eks <pre><code># name=&lt;give your cloud9 a name&gt;\ndatestring=$(date +%Y%m%d-%H%M)\nname=${name:=cloud9-$datestring}\nexport AWS_DEFAULT_REGION=us-east-2 # need put each command\n\n# VPC_ID=&lt;your vpc id&gt; \n# ensure you have public subnet in it\nDEFAULT_VPC_ID=$(aws ec2 describe-vpcs \\\n  --filter Name=is-default,Values=true \\\n  --query 'Vpcs[0].VpcId' --output text \\\n  --region ${AWS_DEFAULT_REGION})\nVPC_ID=${VPC_ID:=$DEFAULT_VPC_ID}\n\nif [[ ! -z ${VPC_ID} ]]; then\n  FIRST_SUBNET=$(aws ec2 describe-subnets \\\n    --filters \"Name=vpc-id,Values=${VPC_ID}\" \\\n    --query 'Subnets[?(AvailabilityZone==`'\"${AWS_DEFAULT_REGION}a\"'` &amp;&amp; MapPublicIpOnLaunch==`true`)].SubnetId' \\\n    --output text \\\n    --region ${AWS_DEFAULT_REGION})\n  aws cloud9 create-environment-ec2 \\\n    --name ${name} \\\n    --image-id amazonlinux-2-x86_64 \\\n    --instance-type m5.xlarge \\\n    --subnet-id ${FIRST_SUBNET} \\\n    --automatic-stop-time-minutes 10080 \\\n    --region ${AWS_DEFAULT_REGION} |tee /tmp/$$\n  echo \"Open URL to access your Cloud9 Environment:\"\n  C9_ID=$(cat /tmp/$$ |jq -r '.environmentId')\n  echo \"https://${AWS_DEFAULT_REGION}.console.aws.amazon.com/cloud9/ide/${C9_ID}\"\nelse\n  echo \"you have no default vpc in $AWS_DEFAULT_REGION\"\nfi\n</code></pre></li> </ul>","tags":["aws/network/vpc","aws/mgmt/cloudformation"]},{"location":"cloud9/create-standard-vpc-for-lab-in-china-region/#description-in-this-template","title":"description in this template","text":"<ul> <li>no s3 endpoint</li> <li>security group named eks-shared-sg (only it self)</li> <li>security group named normal-sg ( icmp/80/443 for all )</li> <li>tag subnet <ul> <li><code>kubernetes.io/role/internal-elb</code> = <code>1</code></li> <li><code>kubernetes.io/role/elb</code> = <code>1</code></li> <li>(option) <code>kubernetes.io/cluster/&lt;vpc_name&gt;</code> = <code>shared</code></li> </ul> </li> <li>verified in china region</li> <li>add tgw subnet and associate tgw route table with <code>0.0.0.0/0</code> to tgw</li> <li>add <code>10.0.0.0/8</code> route to public/private1A/private2A route table</li> </ul>","tags":["aws/network/vpc","aws/mgmt/cloudformation"]},{"location":"cloud9/create-standard-vpc-for-lab-in-china-region/#refer","title":"refer","text":"<ul> <li>../CLI/awscli/cloudformation-cmd </li> <li>quickstart-aws-vpc </li> </ul>","tags":["aws/network/vpc","aws/mgmt/cloudformation"]},{"location":"cloud9/quick-setup-cloud9/","title":"quick setup cloud9 script","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/cloud9","aws/container/eks"]},{"location":"cloud9/quick-setup-cloud9/#quick-setup-cloud9","title":"Quick Setup Cloud9","text":"<p>\u5728 setup-cloud9-for-eks \u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u7b80\u5316\u64cd\u4f5c\uff0c\u5728 cloud9 \u4e2d\u5373\u5b8c\u6210\u6240\u6709\u521d\u59cb\u5316\u52a8\u4f5c\u3002</p>","tags":["aws/cloud9","aws/container/eks"]},{"location":"cloud9/quick-setup-cloud9/#spin-up-a-cloud9-instance-in-your-region","title":"spin-up-a-cloud9-instance-in-your-region","text":"<ul> <li>\u70b9\u51fb\u8fd9\u91cc \u8fd0\u884c cloudshell\uff0c\u6267\u884c\u4ee3\u7801\u5757\u521b\u5efa cloud9 \u6d4b\u8bd5\u73af\u5883 (open cloudshell, and then execute following code to create cloud9 environment)<ul> <li>\u901a\u8fc7 <code>name</code> \u81ea\u5b9a\u4e49 cloud9 \u7684\u540d\u79f0\uff0c\u5982\u679c\u4e0d\u6307\u5b9a\u5c06\u81ea\u52a8\u521b\u5efa</li> <li>cloud9 \u5c06\u521b\u5efa\u5728\u9ed8\u8ba4 vpc \u4e2d\u7b2c\u4e00\u4e2a\u516c\u6709\u5b50\u7f51\u4e2d</li> <li>\u7b49\u5f85\u5b9e\u4f8b\u521b\u5efa\u5b8c\u6210\u5e76\u83b7\u53d6\u5230 instance_id</li> </ul> </li> <li>ensure aws region is correct and walkthrough <pre><code>aws configure list\nexport AWS_DEFAULT_REGION AWS_REGION\n\n# name=&lt;give your cloud9 a name&gt;\ndatestring=$(TZ=CST-8 date +%Y%m%d-%H%M)\necho ${name:=cloud9-$datestring}\n\n# VPC_ID=&lt;your vpc id&gt; \n# ensure you have public subnet in it\nDEFAULT_VPC_ID=$(aws ec2 describe-vpcs \\\n    --filter Name=is-default,Values=true \\\n    --query 'Vpcs[0].VpcId' --output text \\\n    --region ${AWS_DEFAULT_REGION})\nVPC_ID=${VPC_ID:=$DEFAULT_VPC_ID}\n\nif [[ ! -z ${VPC_ID} ]]; then\n    FIRST_SUBNET=$(aws ec2 describe-subnets \\\n        --filters \"Name=vpc-id,Values=${VPC_ID}\" \\\n        --query 'Subnets[?(MapPublicIpOnLaunch==`true`)].SubnetId' \\\n        --output text \\\n        --region ${AWS_DEFAULT_REGION} |\\\n        xargs -n 1 |tail -n 1)\n    aws cloud9 create-environment-ec2 \\\n        --name ${name} \\\n        --image-id amazonlinux-2-x86_64 \\\n        --instance-type m5.large \\\n        --subnet-id ${FIRST_SUBNET%% *} \\\n        --automatic-stop-time-minutes 10080 \\\n        --region ${AWS_DEFAULT_REGION} |tee /tmp/$$\n    echo \"Open URL to access your Cloud9 Environment:\"\n    C9_ID=$(cat /tmp/$$ |jq -r '.environmentId')\n    echo \"https://${AWS_DEFAULT_REGION}.console.aws.amazon.com/cloud9/ide/${C9_ID}\"\nelse\n    echo \"you have no default vpc in ${AWS_DEFAULT_REGION}\"\nfi\n\n# wait instance could be see from ec2 :D\nwatch -g -n 2 aws ec2 describe-instances \\\n    --filters \"Name=tag:Name,Values=aws-cloud9-${name}-${C9_ID}\" \\\n    --query \"Reservations[].Instances[].InstanceId\" --output text\n</code></pre></li> </ul>","tags":["aws/cloud9","aws/container/eks"]},{"location":"cloud9/quick-setup-cloud9/#prefer-stay-in-cloudshell-to-initiate-cloud9","title":"(prefer) stay in cloudshell to initiate cloud9","text":"<ul> <li>\u4ee3\u7801\u5c06\u4ece GitHub \u4e0b\u8f7d\uff1a<ul> <li>https://github.com/panlm/panlm.github.io/raw/main/docs/cloud9/setup-cloud9-for-eks.md</li> </ul> </li> <li>\u4ee3\u7801\u5c06\u5b8c\u6210\uff1a<ul> <li>\u521b\u5efa\u89d2\u8272\u540d\u4e3a <code>ec2-admin-role-*</code>\uff0c\u6dfb\u52a0\u7ba1\u7406\u5458\u6743\u9650\uff0c\u4e14\u5141\u8bb8 4 \u4e2a\u5176\u4ed6\u89d2\u8272 assume</li> <li>\u5982\u679c cloud9 \u7684\u5b9e\u4f8b\u5df2\u7ecf\u6709\u5173\u8054\u7684 role\uff0c\u5219\u5c06 role \u6dfb\u52a0\u7ba1\u7406\u5458\u6743\u9650\uff0c\u5982\u679c\u6ca1\u6709\u5219\u8d4b\u4e88\u65b0\u5efa\u7684\u89d2\u8272</li> <li>\u5141\u8bb8 cloud9 \u7684\u5b9e\u4f8b\u88ab\u5176\u4ed6 2 \u4e2a\u89d2\u8272\u4f7f\u7528</li> <li>\u91cd\u542f cloud9\uff0c\u7b49\u5f85 cloud9 \u53ef\u4ee5\u88ab ssm \u8bbf\u95ee</li> <li>\u68c0\u67e5\u811a\u672c\u5b58\u5728\uff0c\u5e76\u4e14\u521b\u5efa ssm \u521d\u59cb\u5316\u811a\u672c</li> <li>\u521b\u5efa\u65e5\u5fd7\u7ec4\uff0c\u5e76\u4f7f\u7528 ssm \u6267\u884c\u521d\u59cb\u5316\u811a\u672c</li> <li>\u663e\u793a\u767b\u5f55 cloud9 \u7684 URL <pre><code>echo ${C9_ID}\necho ${name}\n\nexport AWS_PAGER=\"\"\nC9_INST_ID=$(aws ec2 describe-instances \\\n    --filters \"Name=tag:Name,Values=aws-cloud9-${name}-${C9_ID}\" \\\n    --query \"Reservations[].Instances[].InstanceId\" --output text)\nMY_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nROLE_NAME=ec2-admin-role-$(TZ=CST-8 date +%Y%m%d-%H%M%S)\n\nsudo yum install -y gettext\n\n# build trust.json\ncat &gt; ec2.json &lt;&lt;-EOF\n{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n    },\n    \"Action\": \"sts:AssumeRole\"\n}\nEOF\nSTATEMENT_LIST=ec2.json\n\nfor i in WSParticipantRole WSOpsRole TeamRole OpsRole ; do\n    aws iam get-role --role-name $i &gt;/dev/null 2&gt;&amp;1\n    if [[ $? -eq 0 ]]; then\n        envsubst &gt;$i.json &lt;&lt;-EOF\n{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"AWS\": \"arn:aws:iam::${MY_ACCOUNT_ID}:role/$i\"\n    },\n    \"Action\": \"sts:AssumeRole\"\n}\nEOF\n        STATEMENT_LIST=$(echo ${STATEMENT_LIST} \"$i.json\")\n    fi\ndone\n\njq -n '{Version: \"2012-10-17\", Statement: [inputs]}' ${STATEMENT_LIST} &gt; trust.json\necho ${STATEMENT_LIST}\nrm -f ${STATEMENT_LIST}\n\n# create role\naws iam create-role --role-name ${ROLE_NAME} \\\n    --assume-role-policy-document file://trust.json\naws iam attach-role-policy --role-name ${ROLE_NAME} \\\n    --policy-arn \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\naws iam attach-role-policy --role-name ${ROLE_NAME} \\\n    --policy-arn \"arn:aws:iam::aws:policy/AdministratorAccess\"\n\ninstance_profile_arn=$(aws ec2 describe-iam-instance-profile-associations \\\n    --filter Name=instance-id,Values=$C9_INST_ID \\\n    --query IamInstanceProfileAssociations[0].IamInstanceProfile.Arn \\\n    --output text)\nif [[ ${instance_profile_arn} == \"None\" ]]; then\n    # create one\n    aws iam create-instance-profile --instance-profile-name ${ROLE_NAME} |tee /tmp/inst-profile-$$.1\n    sleep 10\n    # attach role to it\n    aws iam add-role-to-instance-profile --instance-profile-name ${ROLE_NAME} --role-name ${ROLE_NAME}\n    sleep 10\n    # attach instance profile to ec2\n    aws ec2 associate-iam-instance-profile \\\n        --iam-instance-profile Name=${ROLE_NAME} \\\n        --instance-id ${C9_INST_ID}\nelse\n    existed_role_name=$(aws iam get-instance-profile \\\n        --instance-profile-name ${instance_profile_arn##*/} \\\n        --query 'InstanceProfile.Roles[0].RoleName' \\\n        --output text)\n    aws iam attach-role-policy --role-name ${existed_role_name} \\\n        --policy-arn \"arn:aws:iam::aws:policy/AdministratorAccess\"\nfi\n\n# share with other specific user\n# cannot use user-arn: arn:aws:iam::${MY_ACCOUNT_ID}:root to share everyone\n# cannot assign role to access cloud9, only root/user/assumed-role/federated-user\nfor i in WSOpsRole/Ops WSParticipantRole/Participant panlm/granted; do\n    aws cloud9 create-environment-membership \\\n        --environment-id ${C9_ID} \\\n        --user-arn arn:aws:sts::${MY_ACCOUNT_ID}:assumed-role/${i} \\\n        --permissions read-write\ndone\n\n# reboot instance, make role effective ASAP\naws ec2 reboot-instances --instance-ids ${C9_INST_ID}\n\n# wait ssm could connect to this instance \nwhile true ; do\n    sleep 60\n    CONN_STAT=$(aws ssm get-connection-status \\\n    --target ${C9_INST_ID} \\\n    --query \"Status\" --output text)\n    echo ${CONN_STAT}\n    if [[ ${CONN_STAT} == 'connected' ]]; then\n        break\n    fi\ndone\n\n# script source location:\n# https://github.com/panlm/panlm.github.io/raw/main/docs/cloud9/setup-cloud9-for-eks.md\n# check script existed or not\nRET_CODE=$(curl -sL -w '%{http_code}' -o /dev/null  https://github.com/panlm/panlm.github.io/raw/main/docs/cloud9/setup-cloud9-for-eks.md)\nif [[ ${RET_CODE} -ne 200 ]]; then\n    echo \"######\"\n    echo \"###### SCRIPT NOT EXISTED\"\n    echo \"######\"\nfi\n\ncat &gt;$$.json &lt;&lt;-'EOF'\n{\n  \"workingDirectory\": [\n    \"\"\n  ],\n  \"executionTimeout\": [\n    \"3600\"\n  ],\n  \"commands\": [\n    \"\",\n    \"TMPFILE=$(mktemp)\",\n    \"curl --location -o $TMPFILE https://github.com/panlm/panlm.github.io/raw/main/docs/cloud9/setup-cloud9-for-eks.md\",\n    \"cat $TMPFILE |awk '/###-SCRIPT-PART-ONE-BEGIN-###/,/###-SCRIPT-PART-ONE-END-###/ {print}' &gt; $TMPFILE-ONE.sh\",\n    \"chmod a+x $TMPFILE-ONE.sh\",\n    \"sudo -u ec2-user bash $TMPFILE-ONE.sh 2&gt;&amp;1\",\n    \"\",\n    \"cat $TMPFILE |awk '/###-SCRIPT-PART-TWO-BEGIN-###/,/###-SCRIPT-PART-TWO-END-###/ {print}' &gt; $TMPFILE-TWO.sh\",\n    \"chmod a+x $TMPFILE-TWO.sh\",\n    \"sudo -u ec2-user bash $TMPFILE-TWO.sh 2&gt;&amp;1\",\n    \"\"\n  ]\n}\nEOF\n\nLOGGROUP_NAME=ssm-runshellscript-log-$(TZ=CST-8 date +%Y%m%d-%H%M)\naws logs create-log-group \\\n    --log-group-name ${LOGGROUP_NAME}\n\naws ssm send-command \\\n    --document-name \"AWS-RunShellScript\" \\\n    --document-version \"1\" \\\n    --targets '[{\"Key\":\"InstanceIds\",\"Values\":[\"'${C9_INST_ID}'\"]}]' \\\n    --parameters file://$$.json \\\n    --timeout-seconds 600 \\\n    --max-concurrency \"50\" --max-errors \"0\"  \\\n    --cloud-watch-output-config CloudWatchLogGroupName=${LOGGROUP_NAME},CloudWatchOutputEnabled=true |tee ssm-$$.json\n# comment \"-a\" in tee 2023/11/20\n\n# wait to Success\nCOMMAND_ID=$(cat ssm-$$.json |jq -r '.Command.CommandId')\nwatch -g -n 10 aws ssm get-command-invocation --command-id ${COMMAND_ID} --instance-id ${C9_INST_ID} --query 'Status' --output text\n\n# disable managed credential and login cloud9\naws cloud9 update-environment  --environment-id $C9_ID --managed-credentials-action DISABLE\necho \"https://${AWS_DEFAULT_REGION}.console.aws.amazon.com/cloud9/ide/${C9_ID}\"\n</code></pre></li> </ul> </li> </ul>","tags":["aws/cloud9","aws/container/eks"]},{"location":"cloud9/quick-setup-cloud9/#alternative-login-cloud9-to-initiate","title":"(alternative) login cloud9 to initiate","text":"<ul> <li>\u4ee3\u7801\u5c06\u4ece GitHub \u4e0b\u8f7d\uff1a<ul> <li>https://github.com/panlm/panlm.github.io/raw/main/docs/cloud9/setup-cloud9-for-eks.md</li> </ul> </li> </ul>","tags":["aws/cloud9","aws/container/eks"]},{"location":"cloud9/quick-setup-cloud9/#script-part-one-two","title":"script-part-one-two","text":"<ul> <li>\u4e0b\u9762\u4ee3\u7801\u5757\u5305\u542b\u4e00\u4e9b\u57fa\u672c\u8bbe\u7f6e\uff0c\u5305\u62ec\uff1a(execute this code block to install tools for your lab, and resize ebs of cloud9)<ul> <li>\u5b89\u88c5\u5e38\u7528\u7684\u8f6f\u4ef6</li> <li>\u4fee\u6539 cloud9 \u78c1\u76d8\u5927\u5c0f (link)</li> </ul> </li> <li>\u5b89\u88c5 eks \u76f8\u5173\u7684\u5e38\u7528\u8f6f\u4ef6 (install some eks related tools)</li> </ul> <pre><code>TMPFILE=$(mktemp)\ncurl --location -o $TMPFILE https://github.com/panlm/panlm.github.io/raw/main/docs/cloud9/setup-cloud9-for-eks.md\nfor i in ONE TWO ; do\n    cat $TMPFILE |awk '/###-SCRIPT-PART-'\"${i}\"'-BEGIN-###/,/###-SCRIPT-PART-'\"${i}\"'-END-###/ {print}' &gt; $TMPFILE-$i.sh\n    chmod a+x $TMPFILE-$i.sh\n    sudo -u ec2-user bash $TMPFILE-$i.sh 2&gt;&amp;1\ndone\n</code></pre>","tags":["aws/cloud9","aws/container/eks"]},{"location":"cloud9/quick-setup-cloud9/#script-part-three","title":"script-part-three","text":"<ul> <li>\u76f4\u63a5\u6267\u884c\u4e0b\u9762\u4ee3\u7801\u5757\u53ef\u80fd\u9047\u5230\u6743\u9650\u4e0d\u591f\u7684\u544a\u8b66\uff0c\u9700\u8981\uff1a<ul> <li>\u5982\u679c\u4f60\u6709 workshop \u7684 Credentials \uff0c\u76f4\u63a5\u5148\u590d\u5236\u7c98\u8d34\u5230\u547d\u4ee4\u884c\uff0c\u518d\u6267\u884c\u4e0b\u5217\u6b65\u9aa4\uff1b(copy and paste your workshop\u2019s credential to CLI and then execute this code block)</li> <li>\u6216\u8005\uff0c\u5982\u679c\u81ea\u5df1\u8d26\u53f7\u7684 cloud9\uff0c\u5148\u7528\u73af\u5883\u53d8\u91cf\u65b9\u5f0f\uff08<code>AWS_ACCESS_KEY_ID</code> \u548c <code>AWS_SECRET_ACCESS_KEY</code>\uff09\u4fdd\u8bc1\u6709\u8db3\u591f\u6743\u9650\u6267\u884c (or using environment variables to export credential yourself)</li> <li>\u4e0b\u9762\u4ee3\u7801\u5757\u5305\u62ec\uff1a<ul> <li>\u7981\u7528 cloud9 \u4e2d\u7684 credential \u7ba1\u7406\uff0c\u4ece <code>~/.aws/credentials</code> \u4e2d\u5220\u9664 <code>aws_session_token=</code> \u884c</li> <li>\u5206\u914d\u7ba1\u7406\u5458\u6743\u9650 role \u5230 cloud9 instance</li> </ul> </li> </ul> </li> </ul> <pre><code>i=THREE\ncat $TMPFILE |awk \"/###-SCRIPT-PART-${i}-BEGIN-###/,/###-SCRIPT-PART-${i}-END-###/ {print}\" &gt; $TMPFILE-$i.sh\nchmod a+x $TMPFILE-$i.sh\nsudo -u ec2-user bash $TMPFILE-$i.sh 2&gt;&amp;1\n</code></pre>","tags":["aws/cloud9","aws/container/eks"]},{"location":"cloud9/quick-setup-cloud9/#open-new-tab-for-verify","title":"open new tab for verify","text":"<ul> <li>\u5728 cloud9 \u4e2d\uff0c\u91cd\u65b0\u6253\u5f00\u4e00\u4e2a terminal \u7a97\u53e3\uff0c\u5e76\u9a8c\u8bc1\u6743\u9650\u7b26\u5408\u9884\u671f\u3002\u4e0a\u9762\u4ee3\u7801\u5757\u5c06\u521b\u5efa\u4e00\u4e2a instance profile \uff0c\u5e76\u5c06\u5173\u8054\u540d\u4e3a <code>adminrole-xxx</code> \u7684 role\uff0c\u6216\u8005\u5728 cloud9 \u73b0\u6709\u7684 role \u4e0a\u5173\u8054 <code>AdministratorAccess</code> role policy\u3002(open new tab to verify you have new role, <code>adminrole-xxx</code>, on your cloud9)</li> </ul> <pre><code>aws sts get-caller-identity\n</code></pre>","tags":["aws/cloud9","aws/container/eks"]},{"location":"cloud9/quick-setup-cloud9/#refer","title":"refer","text":"<ul> <li>open console from local ../CLI/linux/assume-tool </li> </ul>","tags":["aws/cloud9","aws/container/eks"]},{"location":"cloud9/setup-cloud9-for-eks/","title":"Setup Cloud9 for EKS","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/eks","aws/cloud9"]},{"location":"cloud9/setup-cloud9-for-eks/#setup-cloud9-for-eks","title":"Setup Cloud9 for EKS","text":"<p>\u5feb\u901f\u8bbe\u7f6e cloud9 \u7528\u4e8e\u65e5\u5e38\u6d4b\u8bd5\u73af\u5883\u642d\u5efa\uff0c\u5305\u542b\u4ece cloudshell \u4e2d\u521b\u5efa cloud9 instance\uff0c\u7136\u540e\u767b\u5f55 cloud9 instance \u8fdb\u884c\u57fa\u7840\u8f6f\u4ef6\u5b89\u88c5\u3001\u78c1\u76d8\u5927\u5c0f\u8c03\u6574\u548c\u5bb9\u5668\u73af\u5883\u76f8\u5173\u8f6f\u4ef6\u5b89\u88c5\u3002\u4e3a\u4e86\u66f4\u65b9\u4fbf\u914d\u7f6e\uff0c\u5728 quick-setup-cloud9 \u4e2d\uff0c\u76f4\u63a5\u53ef\u4ee5\u4ec5\u901a\u8fc7 cloudshell \u5373\u5b8c\u6210\u6240\u6709\u521d\u59cb\u5316\u52a8\u4f5c\uff0c\u767b\u5f55 cloud9 instance \u540e\u5bc4\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528\u3002</p>","tags":["aws/container/eks","aws/cloud9"]},{"location":"cloud9/setup-cloud9-for-eks/#spin-up-a-cloud9-instance-in-your-region","title":"spin-up-a-cloud9-instance-in-your-region","text":"<ul> <li> <p>\u70b9\u51fb\u8fd9\u91cc \u8fd0\u884c cloudshell\uff0c\u6267\u884c\u4ee3\u7801\u5757\u521b\u5efa cloud9 \u6d4b\u8bd5\u73af\u5883 (open cloudshell, and then execute following code to create cloud9 environment) <pre><code># name=&lt;give your cloud9 a name&gt;\ndatestring=$(date +%Y%m%d-%H%M)\necho ${name:=cloud9-$datestring}\n\n# VPC_ID=&lt;your vpc id&gt; \n# ensure you have public subnet in it\nDEFAULT_VPC_ID=$(aws ec2 describe-vpcs \\\n  --filter Name=is-default,Values=true \\\n  --query 'Vpcs[0].VpcId' --output text \\\n  --region ${AWS_DEFAULT_REGION})\nVPC_ID=${VPC_ID:=$DEFAULT_VPC_ID}\n\nif [[ ! -z ${VPC_ID} ]]; then\n  FIRST_SUBNET=$(aws ec2 describe-subnets \\\n    --filters \"Name=vpc-id,Values=${VPC_ID}\" \\\n    --query 'Subnets[?(AvailabilityZone==`'\"${AWS_DEFAULT_REGION}a\"'` &amp;&amp; MapPublicIpOnLaunch==`true`)].SubnetId' \\\n    --output text \\\n    --region ${AWS_DEFAULT_REGION})\n  aws cloud9 create-environment-ec2 \\\n    --name ${name} \\\n    --image-id amazonlinux-2-x86_64 \\\n    --instance-type m5.large \\\n    --subnet-id ${FIRST_SUBNET%% *} \\\n    --automatic-stop-time-minutes 10080 \\\n    --region ${AWS_DEFAULT_REGION} |tee /tmp/$$\n  echo \"Open URL to access your Cloud9 Environment:\"\n  C9_ID=$(cat /tmp/$$ |jq -r '.environmentId')\n  echo \"https://${AWS_DEFAULT_REGION}.console.aws.amazon.com/cloud9/ide/${C9_ID}\"\nelse\n  echo \"you have no default vpc in $AWS_DEFAULT_REGION\"\nfi\n</code></pre></p> </li> <li> <p>\u70b9\u51fb\u8f93\u51fa\u7684 URL \u94fe\u63a5\uff0c\u6253\u5f00 cloud9 \u6d4b\u8bd5\u73af\u5883 (click the URL at the bottom to open cloud9 environment) </p> </li> </ul>","tags":["aws/container/eks","aws/cloud9"]},{"location":"cloud9/setup-cloud9-for-eks/#using-internal-proxy-or-not","title":"using internal proxy or not","text":"<ul> <li>\u5982\u679c\u4f60\u4e0d\u9700\u8981\u4f7f\u7528\u4ee3\u7406\u670d\u52a1\u5668\u4e0b\u8f7d\u8f6f\u4ef6\u5305\uff0c\u8df3\u8fc7\u6267\u884c\u4e0b\u9762\u4ee3\u7801 (skip this code block if you do not need proxy in your environment) <pre><code>cat &gt;&gt; ~/.bash_profile &lt;&lt;-EOF\nexport http_proxy=http://10.101.1.55:998\nexport https_proxy=http://10.101.1.55:998\nexport NO_PROXY=169.254.169.254,10.0.0.0/8,172.16.0.0/16,192.168.0.0/16\nEOF\nsource ~/.bash_profile\n</code></pre></li> </ul>","tags":["aws/container/eks","aws/cloud9"]},{"location":"cloud9/setup-cloud9-for-eks/#install-in-cloud9-","title":"install-in-cloud9-","text":"<ul> <li> <p>\u4e0b\u9762\u4ee3\u7801\u5757\u5305\u542b\u4e00\u4e9b\u57fa\u672c\u8bbe\u7f6e\uff0c\u5305\u62ec\uff1a(execute this code block to install tools for your lab, and resize ebs of cloud9)</p> <ul> <li>\u5b89\u88c5\u66f4\u65b0\u5e38\u7528\u7684\u8f6f\u4ef6</li> <li>\u4fee\u6539 cloud9 \u78c1\u76d8\u5927\u5c0f (link) <pre><code>###-SCRIPT-PART-ONE-BEGIN-###\necho \"###\"\necho \"SCRIPT-PART-ONE-BEGIN\"\necho \"###\"\n# set size as your expectation, otherwize 100g as default volume size\n# size=200\n\n# install others\nsudo yum -y install jq gettext bash-completion moreutils wget\n\n# install terraform \nsudo yum install -y yum-utils shadow-utils\nsudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo\nsudo yum -y install terraform\n\n# install awscli v2\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o /tmp/awscliv2.zip\necho A |unzip /tmp/awscliv2.zip -d /tmp\nsudo /tmp/aws/install --update 2&gt;&amp;1 &gt;/tmp/awscli-install.log\necho \"complete -C '/usr/local/bin/aws_completer' aws\" &gt;&gt; ~/.bash_profile\n\n# remove existed aws\nif [[ $? -eq 0 ]]; then\n  sudo yum remove -y awscli\n  source ~/.bash_profile\n  aws --version\nfi\n\n# install awscli v1\n# curl \"https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\" -o \"awscli-bundle.zip\"\n# unzip awscli-bundle.zip\n# sudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws\n\n# install ssm session plugin\ncurl \"https://s3.amazonaws.com/session-manager-downloads/plugin/latest/linux_64bit/session-manager-plugin.rpm\" -o \"/tmp/session-manager-plugin.rpm\"\nsudo yum install -y /tmp/session-manager-plugin.rpm\n\n# your default region \nexport AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\n\n# change root volume size\nif [[ -c /dev/nvme0 ]]; then\n  wget -qO- https://github.com/amazonlinux/amazon-ec2-utils/raw/main/ebsnvme-id &gt;/tmp/ebsnvme-id\n  VOLUME_ID=$(sudo python3 /tmp/ebsnvme-id -v /dev/nvme0 |awk '{print $NF}')\n  DEVICE_NAME=/dev/nvme0n1\nelse\n  C9_INST_ID=$(curl 169.254.169.254/latest/meta-data/instance-id)\n  VOLUME_ID=$(aws ec2 describe-volumes --filters Name=attachment.instance-id,Values=${C9_INST_ID} --query \"Volumes[0].VolumeId\" --output text)\n  DEVICE_NAME=/dev/xvda\nfi\n\naws ec2 modify-volume --volume-id ${VOLUME_ID} --size ${size:-100}\nsleep 10\nsudo growpart ${DEVICE_NAME} 1\nsudo xfs_growfs -d /\n\nif [[ $? -eq 1 ]]; then\n  ROOT_PART=$(df |grep -w / |awk '{print $1}')\n  sudo resize2fs ${ROOT_PART}\nfi\n\necho \"###\"\necho \"SCRIPT-PART-ONE-END\"\necho \"###\"\n###-SCRIPT-PART-ONE-END-###\n</code></pre></li> </ul> </li> <li> <p>\u5b89\u88c5 eks \u76f8\u5173\u7684\u5e38\u7528\u8f6f\u4ef6 (install some eks related tools) <pre><code>###-SCRIPT-PART-TWO-BEGIN-###\necho \"###\"\necho \"SCRIPT-PART-TWO-BEGIN\"\necho \"###\"\n\nmv -f ~/.bash_completion ~/.bash_completion.$(date +%N)\n# install kubectl with +/- 1 cluster version 1.24.17 / 1.25.14 / 1.26.9 / 1.27.6\n# refer: https://kubernetes.io/releases/\n# sudo curl --location -o /usr/local/bin/kubectl \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo curl --silent --location -o /usr/local/bin/kubectl \"https://storage.googleapis.com/kubernetes-release/release/v1.25.14/bin/linux/amd64/kubectl\"\nsudo chmod +x /usr/local/bin/kubectl\n\n/usr/local/bin/kubectl completion bash &gt;&gt;  ~/.bash_completion\nsource /etc/profile.d/bash_completion.sh\nsource ~/.bash_completion\nalias k=kubectl \ncomplete -F __start_kubectl k\necho \"alias k=kubectl\" &gt;&gt; ~/.bashrc\necho \"complete -F __start_kubectl k\" &gt;&gt; ~/.bashrc\n\n# install eksctl\n# consider install eksctl version 0.89.0\n# if you have older version yaml \n# https://eksctl.io/announcements/nodegroup-override-announcement/\ncurl -L \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp/\nsudo mv -v /tmp/eksctl /usr/local/bin\n/usr/local/bin/eksctl completion bash &gt;&gt; ~/.bash_completion\nsource /etc/profile.d/bash_completion.sh\nsource ~/.bash_completion\n\n# install kubectx\ncurl -L \"https://github.com/ahmetb/kubectx/releases/download/v0.9.5/kubectx_v0.9.5_linux_x86_64.tar.gz\" |tar xz -C /tmp/\nsudo mv -f /tmp/kubectx /usr/local/bin/\n# install kubens\ncurl -L \"https://github.com/ahmetb/kubectx/releases/download/v0.9.5/kubens_v0.9.5_linux_x86_64.tar.gz\" |tar xz -C /tmp/\nsudo mv -f /tmp/kubens /usr/local/bin/\n\n# install k9s\ncurl -L \"https://github.com/derailed/k9s/releases/latest/download/k9s_Linux_amd64.tar.gz\" |tar xz -C /tmp/\nsudo mv -f /tmp/k9s /usr/local/bin/\n\n# install eksdemo\ncurl -L \"https://github.com/awslabs/eksdemo/releases/latest/download/eksdemo_$(uname -s)_$(uname -p).tar.gz\" |tar xz -C /tmp/\nsudo mv -v /tmp/eksdemo /usr/local/bin\n/usr/local/bin/eksdemo completion bash &gt;&gt; ~/.bash_completion\nsource /etc/profile.d/bash_completion.sh\nsource ~/.bash_completion\n\n# helm newest version (3.10.3)\ncurl -sSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\n# helm 3.8.2 (helm 3.9.0 will have issue #10975)\n# wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz\n# tar xf helm-v3.8.2-linux-amd64.tar.gz\n# sudo mv linux-amd64/helm /usr/local/bin/helm\n/usr/local/bin/helm version --short\n\n# install aws-iam-authenticator 0.6.11 (2023/10) \nwget -O /tmp/aws-iam-authenticator https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.6.11/aws-iam-authenticator_0.6.11_linux_amd64\nchmod +x /tmp/aws-iam-authenticator\nsudo mv /tmp/aws-iam-authenticator /usr/local/bin/\n\n# install kube-no-trouble\nsh -c \"$(curl -sSL https://git.io/install-kubent)\"\n\n# install kubectl convert plugin\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert\" --output-dir /tmp\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert.sha256\" --output-dir /tmp\necho \"$(cat /tmp/kubectl-convert.sha256) /tmp/kubectl-convert\" | sha256sum --check\nsudo install -o root -g root -m 0755 /tmp/kubectl-convert /usr/local/bin/kubectl-convert\nrm /tmp/kubectl-convert /tmp/kubectl-convert.sha256\n\n# option install jwt-cli\n# https://github.com/mike-engel/jwt-cli/blob/main/README.md\n# sudo yum -y install cargo\n# cargo install jwt-cli\n# sudo ln -sf ~/.cargo/bin/jwt /usr/local/bin/jwt\n\n# install flux &amp; fluxctl\ncurl -s https://fluxcd.io/install.sh | sudo -E bash\n/usr/local/bin/flux -v\nsource &lt;(/usr/local/bin/flux completion bash)\n\n# sudo wget -O /usr/local/bin/fluxctl $(curl https://api.github.com/repos/fluxcd/flux/releases/latest | jq -r \".assets[] | select(.name | test(\\\"linux_amd64\\\")) | .browser_download_url\")\n# sudo chmod 755 /usr/local/bin/fluxctl\n# fluxctl version\n# fluxctl identity --k8s-fwd-ns flux\n\necho \"###\"\necho \"SCRIPT-PART-TWO-END\"\necho \"###\"\n###-SCRIPT-PART-TWO-END-###\n</code></pre></p> </li> <li> <p>\u76f4\u63a5\u6267\u884c\u4e0b\u9762\u4ee3\u7801\u5757\u53ef\u80fd\u9047\u5230\u6743\u9650\u4e0d\u591f\u7684\u544a\u8b66\uff0c\u9700\u8981\uff1a</p> <ul> <li>\u5982\u679c\u4f60\u6709 workshop \u7684 Credentials \uff0c\u76f4\u63a5\u5148\u590d\u5236\u7c98\u8d34\u5230\u547d\u4ee4\u884c\uff0c\u518d\u6267\u884c\u4e0b\u5217\u6b65\u9aa4\uff1b(copy and paste your workshop\u2019s credential to CLI and then execute this code block)</li> <li>\u6216\u8005\uff0c\u5982\u679c\u81ea\u5df1\u8d26\u53f7\u7684 cloud9\uff0c\u5148\u7528\u73af\u5883\u53d8\u91cf\u65b9\u5f0f\uff08<code>AWS_ACCESS_KEY_ID</code> \u548c <code>AWS_SECRET_ACCESS_KEY</code>\uff09\u4fdd\u8bc1\u6709\u8db3\u591f\u6743\u9650\u6267\u884c (or using environment variables to export credential yourself)</li> <li>\u4e0b\u9762\u4ee3\u7801\u5757\u5305\u62ec\uff1a<ul> <li>\u7981\u7528 cloud9 \u4e2d\u7684 credential \u7ba1\u7406\uff0c\u4ece <code>~/.aws/credentials</code> \u4e2d\u5220\u9664 <code>aws_session_token=</code> \u884c</li> <li>\u5206\u914d\u7ba1\u7406\u5458\u6743\u9650 role \u5230 cloud9 instance <pre><code>###-SCRIPT-PART-THREE-BEGIN-###\necho \"###\"\necho \"SCRIPT-PART-THREE-BEGIN\"\necho \"###\"\n\naws cloud9 update-environment  --environment-id $C9_PID --managed-credentials-action DISABLE\nrm -vf ${HOME}/.aws/credentials\n\n# ---\nexport AWS_PAGER=\"\"\nexport AWS_DEFAULT_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')\nC9_INST_ID=$(curl 169.254.169.254/latest/meta-data/instance-id)\nROLE_NAME=adminrole-$(TZ=CST-8 date +%Y%m%d-%H%M%S)\nMY_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n\ncat &gt; ec2.json &lt;&lt;-EOF\n{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n    },\n    \"Action\": \"sts:AssumeRole\"\n}\nEOF\nSTATEMENT_LIST=ec2.json\n\nfor i in WSParticipantRole WSOpsRole TeamRole OpsRole ; do\n  aws iam get-role --role-name $i &gt;/dev/null 2&gt;&amp;1\n  if [[ $? -eq 0 ]]; then\n    envsubst &gt;$i.json &lt;&lt;-EOF\n{\n  \"Effect\": \"Allow\",\n  \"Principal\": {\n    \"AWS\": \"arn:aws:iam::${MY_ACCOUNT_ID}:role/$i\"\n  },\n  \"Action\": \"sts:AssumeRole\"\n}\nEOF\n    STATEMENT_LIST=$(echo ${STATEMENT_LIST} \"$i.json\")\n  fi\ndone\n\njq -n '{Version: \"2012-10-17\", Statement: [inputs]}' ${STATEMENT_LIST} &gt; trust.json\necho ${STATEMENT_LIST}\nrm -f ${STATEMENT_LIST}\n\n# create role\naws iam create-role --role-name ${ROLE_NAME} \\\n  --assume-role-policy-document file://trust.json\naws iam attach-role-policy --role-name ${ROLE_NAME} \\\n  --policy-arn \"arn:aws:iam::aws:policy/AdministratorAccess\"\n\ninstance_profile_arn=$(aws ec2 describe-iam-instance-profile-associations \\\n  --filter Name=instance-id,Values=$C9_INST_ID \\\n  --query IamInstanceProfileAssociations[0].IamInstanceProfile.Arn \\\n  --output text)\nif [[ ${instance_profile_arn} == \"None\" ]]; then\n  # create one\n  aws iam create-instance-profile \\\n    --instance-profile-name ${ROLE_NAME}\n  sleep 10\n  # attach role to it\n  aws iam add-role-to-instance-profile \\\n    --instance-profile-name ${ROLE_NAME} \\\n    --role-name ${ROLE_NAME}\n  sleep 10\n  # attach instance profile to ec2\n  aws ec2 associate-iam-instance-profile \\\n    --iam-instance-profile Name=${ROLE_NAME} \\\n    --instance-id ${C9_INST_ID}\nelse\n  existed_role_name=$(aws iam get-instance-profile \\\n    --instance-profile-name ${instance_profile_arn##*/} \\\n    --query 'InstanceProfile.Roles[0].RoleName' \\\n    --output text)\n  aws iam attach-role-policy --role-name ${existed_role_name} \\\n    --policy-arn \"arn:aws:iam::aws:policy/AdministratorAccess\"\nfi\n\necho \"###\"\necho \"SCRIPT-PART-THREE-END\"\necho \"###\"\n###-SCRIPT-PART-THREE-END-###\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>\u5728 cloud9 \u4e2d\uff0c\u91cd\u65b0\u6253\u5f00\u4e00\u4e2a terminal \u7a97\u53e3\uff0c\u5e76\u9a8c\u8bc1\u6743\u9650\u7b26\u5408\u9884\u671f\u3002\u4e0a\u9762\u4ee3\u7801\u5757\u5c06\u521b\u5efa\u4e00\u4e2a instance profile \uff0c\u5e76\u5c06\u5173\u8054\u540d\u4e3a <code>adminrole-xxx</code> \u7684 role\uff0c\u6216\u8005\u5728 cloud9 \u73b0\u6709\u7684 role \u4e0a\u5173\u8054 <code>AdministratorAccess</code> role policy\u3002(open new tab to verify you have new role, <code>adminrole-xxx</code>, on your cloud9) <pre><code>aws sts get-caller-identity\n</code></pre></p> </li> </ul>","tags":["aws/container/eks","aws/cloud9"]},{"location":"cloud9/setup-cloud9-for-eks/#reference","title":"reference","text":"<ul> <li>https://docs.amazonaws.cn/en_us/eks/latest/userguide/install-aws-iam-authenticator.html</li> <li>switch-role-to-create-dedicate-cloud9</li> </ul>","tags":["aws/container/eks","aws/cloud9"]},{"location":"data-analytics/mwaa-lab/","title":"mwaa-lab","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/analytics"]},{"location":"data-analytics/mwaa-lab/#mwaa-lab","title":"mwaa-lab","text":"","tags":["aws/analytics"]},{"location":"data-analytics/mwaa-lab/#prepare-endpoint-for-your-private-network-","title":"prepare-endpoint-for-your-private-network-","text":"<p>\u4e2d\u56fd\u533a mwaa \u670d\u52a1\u521a\u4e0a\u7ebf\uff0c\u7531\u4e8e\u6587\u6863\u8fd8\u672a\u6309\u7167\u4e2d\u56fd\u533a\u7279\u6b8a\u6027\u8fdb\u884c\u63cf\u8ff0\uff0c\u56e0\u6b64\u4f1a\u5bfc\u81f4\u5ba2\u6237\u7591\u60d1\uff08\u6587\u6863\u94fe\u63a5link\uff09\u3002\u672c\u6587\u63d0\u4f9b\u4e24\u79cd\u65b9\u5f0f\u53ef\u4ee5\u6ee1\u8db3\u521b\u5efa mwaa \u7684\u7f51\u7edc\u6761\u4ef6</p>","tags":["aws/analytics"]},{"location":"data-analytics/mwaa-lab/#_1","title":"\u547d\u4ee4\u884c","text":"<p>\u4ece\u672c\u5730\u547d\u4ee4\u884c\u5bf9\u4e8e\u76ee\u6807 vpc \u8fdb\u884c\u5b9a\u5236\uff0c\u9996\u5148\u786e\u4fdd\u4f60\u6709\u5b89\u88c5\u6700\u65b0\u7248\u672c awscli\uff0c\u53e6\u5916\uff0c\u9700\u8981\u6709 credential \u914d\u7f6e\u5728\u547d\u4ee4\u884c</p> <pre><code>VPC_ID=vpc-0ab3bxxxx\nPRIV_SUBNET_STRING=\"subnet-0b314xxx1 subnet-0bddxxxf61\"\n\nexport AWS_PAGER=\"\"\nexport AWS_REGION=cn-northwest-1\nexport AWS_DEFAULT_REGION=${AWS_REGION}\nexport AWS_PROFILE=panlmcn\n\nROUTE_TABLE_STRING=$(for i in ${PRIV_SUBNET_STRING}; do\naws ec2 describe-route-tables \\\n--filter Name=association.subnet-id,Values=$i \\\n--query RouteTables[].RouteTableId \\\n--output text\ndone |sort -u |xargs)\n\n# create security group\nSECURITY_GROUP_NAME=mwaa-endp-$RANDOM\nSECURITY_GROUP_ID=$(aws ec2 create-security-group \\\n  --description ${SECURITY_GROUP_NAME} \\\n  --group-name ${SECURITY_GROUP_NAME} \\\n  --vpc-id ${VPC_ID} \\\n  --query 'GroupId' --output text )\n# self traffic allowed\naws ec2 authorize-security-group-ingress \\\n  --group-id ${SECURITY_GROUP_ID} \\\n  --protocol -1 \\\n  --port -1 \\\n  --source-group ${SECURITY_GROUP_ID}\n\n# china region endpoint services needed by mwaa in private env\nCN_SERVICE_LIST=\"\ncn.com.amazonaws.${AWS_REGION}.monitoring\ncn.com.amazonaws.${AWS_REGION}.ecr.dkr\ncn.com.amazonaws.${AWS_REGION}.ecr.api\ncom.amazonaws.${AWS_REGION}.logs\ncn.com.amazonaws.${AWS_REGION}.sqs\ncom.amazonaws.${AWS_REGION}.kms\ncn.com.amazonaws.${AWS_REGION}.airflow.api  \ncn.com.amazonaws.${AWS_REGION}.airflow.env  \n\"\n\n# global region endpoint services needed by mwaa in private env\nSERVICE_LIST=\"\ncom.amazonaws.${AWS_REGION}.monitoring\ncom.amazonaws.${AWS_REGION}.ecr.dkr\ncom.amazonaws.${AWS_REGION}.ecr.api\ncom.amazonaws.${AWS_REGION}.logs\ncom.amazonaws.${AWS_REGION}.sqs\ncom.amazonaws.${AWS_REGION}.kms\ncom.amazonaws.${AWS_REGION}.airflow.api\ncom.amazonaws.${AWS_REGION}.airflow.env\ncom.amazonaws.${AWS_REGION}.airflow.ops\n\"\n\necho ${AWS_REGION} |egrep -q '^cn-'\nif [[ $? -eq 0 ]]; then\n    SERVICE_LIST=${CN_SERVICE_LIST}\nfi\n\n# create interface endpoint\nfor i in ${SERVICE_LIST}; do\naws ec2 create-vpc-endpoint \\\n    --vpc-id ${VPC_ID} \\\n    --vpc-endpoint-type Interface \\\n    --service-name $i \\\n    --subnet-id ${PRIV_SUBNET_STRING} \\\n    --security-group-id ${SECURITY_GROUP_ID}\ndone\n\nenvsubst &gt;s3-gw-endpoint-policy.json &lt;&lt;-EOF\n{\n  \"Statement\": [\n    {\n      \"Principal\": \"*\",\n      \"Action\": \"*\",\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\nEOF\n\n# create s3 gateway endpoint\naws ec2 create-vpc-endpoint --vpc-id ${VPC_ID} \\\n--service-name com.amazonaws.${AWS_REGION}.s3 \\\n--route-table-ids ${ROUTE_TABLE_STRING} \\\n--vpc-endpoint-type Gateway \\\n--policy-document file://s3-gw-endpoint-policy.json\n</code></pre>","tags":["aws/analytics"]},{"location":"data-analytics/mwaa-lab/#cloudformation","title":"cloudformation \u6a21\u7248","text":"<ul> <li>\u4f7f\u7528\u8fd9\u4e2a\u6a21\u7248\uff08download\uff09\u521b\u5efa\u4e13\u7528\u4e8e mwaa \u7684 vpc \u73af\u5883</li> </ul>","tags":["aws/analytics"]},{"location":"data-analytics/mwaa-lab/#create-mwaa-environment","title":"create mwaa environment","text":"<ul> <li> <p>\u9009\u62e9\u4e4b\u524d\u547d\u4ee4\u884c\u4e2d\u6307\u5b9a\u7684 vpc \u548c \u5b50\u7f51 </p> </li> <li> <p>\u4f7f\u7528\u79c1\u6709\u7f51\u7edc\u66b4\u9732 web \u670d\u52a1\u5668\uff0c\u540e\u7eed\u53ef\u4ee5\u901a\u8fc7 ssh tunnel \u6216\u8005 ssm tunnel \u65b9\u5f0f\u8fdc\u7a0b\u8bbf\u95ee </p> </li> <li> <p>\u540c\u65f6\u9009\u62e9 \u201c\u521b\u5efa\u65b0\u5b89\u5168\u7ec4\u201d \u548c \u201c\u4e4b\u524d\u547d\u4ee4\u884c\u521b\u5efa\u7684\u5b89\u5168\u7ec4\uff08\u7528\u4e8e interface endpoint\uff09\u201d </p> </li> </ul>","tags":["aws/analytics"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/","title":"rds-mysql-replica-cross-region-cross-account","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#rds-mysql-replica-cross-region-cross-account","title":"rds-mysql-replica-cross-region-cross-account","text":"","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#_1","title":"\u6982\u8ff0","text":"<p>\u672c\u5730 RDS-A \uff0c\u5e0c\u671b\u80fd\u521b\u5efa\u4e00\u4e2a\u8de8\u8d26\u53f7\u7684 RDS-B \u4f5c\u4e3a\u8bfb\u526f\u672c</p> <ul> <li>\u5148\u521b\u5efa RDS-A</li> <li>\u521b\u5efa\u672c\u5730 Replica</li> <li>\u5feb\u7167\u8be5 Replica</li> <li>\u5171\u4eab\u5feb\u7167\u5230\u53e6\u4e00\u4e2a\u8d26\u53f7</li> <li>\u5728\u53e6\u4e00\u4e2a\u8d26\u53f7\u4e2d\u5c06\u5feb\u7167\u590d\u5236\u4e00\u4efd<ul> <li>\u6b64\u65f6\u53ef\u4ee5\u4f7f\u7528kms\uff0c\u5982\u679c\u6e90\u5e93\u6ca1\u6709\u52a0\u5bc6</li> </ul> </li> <li>\u4ece\u590d\u5236\u51fa\u6765\u7684\u5feb\u7167\u6062\u590d\u6570\u636e\u5e93</li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#_2","title":"\u573a\u666f","text":"<ul> <li>\u8de8\u8d26\u53f7\u521b\u5efa\u8bfb\u526f\u672c\u7684 rds \u6570\u636e\u5e93</li> <li>\u5c06\u672a\u52a0\u5bc6\u6570\u636e\u5e93\u8f6c\u6362\u6210\u52a0\u5bc6\u5b58\u50a8</li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#create-rds-mysql","title":"create rds mysql","text":"","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#prep-","title":"prep-","text":"<ul> <li>\u51c6\u5907\u6d4b\u8bd5\u73af\u5883\uff0c\u5efa\u8bae\u4f7f\u7528 cloud9 \u8fdb\u884c\u64cd\u4f5c\uff0c\u5e76\u4e14\u5b89\u88c5\u4e0b\u9762\u8f6f\u4ef6</li> <li>\u5982\u679c\u8de8\u8d26\u53f7\u590d\u5236\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u4f60\u9700\u8981\u540c\u6837\u7684 cloud9 \u5728\u53e6\u4e00\u4e2a\u73af\u5883\u4e2d\uff0c\u5e76\u4e14\u5b89\u88c5\u4e0b\u9762\u8f6f\u4ef6 <pre><code>sudo yum install -y jq \nsudo rpm -Uvh https://repo.mysql.com/mysql80-community-release-el7.rpm\nsudo yum install -y mysql-community-client --enablerepo=mysql80-community\n\nexport AWS_PAGER=\"\"\n</code></pre></li> </ul> <p>^z60dbq</p> <ul> <li>\u83b7\u53d6 cloud9 \u6240\u5728\u5b50\u7f51\uff0c\u6d4b\u8bd5\u4f1a\u4f7f\u7528\u8be5\u7f51\u7edc <pre><code># cloud 9 subnet\nRDS_NAME=db1\nINST_ID=$(curl http://169.254.169.254/1.0/meta-data/instance-id 2&gt;/dev/null)\nVPC_ID=$(aws ec2 describe-instances --instance-ids ${INST_ID} --query 'Reservations[0].Instances[0].VpcId' --output text)\nAWS_REGION=$(curl 2&gt;/dev/null http://169.254.169.254/latest/dynamic/instance-identity/document |jq -r '.region')\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#subnet-group-","title":"subnet-group-","text":"<ul> <li>\u5982\u679c\u8de8\u8d26\u53f7\u590d\u5236\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u8be5\u6b65\u9aa4\u9700\u8981\u5728\u53e6\u4e00\u4e2a\u8d26\u53f7\u4e2d\u88ab\u91cd\u590d\u6267\u884c <pre><code>SG_NAME=${RDS_NAME}-${RANDOM}\naws ec2 create-security-group  \\\n  --description ${SG_NAME}     \\\n  --group-name ${SG_NAME}      \\\n  --vpc-id ${VPC_ID}\n\nRDS_SG=$(aws ec2 describe-security-groups      \\\n  --filters Name=group-name,Values=${SG_NAME}         \\\n            Name=vpc-id,Values=${VPC_ID}              \\\n  --query \"SecurityGroups[0].GroupId\" --output text)\n\necho \"RDS security group ID: ${RDS_SG}\"\n\naws ec2 authorize-security-group-ingress  \\\n  --group-id ${RDS_SG}                    \\\n  --protocol tcp                          \\\n  --port 3306                             \\\n  --cidr '0.0.0.0/0'\n\nPUBLIC_SUBNETS_ID=$(aws ec2 describe-subnets        \\\n  --filters \"Name=vpc-id,Values=$VPC_ID\"                   \\\n  --query 'Subnets[?MapPublicIpOnLaunch==`true`].SubnetId' \\\n  --output json | jq -c .)\n\n# create a db subnet group\naws rds create-db-subnet-group               \\\n  --db-subnet-group-name ${RDS_NAME}         \\\n  --db-subnet-group-description ${RDS_NAME}  \\\n  --subnet-ids ${PUBLIC_SUBNETS_ID} \n</code></pre></li> </ul> <p>^rav4er</p>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#create-rds-mysql-cont","title":"create rds mysql (cont.)","text":"<ul> <li>\u521b\u5efa rds \u6570\u636e\u5e93</li> <li>\u5bc6\u7801\u4fdd\u5b58\u5728 <code>~/rds_password</code> \u4e2d <pre><code># generate a password for RDS\nexport RDS_PASSWORD=\"$(date | md5sum  |cut -f1 -d' ')\"\necho ${RDS_PASSWORD}  &gt; ~/rds_password\n\n# install supported oldest mysql version \nENGINE_VER=$(aws rds describe-db-engine-versions --engine mysql --query \"DBEngineVersions[].EngineVersion\" |grep -Eo '5\\.7\\.[0-9]+' |sort |head -n 1)\n\n# create RDS MySQL instance\n# INSTANCE_TYPE=db.m5.xlarge\n# STORAGE_SIZE=5000\naws rds create-db-instance                          \\\n  --db-instance-identifier ${RDS_NAME}              \\\n  --db-name ${RDS_NAME}                             \\\n  --db-instance-class ${INSTANCE_TYPE:-db.m5.large} \\\n  --engine mysql                                    \\\n  --engine-version ${ENGINE_VER}                    \\\n  --db-subnet-group-name ${RDS_NAME}                \\\n  --vpc-security-group-ids ${RDS_SG}                \\\n  --master-username ${RDS_NAME}                     \\\n  --publicly-accessible                             \\\n  --master-user-password ${RDS_PASSWORD}            \\\n  --backup-retention-period 1                       \\\n  --allocated-storage ${STORAGE_SIZE:-50} \n\n# --storage-encrypted\n\n# get rds status util `available`\nstatus=\"\"\nuntil [[ ${status} == \"available\" ]]; do\nstatus=$(aws rds describe-db-instances       \\\n  --db-instance-identifier ${RDS_NAME}       \\\n  --query \"DBInstances[].DBInstanceStatus\"   \\\n  --output text)\necho ${status}\nsleep 60\ndone\n\nRDS_HOSTNAME=$(aws rds describe-db-instances    \\\n  --db-instance-identifier ${RDS_NAME}     \\\n  --query \"DBInstances[].Endpoint.Address\"    \\\n  --output text)\n\nRDS_ARN=$(aws rds describe-db-instances    \\\n  --db-instance-identifier ${RDS_NAME}     \\\n  --query \"DBInstances[].DBInstanceArn\"    \\\n  --output text)\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#create-read-replica","title":"create read replica","text":"<ul> <li>\u521b\u5efa\u8bfb\u526f\u672c <pre><code>RDS_REP1_NAME=${RDS_NAME}-rep1 \n\n# # enable auto backup if you miss it in creation\n# aws rds modify-db-instance \\\n#   --db-instance-identifier ${RDS_NAME} \\\n#   --backup-retention-period 1  \\\n#   --apply-immediately\n\n# create read replica\naws rds create-db-instance-read-replica \\\n  --db-instance-identifier ${RDS_REP1_NAME} \\\n  --region ${AWS_REGION} \\\n  --source-region ${AWS_REGION} \\\n  --source-db-instance-identifier ${RDS_ARN}\n\n# get rds status util `available`\nstatus=\"\"\nuntil [[ ${status} == \"available\" ]]; do\nstatus=$(aws rds describe-db-instances            \\\n  --db-instance-identifier ${RDS_REP1_NAME}       \\\n  --query \"DBInstances[].DBInstanceStatus\"        \\\n  --output text)\necho ${status}\nsleep 60\ndone\n\nRDS_REP1_HOSTNAME=$(aws rds describe-db-instances    \\\n  --db-instance-identifier ${RDS_REP1_NAME}     \\\n  --query \"DBInstances[].Endpoint.Address\"    \\\n  --output text)\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#on-master","title":"on master","text":"<ul> <li>\u5728\u4e3b\u5e93\u4e2d\u521b\u5efa\u590d\u5236\u7528\u6237</li> <li>\u914d\u7f6e binlog \u7684\u4fdd\u7559\u5468\u671f\uff0c\u9700\u8981\u5728\u6b64\u671f\u95f4\u5b8c\u6210\u8fdc\u7a0b\u8bfb\u526f\u672c\u521b\u5efa\u5e76\u4e14\u6062\u590d\u590d\u5236 <pre><code>echo mysql -h${RDS_HOSTNAME} -u${RDS_NAME} -p${RDS_PASSWORD}\n</code></pre></li> </ul> <pre><code>call mysql.rds_set_configuration('binlog retention hours', 24);\nCREATE USER 'repl_user'@'%' IDENTIFIED BY 'repl_password';\nGRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'repl_user'@'%';\n</code></pre> <p>^gycsd4</p>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#on-slave","title":"on slave","text":"<pre><code>echo mysql -h${RDS_REP1_HOSTNAME} -u${RDS_NAME} -p${RDS_PASSWORD}\n</code></pre> <ul> <li> <p>\u68c0\u67e5\u590d\u5236\u72b6\u6001 <pre><code>SHOW SLAVE STATUS\\G\n</code></pre></p> </li> <li> <p>\u5f53\u4e0b\u9762\u503c\u4e3a 0 \u65f6\uff0c\u53ef\u4ee5\u4e2d\u65ad\u590d\u5236</p> <p>Seconds_Behind_Master: 0</p> </li> </ul> <pre><code>call mysql.rds_stop_replication();\nSHOW SLAVE STATUS\\G\n</code></pre> <ul> <li>\u4e2d\u65ad\u590d\u5236\uff0c\u5e76\u4e14\u8bb0\u5f55\u65ad\u70b9\uff0c\u5728\u540e\u7eed\u6062\u590d\u590d\u5236\u65f6\u4f7f\u7528</li> </ul> <p>Relay_Master_Log_File: mysql-bin-changelog.000009 Exec_Master_Log_Pos: 154</p> <p>^gjmipb</p>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#create-snapshot-on-replica","title":"create snapshot on replica","text":"<ul> <li>\u521b\u5efa\u5feb\u7167 <pre><code>RDS_REP1_SNAP_NAME=${RDS_REP1_NAME}-snap-1\naws rds create-db-snapshot \\\n--db-snapshot-identifier ${RDS_REP1_SNAP_NAME} \\\n--db-instance-identifier ${RDS_REP1_NAME}\n\nSHARED_SNAP_ARN=$(aws rds describe-db-snapshots \\\n--db-snapshot-identifier ${RDS_REP1_SNAP_NAME} \\\n--query 'DBSnapshots[].DBSnapshotArn' \\\n--output text)\n\necho \"SHARED_SNAP_ARN=${SHARED_SNAP_ARN}\"\n\n# get snapshot status util `available`\nwhile true ; do\nstatus=$(aws rds describe-db-snapshots \\\n--db-snapshot-identifier ${RDS_REP1_SNAP_NAME} \\\n--query 'DBSnapshots[].Status' \\\n--output text)\necho $status\nif [[ $status == \"available\" ]]; then\n  break\nfi\nsleep 60\ndone\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#share-snapshot","title":"share snapshot","text":"<ul> <li>\u8de8\u8d26\u53f7\u5171\u4eab\u5feb\u7167</li> <li>\u8f93\u5165\u76ee\u6807\u8d26\u53f7 ID <pre><code>aws rds modify-db-snapshot-attribute \\\n    --db-snapshot-identifier ${RDS_REP1_SNAP_NAME} \\\n    --attribute-name restore \\\n    --values-to-add &lt;target_account_id&gt;\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#copy-snapshot-local","title":"copy snapshot local","text":"<ul> <li>\u5982\u679c\u662f\u8de8\u8d26\u53f7\u590d\u5236\uff0c\u5219\u8be5\u6b65\u9aa4\u6267\u884c\u5728\u53e6\u4e00\u4e2a\u8d26\u53f7\u4e2d\uff0c\u9700\u8981\u5148\u8fdb\u884c\u4e00\u4e9b\u73af\u5883\u51c6\u5907\u5de5\u4f5c </li> </ul> <p>refer: git/git-mkdocs/data-analytics/rds-mysql-replica-cross-region-cross-account</p>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#check-snapshot","title":"check snapshot","text":"<ul> <li>\u5c06\u6e90\u8d26\u53f7\u7684\u73af\u5883\u53d8\u91cf\u590d\u5236\u5230\u73b0\u6709\u8d26\u53f7\u7684\u547d\u4ee4\u884c\u7a97\u53e3\u65b9\u4fbf\u6267\u884c\u540e\u7eed\u64cd\u4f5c <pre><code>SHARED_SNAP_ARN=???\nLOCAL_SNAP_NAME=local-snap-$RANDOM\n\n# get shared snapshot\naws rds describe-db-snapshots --include-shared \\\n--db-snapshot-identifier ${SHARED_SNAP_ARN}\n\n# ensure `SnapshotType` is `shared`\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#copy-without-kms","title":"copy without kms","text":"<ul> <li>\u590d\u5236\u5feb\u7167\u5230\u672c\u8d26\u53f7\uff0c\u4e14\u4e0d\u4fee\u6539\u539f\u6709\u6570\u636e\u5e93\u672a\u52a0\u5bc6\u72b6\u6001 <pre><code>aws rds copy-db-snapshot \\\n--source-db-snapshot-identifier ${SHARED_SNAP_ARN} \\\n--target-db-snapshot-identifier ${LOCAL_SNAP_NAME}\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#copy-with-kms-option","title":"copy with kms (option)","text":"<ul> <li>\uff08\u53ef\u9009\uff09\u590d\u5236\u5feb\u7167\u5230\u672c\u8d26\u53f7\uff0c\u4e14\u4fee\u6539\u539f\u6709\u6570\u636e\u5e93\u672a\u52a0\u5bc6\u72b6\u6001\u4e3a\u52a0\u5bc6\u72b6\u6001</li> <li>\u63d0\u524d\u521b\u5efa\u6240\u9700\u8981\u7684CMK\uff0c\u6216\u8005\u6307\u5b9aKMS <pre><code># KEY_ARN=???\nKEY_ARN=$(aws kms create-key |jq -r '.KeyMetadata.Arn')\n\naws rds copy-db-snapshot \\\n--source-db-snapshot-identifier ${SHARED_SNAP_ARN} \\\n--target-db-snapshot-identifier ${LOCAL_SNAP_NAME}\n--kms-key-id ${KEY_ARN}\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#wait-snapshot-complete","title":"wait snapshot complete","text":"<ul> <li>\u7b49\u5f85\u590d\u5236\u5feb\u7167\u64cd\u4f5c\u5b8c\u6210 <pre><code># get snapshot status util `available`\nstatus=\"\"\nuntil [[ ${status} == \"available\" ]]; do\nstatus=$(aws rds describe-db-snapshots        \\\n  --db-snapshot-identifier ${LOCAL_SNAP_NAME} \\\n  --query 'DBSnapshots[].Status'              \\\n  --output text)\necho ${status}\nsleep 60\ndone\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#restore","title":"restore","text":"","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#prep","title":"prep","text":"<pre><code>VPC_ID=vpc-0c6e8c75ad4af1ee5\nRDS_NAME=db1-restore\n</code></pre>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#subnet-group","title":"subnet group","text":"<ul> <li>\u5982\u679c\u662f\u8de8\u8d26\u53f7\uff0c\u9700\u8981\u91cd\u65b0\u521b\u5efa subnet group </li> </ul> <p>refer: git/git-mkdocs/data-analytics/rds-mysql-replica-cross-region-cross-account</p>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#restore-db","title":"restore db","text":"<ul> <li>\u4ece\u590d\u5236\u7684\u5feb\u7167\u6062\u590d\u6570\u636e\u5e93 <pre><code># restore RDS MySQL instance\naws rds restore-db-instance-from-db-snapshot \\\n  --db-snapshot-identifier ${LOCAL_SNAP_NAME} \\\n  --db-instance-identifier ${RDS_NAME}     \\\n  --db-instance-class db.t3.micro          \\\n  --engine mysql                           \\\n  --db-subnet-group-name ${RDS_NAME}       \\\n  --vpc-security-group-ids ${RDS_SG}       \\\n  --publicly-accessible   \n\n# get rds status util `available`\nstatus=\"\"\nuntil [[ ${status} == \"available\" ]]; do\nstatus=$(aws rds describe-db-instances       \\\n  --db-instance-identifier ${RDS_NAME}       \\\n  --query \"DBInstances[].DBInstanceStatus\"   \\\n  --output text)\necho ${status}\nsleep 60\ndone\n\nTARGET_RDS_HOSTNAME=$(aws rds describe-db-instances    \\\n  --db-instance-identifier ${RDS_NAME}     \\\n  --query \"DBInstances[].Endpoint.Address\"    \\\n  --output text)\n</code></pre></li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#on-target","title":"on target","text":"<ul> <li> <p>\u8fde\u63a5\u5230\u6062\u590d\u540e\u7684 rds \u6570\u636e\u5e93\uff0c\u6ce8\u610f\u7528\u6237\u540d\u4e3a\u6e90\u8d26\u53f7\u4e3b\u5e93\uff0c\u5bc6\u7801\u5728\u6e90\u8d26\u53f7 <code>~/rds_password</code> \u4e2d <pre><code># user should be origin db1 \n# pass should be saved in ~/rds_password\necho mysql -h${TARGET_RDS_HOSTNAME} -u${RDS_NAME} -p${RDS_PASSWORD}\n</code></pre></p> </li> <li> <p>\u4fee\u6539\u4e0b\u9762\u8bed\u53e5\uff0c\u5e76\u4e14\u6267\u884c</p> </li> <li>\u6e90\u8d26\u53f7\u4e3b\u5e93dns</li> <li>\u786e\u8ba4\u590d\u5236\u7528\u6237\u7684\u7528\u6237\u540d\u548c\u5bc6\u7801 (^gycsd4)</li> <li> <p>\u786e\u8ba4\u4e4b\u524d\u8bb0\u5f55\u7684\u65ad\u70b9 (^gjmipb)  <pre><code>CALL mysql.rds_set_external_master (\n  'db1.ckzqxxxxxxrg.us-east-2.rds.amazonaws.com'\n  , 3306\n  , 'repl_user'\n  , 'repl_password'\n  , 'mysql-bin-changelog.000009'\n  , 154\n  , 0\n  );\n</code></pre></p> </li> <li> <p>\u786e\u8ba4\u6062\u590d\u590d\u5236\u64cd\u4f5c\u6210\u529f <pre><code>CALL mysql.rds_start_replication();\n\nSHOW SLAVE STATUS\\G\n</code></pre></p> </li> <li> <p>\u5982\u679c\u6062\u590d\u590d\u5236\u64cd\u4f5c\u6210\u529f\u5c06\u51fa\u73b0\u4e0b\u9762\u8f93\u51fa <pre><code>+-------------------------+\n| Message                 |\n+-------------------------+\n| Slave running normally. |\n+-------------------------+\n1 row in set (1.01 sec)\n</code></pre></p> </li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#refer","title":"refer","text":"<ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-cross-region-replica/</li> <li>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/mysql_rds_set_external_master.html</li> <li>https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/</li> </ul>","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#issue","title":"issue","text":"","tags":["aws/database/rds"]},{"location":"data-analytics/rds-mysql-replica-cross-region-cross-account/#host-error-in-mysqluser","title":"host error in mysql.user","text":"<pre><code>mysql&gt; select user,host from mysql.user;\n+------------------+-----------+\n| user             | host      |\n+------------------+-----------+\n| rdsrepladmin     | %         |\n| rdsworkshop      | %         |\n| repl_user        | *         |\n| mysql.infoschema | localhost |\n| mysql.session    | localhost |\n| mysql.sys        | localhost |\n| rdsadmin         | localhost |\n+------------------+-----------+\n7 rows in set (0.00 sec)\n\nmysql&gt; update mysql.user set host='%' where user='repl_user';\nQuery OK, 1 row affected (0.01 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql&gt; flush privileges;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&gt; \n</code></pre>","tags":["aws/database/rds"]},{"location":"data-analytics/redshift-data-api-lab/","title":"redshift-data-api-lab","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#redshift-data-api-lab","title":"redshift-data-api-lab","text":"","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#_1","title":"\u4f7f\u7528\u573a\u666f","text":"<p>Amazon Redshift \u6570\u636e API \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528\u6240\u6709\u7c7b\u578b\u7684\u4f20\u7edf\u3001\u4e91\u539f\u751f\u548c\u5bb9\u5668\u5316\u3001\u57fa\u4e8e Web \u670d\u52a1\u7684\u65e0\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u5e94\u7528\u7a0b\u5e8f\u8f7b\u677e\u8bbf\u95ee\u6765\u81ea Amazon Redshift \u7684\u6570\u636e\u3002</p> <p></p> <p>Amazon Redshift Data API \u4e0d\u80fd\u66ff\u4ee3 JDBC \u548c ODBC \u9a71\u52a8\u7a0b\u5e8f\uff0c\u9002\u7528\u4e8e\u4e0d\u9700\u8981\u4e0e\u96c6\u7fa4\u5efa\u7acb\u6301\u4e45\u8fde\u63a5\u7684\u7528\u4f8b\u3002\u5b83\u9002\u7528\u4e8e\u4ee5\u4e0b\u7528\u4f8b\uff1a</p> <ul> <li>\u4f7f\u7528 AWS \u5f00\u53d1\u5de5\u5177\u5305\u652f\u6301\u7684\u4efb\u4f55\u7f16\u7a0b\u8bed\u8a00\u4ece\u81ea\u5b9a\u4e49\u5e94\u7528\u7a0b\u5e8f\u8bbf\u95ee Amazon Redshift\u3002\u8fd9\u4f7f\u60a8\u80fd\u591f\u96c6\u6210\u57fa\u4e8e Web \u670d\u52a1\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u4f7f\u7528 API \u8bbf\u95ee\u6765\u81ea Amazon Redshift \u7684\u6570\u636e\u4ee5\u8fd0\u884c SQL \u8bed\u53e5\u3002\u4f8b\u5982\uff0c\u60a8\u53ef\u4ee5\u4ece JavaScript \u8fd0\u884c SQL\u3002</li> <li>\u6784\u5efa\u65e0\u670d\u52a1\u5668\u6570\u636e\u5904\u7406\u5de5\u4f5c\u6d41\u7a0b\u3002</li> <li>\u8bbe\u8ba1\u5f02\u6b65 Web \u4eea\u8868\u677f\uff0c\u56e0\u4e3a Data API \u5141\u8bb8\u60a8\u8fd0\u884c\u957f\u65f6\u95f4\u8fd0\u884c\u7684\u67e5\u8be2\uff0c\u800c\u65e0\u9700\u7b49\u5f85\u5b83\u5b8c\u6210\u3002</li> <li>\u8fd0\u884c\u4e00\u6b21\u67e5\u8be2\u5e76\u591a\u6b21\u68c0\u7d22\u7ed3\u679c\uff0c\u800c\u65e0\u9700\u5728 24 \u5c0f\u65f6\u5185\u518d\u6b21\u8fd0\u884c\u67e5\u8be2\u3002</li> <li>\u4f7f\u7528 AWS Step Functions\u3001Lambda \u548c\u5b58\u50a8\u8fc7\u7a0b\u6784\u5efa\u60a8\u7684 ETL \u7ba1\u9053\u3002</li> <li>\u7b80\u5316\u4e86\u4ece Amazon SageMaker \u548c Jupyter \u7b14\u8bb0\u672c\u5bf9 Amazon Redshift \u7684\u8bbf\u95ee\u3002</li> <li>\u4f7f\u7528 Amazon EventBridge \u548c Lambda \u6784\u5efa\u4e8b\u4ef6\u9a71\u52a8\u7684\u5e94\u7528\u7a0b\u5e8f\u3002</li> <li>\u8c03\u5ea6 SQL \u811a\u672c\u4ee5\u7b80\u5316\u7269\u5316\u89c6\u56fe\u7684\u6570\u636e\u52a0\u8f7d\u3001\u5378\u8f7d\u548c\u5237\u65b0\u3002</li> </ul>","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#-redshift-","title":"\u521d\u59cb\u5316-redshift-\u96c6\u7fa4-","text":"<ul> <li>\u521b\u5efa redshift \u96c6\u7fa4 (link), or open this cloudformation template directly, or download from below URL<ul> <li>\u521b\u5efa vpc \u52a0 2 \u4e2a\u516c\u6709\u5b50\u7f51\uff0c\u5e76\u4e14\u521b\u5efa public access \u7684 redshift \u96c6\u7fa4</li> <li>InboundTraffic \u2013&gt; <code>0.0.0.0/0</code></li> <li>EETeamRoleArn \u2013&gt; <code>arn:aws:iam::xxxxxxxxxxxx:role/TeamRole</code></li> <li>MasterUserPassword \u2013&gt; default</li> <li>DataLoadingPrimaryCluster \u2013&gt; Yes <ul> <li>check cloudwatch for more detail</li> <li>data loading need more 10 mins after cloudformation completed</li> </ul> </li> </ul> </li> <li>(option) \u7136\u540e\u4ece\u8fd9\u91cc\u52a0\u8f7d\u6570\u636e (link)</li> </ul> <pre><code>wget 'https://github.com/panlm/aws-labs/raw/main/redshift-data-api/redshift-immersion.yaml'\n</code></pre>","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#rest-api-lab","title":"rest-api lab","text":"<ul> <li>postman example</li> </ul>","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#list-database","title":"list database","text":"<p>post url: <code>https://redshift-data.us-east-2.amazonaws.com/</code></p> <p>head: <code>x-amz-target</code>: <code>RedshiftData.ListDatabases</code> <code>Content-Type</code>: <code>application/x-amz-json-1.1</code></p> <p>body: <pre><code>{\n    \"ClusterIdentifier\": \"redshift-cluster-1\",\n    \"Database\": \"dev\",\n    \"DbUser\": \"awsuser\"\n}\n</code></pre></p>","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#list-tables","title":"list tables","text":"<p>head: <code>x-amz-target</code>: <code>RedshiftData.ListTables</code></p>","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#execute-statement","title":"execute statement","text":"<p>head: <code>x-amz-target</code>: <code>RedshiftData.ExecuteStatement</code></p> <p>body: <pre><code>{\n    \"ClusterIdentifier\": \"redshift-cluster-1\",\n    \"Database\": \"dev\",\n    \"DbUser\": \"awsuser\",\n    \"Sql\": \"SELECT * FROM \\\"dev\\\".\\\"public\\\".\\\"event\\\";\"\n}\n</code></pre></p>","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#command-line-lab","title":"command line lab","text":"<ul> <li>shell/python example</li> </ul> <pre><code>aws redshift-data list-tables  --database dev \\\n    --db-user admin \\\n    --cluster-identifier redshift-cluster-us \\\n    --region us-east-1  \\\n    --table-pattern \"prod%\" \\\n    --schema-pattern \"rs%\"\n</code></pre> <pre><code>{\n    \"Tables\": [\n        {\n            \"name\": \"event\",\n            \"schema\": \"public\",\n            \"type\": \"TABLE\"\n        }\n    ]\n}\n</code></pre>","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#reference","title":"reference","text":"<ul> <li>Get started with the Amazon Redshift Data API</li> <li>Using the Amazon Redshift Data API to interact with Amazon Redshift clusters</li> </ul>","tags":["aws/database/redshift"]},{"location":"data-analytics/redshift-data-api-lab/#broken","title":"broken","text":"<ul> <li>Build a REST API to enable data consumption from Amazon Redshift</li> </ul> <p>us-east-1 only</p> <p>post data: <pre><code>{\n    \"createdate\": \"03/01/2022\",\n    \"productname\": \"Flower\",\n    \"sku\": \"FLOWER123\",\n    \"requesttype\": \"Product\"\n}\n</code></pre></p>","tags":["aws/database/redshift"]},{"location":"others/POC-mig-filezilla-to-transfer-family/","title":"migrate-filezilla-to-transfer-family","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#poc-to-transfer-family","title":"POC-to-transfer-family","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#requirement","title":"requirement","text":"<ul> <li>same domain for both sftp and ftps server</li> <li>BU manage users themselves</li> <li>password and public key access</li> </ul>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#diagram","title":"diagram","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#walkthrough","title":"walkthrough","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#directory-service","title":"directory-service","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#create-ad-","title":"create-AD-","text":"<ul> <li>create managed directory service on aws <pre><code>AD=corp2.aws.panlm.xyz\nPASS=${PASS:-passworD.1}\n\nexport AWS_PAGER=\"\"\nVPC=$(aws ec2 describe-vpcs \\\n    --filters \"Name=isDefault,Values=true\" \\\n    --query \"Vpcs[0].VpcId\" \\\n    --output text)\nSUBNETS=$(aws ec2 describe-subnets \\\n    --filters \"Name=vpc-id,Values=${VPC}\" \"Name=map-public-ip-on-launch,Values=true\" \\\n    --query \"Subnets[].SubnetId\" \\\n    --output text |awk 'BEGIN{OFS=\",\"} {print $1,$2}')\n\naws ds create-microsoft-ad \\\n    --name ${AD} \\\n    --short-name ${AD%%.*} \\\n    --password ${PASS} \\\n    --edition Standard \\\n    --vpc-settings VpcId=${VPC},SubnetIds=${SUBNETS} |tee /tmp/ds-$$.1\nMSDS_ID=$(cat /tmp/ds-$$.1 |jq -r '.DirectoryId')\n\n# until \"Active\"\nwatch -g -n 60 aws ds describe-directories \\\n    --directory-ids ${MSDS_ID} \\\n    --query DirectoryDescriptions[0].Stage \\\n    --output text\n\nMSDS_IP=($(aws ds describe-directories \\\n    --directory-ids ${MSDS_ID} \\\n    --query DirectoryDescriptions[0].DnsIpAddrs \\\n    --output text))\n</code></pre></li> </ul>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#create-instance-and-join-domain","title":"create instance and join domain","text":"<ul> <li>launch instance and join domain seamlessly (link)</li> <li> <p>create role for joining domain instance <pre><code>ROLE_NAME=ec2-msds-role-$(TZ=EAT-8 date +%Y%m%d-%H%M%S)\ncat &gt; trust.json &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"ec2.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\nEOF\naws iam create-role --role-name ${ROLE_NAME} \\\n    --assume-role-policy-document file://trust.json\naws iam attach-role-policy --role-name ${ROLE_NAME} \\\n    --policy-arn \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\naws iam attach-role-policy --role-name ${ROLE_NAME} \\\n    --policy-arn \"arn:aws:iam::aws:policy/AmazonSSMDirectoryServiceAccess\"\naws iam attach-role-policy --role-name ${ROLE_NAME} \\\n    --policy-arn \"arn:aws:iam::aws:policy/AWSOpsWorksCloudWatchLogs\"\naws iam create-instance-profile --instance-profile-name ${ROLE_NAME} |tee /tmp/inst-profile-$$.1\naws iam add-role-to-instance-profile --instance-profile-name ${ROLE_NAME} --role-name ${ROLE_NAME}\nINSTANCE_PROFILE_ARN=$(cat /tmp/inst-profile-$$.1 |jq -r '.InstanceProfile.Arn')\n</code></pre></p> </li> <li> <p>create key - git/git-mkdocs/CLI/awscli/ec2-cmd</p> </li> <li>create instance <pre><code># IMAGE_ID=ami-06fe4639440b3ab22 # windows 2019 base\nIMAGE_ID=ami-0dd478adda4cc704d # windows 2016\nAWS_REGION=us-east-2\necho ${INSTANCE_PROFILE_ARN}\n\nKEY_NAME=aws-key\nSTR=$(TZ=EAT-8 date +%H%M)\naws ec2 run-instances \\\n    --region ${AWS_REGION} --key-name ${KEY_NAME} \\\n    --image-id ${IMAGE_ID} --instance-type m5.large \\\n    --iam-instance-profile Arn=${INSTANCE_PROFILE_ARN} \\\n    --associate-public-ip-address \\\n    --private-dns-name-options \"HostnameType=ip-name,EnableResourceNameDnsARecord=true,EnableResourceNameDnsAAAARecord=false\" \\\n    --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=win-'\"${STR}\"'},{Key=os,Value=windows}]' |tee /tmp/instance-$$.1\nINST_ID=$(cat /tmp/instance-$$.1 |jq -r '.Instances[0].InstanceId')\n# private dns name option is important for join domain\n# false/false will run ssm document failed\n# true/false will run ssm document successful\n\nwhile true ; do\n    RESULT=$(aws ec2 describe-instance-status \\\n        --instance-ids ${INST_ID} \\\n        --query 'InstanceStatuses[?(InstanceStatus.Status==`ok` &amp;&amp; SystemStatus.Status==`ok`)].InstanceId' \\\n        --output text)\n    if [[ $RESULT == \"${INST_ID}\" ]]; then\n        break\n    else\n        sleep 60\n    fi\ndone\n</code></pre></li> </ul> <p>^goacm2</p>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#join-domain-","title":"join-domain-","text":"<ul> <li>(option) logon windows instance once at least </li> <li> <p>execute ssm document to join domain <pre><code>echo ${AD}\necho ${MSDS_ID}\necho ${INST_ID}\necho ${MSDS_IP[@]}\necho ${AWS_REGION}\n\naws ssm send-command \\\n    --document-name \"AWS-JoinDirectoryServiceDomain\" \\\n    --document-version \"1\" \\\n    --targets '[{\"Key\":\"InstanceIds\",\"Values\":[\"'\"${INST_ID}\"'\"]}]' \\\n    --parameters '{\"directoryOU\":[\"\"],\"directoryId\":[\"'\"${MSDS_ID}\"'\"],\"directoryName\":[\"'\"${AD}\"'\"],\"dnsIpAddresses\":[\"'\"${MSDS_IP[0]}\"'\",\"'\"${MSDS_IP[1]}\"'\"]}' \\\n    --timeout-seconds 600 \\\n    --max-concurrency \"50\" \\\n    --max-errors \"0\" \\\n    --cloud-watch-output-config '{\"CloudWatchOutputEnabled\":true,\"CloudWatchLogGroupName\":\"ssm-powershell-log-2024\"}' \\\n    --region ${AWS_REGION} |tee /tmp/ssm-$$.json\n\n# wait to Success\nCOMMAND_ID=$(cat /tmp/ssm-$$.json |jq -r '.Command.CommandId')\nwatch -g -n 10 aws ssm get-command-invocation --command-id ${COMMAND_ID} --instance-id ${INST_ID} --query 'Status' --output text\n</code></pre></p> </li> <li> <p>check join domain successfully or not, see <code>Domain:</code> in output <pre><code>aws ssm send-command \\\n--document-name \"AWS-RunPowerShellScript\" \\\n--document-version \"1\" \\\n--targets '[{\"Key\":\"InstanceIds\",\"Values\":[\"'\"${INST_ID}\"'\"]}]' \\\n--parameters '{\"workingDirectory\":[\"\"],\"executionTimeout\":[\"3600\"],\"commands\":[\"systeminfo\"]}' \\\n--timeout-seconds 600 --max-concurrency \"50\" --max-errors \"0\" \\\n--cloud-watch-output-config '{\"CloudWatchOutputEnabled\":true,\"CloudWatchLogGroupName\":\"ssm-powershell-log-2024\"}' --region us-east-2\n</code></pre></p> </li> </ul>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#install-some-tool-to-manage-ad","title":"install some tool to manage AD","text":"<ul> <li> <p>install <code>Remote Server Administration Tools</code> from powershell <pre><code>Install-WindowsFeature RSAT-ADDS-Tools\nInstall-WindowsFeature -Name \"RSAT-AD-PowerShell\" -IncludeAllSubFeature\n</code></pre></p> </li> <li> <p>login instance with domain admin <code>admin@your.domain.com</code></p> </li> <li>create group for sftp access <code>testgroup1</code></li> <li>create user belong this group <code>testuser1</code></li> <li>get sid of ad group (link) <pre><code>Get-ADGroup -Filter {samAccountName -like \"testgroup1*\"} -Properties * | Select SamAccountName,ObjectSid\n</code></pre></li> </ul>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#transfer-family","title":"transfer family","text":"<ul> <li>allocate at least one elastic ip in your vpc</li> <li>request a certificate <code>ftp.your.domain.com</code> in acm</li> <li>create role to access s3, using transfer service as trust entity <code>access-s3-role</code></li> <li> <p>add directory service permissions to current user/role who will create ftp server (link)</p> </li> <li> <p>create server with sftp and ftps protocol</p> </li> <li>select <code>aws directory service</code> as IdP</li> <li>select <code>vpc hosted</code> as endpoint type, using <code>internet facing</code> access</li> <li>select public subnet and elastic ip</li> <li>modify or create security group for ports: <code>22,21,8192-8200</code> </li> <li>s3 as backend</li> <li> <p>select <code>enable</code> in <code>TLS session resumption</code> (link)</p> </li> <li> <p>after server created, add sid to <code>accesses</code>, mapping to role <code>access-s3-role</code></p> </li> <li>test user</li> </ul>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#route53","title":"route53","text":"<ul> <li>create cname record <code>ftp.your.domain.com</code> to ftp server\u2019s endpoint, or alias to vpce public dns name</li> </ul>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#access-ftp-server","title":"access ftp server","text":"<pre><code>sftp testuser1@your.domain.com@ftp.your.domain.com\n# or\nlftp -d ftp://ftp.your.domain.com \\\n-u 'testuser1@your.domain.com,PASSWORD'\n</code></pre>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#conclusion","title":"conclusion","text":"<p>meeting requirement</p>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#refer","title":"refer","text":"<ul> <li>https://aws.amazon.com/blogs/aws/new-aws-transfer-for-ftp-and-ftps-in-addition-to-existing-sftp/</li> <li>https://aws.amazon.com/blogs/storage/announcing-the-open-source-release-of-web-client-for-aws-transfer-family/</li> </ul>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#screenshots","title":"screenshots","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#ftp-client-configuration","title":"ftp client configuration","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#upload-and-download-using-ftp-client","title":"upload and download using ftp client","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#s3-bucket","title":"s3 bucket","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#transfer-family_1","title":"transfer family","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#directory-service_1","title":"directory service","text":"","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-mig-filezilla-to-transfer-family/#refer_1","title":"refer","text":"<ul> <li>https://repost.aws/knowledge-center/manage-ad-directory-from-ec2-windows</li> </ul>","tags":["aws/storage/transfer-family","aws/mgmt/directory-service"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/","title":"POC-prometheus-with-thanos-manually","text":"<p>[!WARNING] just backup copy  no more update here refer: TC-prometheus-ha-architect-with-thanos</p>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#poc-prometheus-with-thanos-manually","title":"POC-prometheus-with-thanos-manually","text":"","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#_1","title":"\u67b6\u6784\u63cf\u8ff0","text":"<p>Prometheus \u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u76d1\u63a7\u548c\u62a5\u8b66\u5de5\u5177\uff0c\u4e13\u4e3a\u5bb9\u5668\u5316\u548c\u4e91\u539f\u751f\u67b6\u6784\u7684\u8bbe\u8ba1\uff0c\u901a\u8fc7\u57fa\u4e8e HTTP \u7684 Pull \u6a21\u5f0f\u91c7\u96c6\u65f6\u5e8f\u6570\u636e\uff0c\u63d0\u4f9b\u529f\u80fd\u5f3a\u5927\u7684\u67e5\u8be2\u8bed\u8a00 PromQL\uff0c\u5e76\u53ef\u89c6\u5316\u5448\u73b0\u76d1\u63a7\u6307\u6807\u4e0e\u751f\u6210\u62a5\u8b66\u4fe1\u606f\u3002\u5ba2\u6237\u666e\u904d\u91c7\u7528\u5176\u7528\u4e8e Kubernetes \u7684\u76d1\u63a7\u4f53\u7cfb\u5efa\u8bbe\u3002Amazon \u4e5f\u5728 2021 \u5e74 9 \u6708\u6b63\u5f0f\u53d1\u5e03\u4e86\u6258\u7ba1\u7684 Prometheus \u670d\u52a1\uff08Amazon Managed Service for Prometheus\uff09\u7b80\u5316\u5ba2\u6237\u90e8\u7f72\u548c\u4f7f\u7528\u3002\u5e76\u4e14\u5728 2023 \u5e74 11 \u6708\u9488\u5bf9 EKS \u53d1\u5e03\u4e86\u6258\u7ba1\u7684 Prometheus \u670d\u52a1\u7684\u65e0\u4ee3\u7406\u91c7\u96c6\u529f\u80fd\uff08\u65b0\u95fb\u7a3f\uff09\uff0c\u8fdb\u4e00\u6b65\u65b9\u4fbf\u5ba2\u6237\u65e0\u9700\u63d0\u524d\u89c4\u5212\uff0c\u4ece\u800c\u53ef\u4ee5\u5f00\u7bb1\u5373\u7528\u7684\u4f7f\u7528 Prometheus \u7684\u76f8\u5173\u7ec4\u4ef6\u3002</p> <p>\u622a\u6b62\u672c\u6587\u53d1\u5e03\u4e4b\u65e5\uff0c\u5728\u4e9a\u9a6c\u900a\u4e2d\u56fd\u533a\u57df\u6682\u65f6\u6ca1\u6709\u53d1\u5e03\u6258\u7ba1\u7684 Prometheus \u670d\u52a1\u3002\u56e0\u6b64\u9488\u5bf9\u4e2d\u56fd\u5ba2\u6237\u5982\u4f55\u90e8\u7f72\u4e00\u5957 Prometheus \u76d1\u63a7\u5e73\u53f0\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5bfc\u53ef\u4ee5\u5e2e\u52a9\u5ba2\u6237\u5feb\u901f\u4f7f\u7528 Prometheus\uff0c\u4ece\u800c\u5c06\u7cbe\u529b\u4e13\u6ce8\u4e8e\u4e1a\u52a1\u9700\u6c42\u3002</p> <p>\u72ec\u7acb Kubernetes \u96c6\u7fa4\u901a\u5e38\u4f7f\u7528 Prometheus Operator \u90e8\u7f72\u6240\u6709\u76f8\u5173\u7ec4\u4ef6\u5305\u62ec Alert Manager\u3001Grafana \u7b49\u3002\u8fd9\u79cd\u72ec\u7acb\u90e8\u7f72\u7684\u76d1\u63a7\u67b6\u6784\u4f18\u70b9\u662f\u90e8\u7f72\u65b9\u4fbf\uff0c\u6570\u636e\u6301\u4e45\u5316\u4f7f\u7528 EBS \u4e5f\u53ef\u4ee5\u6ee1\u8db3\u5927\u90e8\u5206\u573a\u666f\u4e0b\u67e5\u8be2\u6027\u80fd\u7684\u8981\u6c42\uff0c\u4f46\u7f3a\u70b9\u4e5f\u663e\u800c\u6613\u89c1\uff0c\u5373\u65e0\u6cd5\u4fdd\u5b58\u592a\u957f\u65f6\u95f4\u7684\u5386\u53f2\u6570\u636e\u3002\u800c\u4e14\u5f53\u5ba2\u6237\u73af\u5883\u4e2d\u96c6\u7fa4\u6570\u91cf\u8f83\u591a\uff0c\u76d1\u63a7\u5e73\u53f0\u81ea\u8eab\u7684\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u8981\u6c42\u8f83\u9ad8\uff0c\u540c\u65f6\u5e0c\u671b\u63d0\u4f9b\u5168\u5c40\u67e5\u8be2\u65f6\uff0c\u7ba1\u7406\u548c\u7ef4\u62a4\u7684\u5de5\u4f5c\u91cf\u4e5f\u968f\u4e4b\u589e\u52a0\u3002</p> <p>Thanos\u662f\u4e00\u5957\u5f00\u6e90\u7ec4\u4ef6\uff0c\u6784\u5efa\u5728 Prometheus \u4e4b\u4e0a\uff0c\u7528\u4ee5\u89e3\u51b3 Prometheus \u5728\u591a\u96c6\u7fa4\u5927\u89c4\u6a21\u73af\u5883\u4e0b\u7684\u9ad8\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002\u5f53\u9700\u8981\u505a\u5386\u53f2\u6027\u80fd\u6570\u636e\u5206\u6790\uff0c\u6216\u8005\u4f7f\u7528 Prometheus \u8fdb\u884c\u6210\u672c\u5206\u6790\u7684\u573a\u666f\u90fd\u4f1a\u4f9d\u8d56\u4e8e\u8f83\u957f\u65f6\u95f4\u7684\u5386\u53f2\u6570\u636e\u3002Thanos \u4e3b\u8981\u901a\u8fc7\u63a5\u6536\u5e76\u5b58\u50a8 Prometheus \u7684\u591a\u96c6\u7fa4\u6570\u636e\u526f\u672c\uff0c\u5e76\u63d0\u4f9b\u5168\u5c40\u67e5\u8be2\u548c\u4e00\u81f4\u6027\u6570\u636e\u8bbf\u95ee\u63a5\u53e3\u7684\u65b9\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e8e Prometheus \u7684\u53ef\u9760\u6027\u3001\u4e00\u81f4\u6027\u548c\u53ef\u7528\u6027\u4fdd\u969c\uff0c\u4ece\u800c\u89e3\u51b3\u4e86 Prometheus \u5355\u96c6\u7fa4\u5728\u5b58\u50a8\u3001\u67e5\u8be2\u5386\u53f2\u6570\u636e\u548c\u5907\u4efd\u7b49\u65b9\u9762\u7684\u6269\u5c55\u6027\u6311\u6218\u3002</p> <p>\u5728\u8ba8\u8bba\u57fa\u4e8e Thanos \u7684\u5404\u79cd Prometheus \u76d1\u63a7\u67b6\u6784\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u4e86\u89e3\u4e0b Thanos \u53ca\u5176\u5e38\u7528\u7684\u7ec4\u4ef6\uff0c\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u53c2\u8003 thanos.io \u3002</p> <ul> <li>Sidecar\uff08\u8fb9\u8f66\uff09\uff1a\u8fd0\u884c\u5728 Prometheus \u7684 Pod \u4e2d\uff0c\u8bfb\u53d6\u5176\u6570\u636e\u4ee5\u4f9b\u67e5\u8be2\u548c/\u6216\u4e0a\u4f20\u5230\u4e91\u5b58\u50a8\u3002</li> <li>Store\uff08\u5b58\u50a8\u7f51\u5173\uff09\uff1a\u7528\u4e8e\u4ece\u5bf9\u8c61\u5b58\u50a8\u6876\uff08\u4f8b\u5982\uff1aAWS S3\uff09\u4e0a\u67e5\u8be2\u6570\u636e\u3002</li> <li>Compactor\uff08\u538b\u7f29\u5668)\uff1a\u5bf9\u5b58\u50a8\u5728\u5bf9\u8c61\u5b58\u50a8\u6876\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u538b\u7f29\u3001\u805a\u5408\u5386\u53f2\u6570\u636e\u4ee5\u51cf\u5c0f\u91c7\u6837\u7cbe\u5ea6\u5e76\u957f\u4e45\u4fdd\u7559\u3002</li> <li>Receive\uff08\u63a5\u6536\u5668\uff09\uff1a\u63a5\u6536\u6765\u81ea Prometheus \u8fdc\u7a0b\u5199\u5165\u65e5\u5fd7\u7684\u6570\u636e\uff0c\u5e76\u5c06\u5176\u4e0a\u4f20\u5230\u5bf9\u8c61\u5b58\u50a8\u3002</li> <li>Ruler\uff08\u89c4\u5219\u5668\uff09\uff1a\u9488\u5bf9 Thanos \u4e2d\u7684\u6570\u636e\u8bc4\u4f30\u8bb0\u5f55\u548c\u8b66\u62a5\u89c4\u5219\u3002</li> <li>Query\uff08\u67e5\u8be2\u5668\uff09\uff1a\u5b9e\u73b0 Prometheus \u7684 v1 API\uff0c\u67e5\u8be2\u5e76\u6c47\u603b\u6765\u81ea\u5e95\u5c42\u7ec4\u4ef6\u7684\u6570\u636e\u3002\u5c06\u6240\u6709\u6570\u636e\u6e90\u6dfb\u52a0\u4e3a Query \u7684 Endpoint\uff0c\u5305\u62ec Sidecar\u3001 Store\u3001 Receive \u7b49\u3002</li> <li>Query Frontend\uff08\u67e5\u8be2\u524d\u7aef\uff09\uff1a\u5b9e\u73b0 Prometheus \u7684 v1 API\uff0c\u5c06\u5176\u4ee3\u7406\u7ed9\u67e5\u8be2\u5668\uff0c\u540c\u65f6\u7f13\u5b58\u54cd\u5e94\uff0c\u5e76\u53ef\u4ee5\u62c6\u5206\u67e5\u8be2\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002</li> </ul> <p>\u7b2c\u4e00\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0b\u56fe\u84dd\u8272\u548c\u7eff\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a</p> <ul> <li>\u88ab\u76d1\u63a7\u96c6\u7fa4\uff08Observee\uff09\u90e8\u7f72 Prometheus \u4e14\u542f\u7528 Thanos \u7684 Sidecar \u65b9\u5f0f\u5c06\u76d1\u63a7\u7684\u5386\u53f2\u6570\u636e\u5b9a\u671f\u5f52\u6863\u5230 S3\uff0c\u901a\u8fc7\u90e8\u7f72 Thanos Store \u7ec4\u4ef6\u67e5\u8be2\u5386\u53f2\u6570\u636e\uff08\u4e0b\u56fe\u4e2d Store \u7ec4\u4ef6\u90e8\u7f72\u5728\u76d1\u63a7\u96c6\u7fa4\u4e2d\uff09\uff0c\u88ab\u76d1\u63a7\u96c6\u7fa4\u4e2d\u4e0d\u542f\u7528 Grafana \u7ec4\u4ef6\uff1b</li> <li>\u76d1\u63a7\u96c6\u7fa4\uff08Observer\uff09\u9664\u4e86\u90e8\u7f72 Prometheus \u4e4b\u5916\uff0c\u5c06\u7edf\u4e00\u90e8\u7f72 Grafana \u4f5c\u4e3a Dashboard \u5c55\u793a\u3002</li> </ul> <p>\u7b2c\u4e8c\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0b\u56fe\u7ea2\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09:</p> <ul> <li>\u88ab\u76d1\u63a7\u96c6\u7fa4\uff08Observee\uff09\u9664\u4e86\u542f\u7528 Thanos Sidecar \u4e4b\u5916\uff0c\u8fd8\u542f\u7528\u4e86 Prometheus \u7684 Remote Write \u529f\u80fd\uff0c\u5c06\u672a\u5f52\u6863\u7684\u6570\u636e\u4ee5 WAL \u65b9\u5f0f\u8fdc\u7a0b\u4f20\u8f93\u5230\u90e8\u7f72\u5728\u76d1\u63a7\u96c6\u7fa4\uff08Observer\uff09\u4e0a\u7684 Thanos Receive\uff0c\u4ee5\u4fdd\u8bc1\u6570\u636e\u7684\u5197\u4f59\u5ea6\u3002 Thanos Receive \u540c\u6837\u53ef\u4ee5\u5c06\u5386\u53f2\u76d1\u63a7\u6570\u636e\u5f52\u6863\u5230 S3 \u4e0a\uff0c\u4e14\u652f\u6301\u88ab Thanos Query \u76f4\u63a5\u67e5\u8be2\uff0c\u540c\u65f6\u907f\u514d\u76f4\u63a5\u67e5\u8be2 Sidecar \u800c\u7ed9\u88ab\u76d1\u63a7\u96c6\u7fa4\u5e26\u6765\u989d\u5916\u7684\u6027\u80fd\u635f\u8017\u3002</li> </ul> <p>\u7b2c\u4e09\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0b\u56fe\u9ec4\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a</p> <ul> <li>\u5728\u591a\u96c6\u7fa4\u76d1\u63a7\u573a\u666f\u4e0b\uff0c\u4e00\u822c\u4f1a\u5728\u6bcf\u4e2a\u96c6\u7fa4\u90e8\u7f72\u72ec\u7acb\u7684 Prometheus \u7ec4\u4ef6\u3002Prometheus \u63d0\u4f9b Agent Mode \u9488\u5bf9\u8fd9\u6837\u7684\u573a\u666f\u53ef\u4ee5\u6700\u5c0f\u5316\u8d44\u6e90\u5360\u7528\uff0c\u76f4\u63a5\u542f\u7528 Remote Write \u529f\u80fd\u5c06\u76d1\u63a7\u6570\u636e\u96c6\u4e2d\u4fdd\u5b58 \uff08\u53ef\u4ee5\u662f\u53e6\u4e00\u4e2a Prometheus \u96c6\u7fa4\uff0c\u6216\u8005 Thanos Receive \u7ec4\u4ef6\uff09\u3002\u5728 AWS \u4e0a\u53ef\u4ee5\u4f7f\u7528\u6258\u7ba1\u7684 Prometheus \u670d\u52a1\u4f5c\u4e3a\u96c6\u4e2d\u76d1\u63a7\u6570\u636e\u6301\u4e45\u5316\uff0c\u63d0\u4f9b\u6700\u597d\u7684\u6027\u80fd\u548c\u6700\u4f4e\u7684\u7ef4\u62a4\u6210\u672c\u3002</li> </ul> <p></p> <p>\u4ee5\u4e0b\u603b\u7ed3\u4e86\u57fa\u4e8e Thanos \u7684\u5404\u79cd Prometheus \u76d1\u63a7\u67b6\u6784\u6240\u9002\u5408\u7684\u573a\u666f\uff1a \u7b2c\u4e00\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0a\u56fe\u84dd\u8272\u548c\u7eff\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a\u9002\u5408\u7edd\u5927\u90e8\u5206\u751f\u4ea7\u73af\u5883\uff0c\u5c24\u5176\u5728\u4e9a\u9a6c\u900a\u4e2d\u56fd\u533a\u57df\u6ca1\u6709\u6258\u7ba1 Prometheus \u670d\u52a1\uff0c\u6b64\u7c7b\u67b6\u6784\u4e5f\u662f\u5ba2\u6237\u9996\u9009\u3002</p> <ul> <li>\u4f18\u70b9<ul> <li>\u67b6\u6784\u7b80\u5355</li> <li>\u53ea\u6709\u4e00\u4efd\u76d1\u63a7\u6570\u636e\uff0c\u6700\u5c0f\u5316\u5b58\u50a8\u6210\u672c\u548c\u5176\u4ed6\u8d44\u6e90\u5f00\u9500</li> </ul> </li> <li>\u7f3a\u70b9 <ul> <li>\u901a\u8fc7 Sidecar \u67e5\u8be2\u5b9e\u65f6\u76d1\u63a7\u6570\u636e\u65f6\uff0c\u5c06\u7ed9\u88ab\u76d1\u63a7\u96c6\u7fa4\u5e26\u6765\u989d\u5916\u6027\u80fd\u635f\u8017</li> </ul> </li> </ul> <p>\u7b2c\u4e8c\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0a\u56fe\u7ea2\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a\u5728 Thanos 0.19 \u7248\u672c\u4e4b\u524d\uff0cSidecar \u8fd8\u6ca1\u6709\u5b9e\u73b0 StoreAPI \u65f6\uff0c\u53ea\u80fd\u901a\u8fc7 Receive \u67e5\u8be2\u6700\u65b0\u7684\u6027\u80fd\u6570\u636e\u3002\u9002\u5408\u9700\u8981\u591a\u526f\u672c\u76d1\u63a7\u6570\u636e\u7684\u7279\u6b8a\u573a\u666f\u3002</p> <ul> <li>\u4f18\u70b9<ul> <li>\u76d1\u63a7\u6570\u636e\u5197\u4f59\uff0c\u53ef\u4ee5\u4f7f\u7528 Compactor \u5bf9\u6570\u636e\u8fdb\u884c\u538b\u7f29\u3001\u805a\u5408\u5386\u53f2\u6570\u636e\u4ee5\u51cf\u5c11\u5b58\u50a8\u6210\u672c</li> <li>\u76f4\u63a5\u4ece Thanos Receive \u67e5\u8be2\u5b9e\u65f6\u6027\u80fd\u6570\u636e\uff0c\u5bf9\u88ab\u76d1\u63a7\u96c6\u7fa4\u6ca1\u6709\u989d\u5916\u6027\u80fd\u635f\u8017</li> </ul> </li> <li>\u7f3a\u70b9<ul> <li>\u67b6\u6784\u590d\u6742\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u5bf9\u5e94\u4e00\u7ec4 Thanos Receive\uff0c\u5efa\u8bae\u914d\u7f6e\u526f\u672c\u6570\u91cf\u548c\u8d44\u6e90\u7b49\u4e0e\u6e90\u96c6\u7fa4\u7684 Prometheus \u526f\u672c\u6570\u91cf\u548c\u8d44\u6e90\u76f8\u540c</li> </ul> </li> </ul> <p>\u7b2c\u4e09\u79cd\u76d1\u63a7\u67b6\u6784\uff08\u5bf9\u5e94\u4e0a\u56fe\u9ec4\u8272\u96c6\u7fa4\u53ca\u7ec4\u4ef6\uff09\uff1a\u6700\u5927\u7a0b\u5ea6\u51cf\u5c11\u5bf9\u4e8e\u88ab\u76d1\u63a7\u96c6\u7fa4\u7684\u8d44\u6e90\u5360\u7528\uff0c\u53ef\u4ee5\u4f7f\u7528 Prometheus \u7684 Agent Mode\uff1a</p> <ul> <li>\u4f18\u70b9<ul> <li>\u67b6\u6784\u7b80\u5355\uff0c\u4f7f\u7528 Agent Mode \u51e0\u4e4e\u65e0\u72b6\u6001\uff0c\u53ef\u4ee5\u4f7f\u7528\u9664 Stateful \u4e4b\u5916\u7684\u5176\u4ed6 Deployment\uff0c\u672c\u5730\u5b58\u50a8\u9700\u6c42\u4f4e\uff08\u9664\u975e\u8fdc\u7a0b Endpoint \u4e0d\u53ef\u7528\u65f6\uff0c\u672c\u5730\u7f13\u5b58\u6570\u636e\u4ee5\u91cd\u8bd5\uff09</li> <li>\u53ef\u5b9e\u73b0\u96c6\u4e2d\u544a\u8b66 - \u544a\u8b66\u5c06\u901a\u8fc7 Thanos Ruler \u5b9a\u4e49\uff0c\u901a\u8fc7 Thanos Query \u67e5\u8be2 Receive \u5e76\u53d1\u9001\u5230\u76d1\u63a7\u96c6\u7fa4\u7684 Alert Manager \u5b9e\u73b0</li> </ul> </li> <li>\u7f3a\u70b9 <ul> <li>\u65e0\u76d1\u63a7\u6570\u636e\u5197\u4f59\uff0c\u67d0\u4e9b\u7ec4\u4ef6\u5c06\u65e0\u6cd5\u5728 Agent Mode \u4e0b\u542f\u7528\uff0c\u4f8b\u5982\uff1aSidecar\u3001Alert\u3001Rules \uff08\u53c2\u89c1\u6587\u6863\uff09</li> </ul> </li> </ul> <p>\u5728\u652f\u6301\u6258\u7ba1 Prometheus \u670d\u52a1\u7684\u4e9a\u9a6c\u900a\u533a\u57df\uff08\u6587\u6863\uff09\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u6258\u7ba1\u670d\u52a1\u66ff\u4ee3\u7b2c\u4e09\u79cd\u76d1\u63a7\u67b6\u6784\uff0c\u5b9e\u73b0\u5b8c\u5168\u5f00\u7bb1\u5373\u7528\uff0c\u540c\u65f6\u907f\u514d\u7ba1\u7406 Thanos \u7ec4\u4ef6\uff0c\u53ea\u9700\u8981\u90e8\u7f72 Grafana \u5373\u53ef\u3002</p>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#walkthrough","title":"walkthrough","text":"<p>Prometheus Operator \u63d0\u4f9b Kubernetes \u539f\u751f\u90e8\u7f72\u548c\u7ba1\u7406 Prometheus \u53ca\u76f8\u5173\u76d1\u63a7\u7ec4\u4ef6\u7684\u529f\u80fd\u3002\u8be5\u9879\u76ee\u7684\u76ee\u7684\u662f\u7b80\u5316\u548c\u81ea\u52a8\u914d\u7f6e Kubernetes \u96c6\u7fa4\u57fa\u4e8e Prometheus \u7684\u76d1\u63a7\u5806\u6808\u3002\u672c\u5b9e\u9a8c\u57fa\u4e8e Prometheus Operator \u90e8\u7f72\u4f5c\u4e3a\u57fa\u7840\uff0c\u5e76\u901a\u8fc7 values.yaml \u53c2\u6570\u6587\u4ef6\u5b9a\u5236\uff0c\u8be6\u7ec6\u4fe1\u606f\u53c2\u89c1\uff08Github\uff09\u3002\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u521b\u5efa 3 \u4e2a EKS \u96c6\u7fa4\uff0c\u5206\u522b\u5bf9\u5e94\u4e0a\u56fe\u4e2d\u7684\u84dd\u8272\u3001\u7ea2\u8272\u3001\u9ec4\u8272\u96c6\u7fa4\u9a8c\u8bc1 Thanos \u76f8\u5173\u914d\u7f6e\u3002 </p> <p>\u672c\u5b9e\u9a8c\u4e2d\u5c06\u4f7f\u7528 Terraform \u5feb\u901f\u521b\u5efa EKS \u96c6\u7fa4\uff0c\u5e76\u4e14\u81ea\u52a8\u90e8\u7f72\u4e0a\u56fe\u4e2d\u76f8\u5173\u7684 Prometheus \u76d1\u63a7\u67b6\u6784\uff08Github\uff09\uff0c\u548cThanos \u76f8\u5173\u7ec4\u4ef6\uff08Github\uff09\uff0c\u5e26\u5927\u5bb6\u4e86\u89e3 Thanos \u7684\u914d\u7f6e\u548c\u5de5\u4f5c\u539f\u7406\u3002</p>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#prometheus","title":"prometheus","text":"<ul> <li>we will create 3 clusters, <code>ekscluster1</code> for observer, <code>ekscluster2</code> and <code>ekscluster3</code> for observee (../EKS/cluster/eks-cluster-with-terraform)</li> <li>following addons will be included in each cluster<ul> <li>argocd</li> <li>aws load balancer controller </li> <li>ebs csi </li> <li>externaldns-for-route53 </li> <li>metrics-server</li> <li>cluster-autoscaler</li> <li><code>DOMAIN_NAME</code> should be <code>environment_name.hosted_zone_name</code>, for example <code>thanos.eks1217.aws.panlm.xyz</code>. Use it in following lab.</li> </ul> </li> <li>get sample yaml  <pre><code>git clone https://github.com/panlm/thanos-example.git\ncd thanos-example\n</code></pre></li> <li> <p>following<code>README.md</code> to build your version yaml files <pre><code>CLUSTER_NAME_1=ekscluster1\nCLUSTER_NAME_2=ekscluster2\nCLUSTER_NAME_3=ekscluster3\nDOMAIN_NAME=thanos.eks1217.aws.panlm.xyz\nTHANOS_BUCKET_NAME=thanos-store-eks1217\nAWS_DEFAULT_REGION=us-east-2\nexport CLUSTER_NAME_1 CLUSTER_NAME_2 CLUSTER_NAME_3 DOMAIN_NAME THANOS_BUCKET_NAME AWS_DEFAULT_REGION\n\nmkdir POC\ncd POC-template\nfind ./ -type d -name \"[a-z]*\" -exec mkdir ../POC/{} \\;\n\nfind ./ -type f -name \"*\" |while read filename ; do\n  cat $filename |envsubst &gt; ../POC/$filename\ndone\n\ncd ../POC/\n</code></pre></p> </li> <li> <p>prepare to install thanos with helm <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\nhelm show values prometheus-community/kube-prometheus-stack &gt; values_default.yaml\ncat values_default.yaml |grep adminPassword\n</code></pre></p> </li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#observer-cluster","title":"observer cluster","text":"<ul> <li>switch to observer (ekscluster1) <pre><code>kubectx ekscluster1\n</code></pre></li> <li> <p>create s3 config file for thanos sidecar <pre><code>DEPLOY_NAME_1=prom-operator-${CLUSTER_NAME_1}\nNAMESPACE_NAME=monitoring\n\nkubectl create ns ${NAMESPACE_NAME}\nkubectl create secret generic thanos-s3-config-${CLUSTER_NAME_1} --from-file=thanos-s3-config-${CLUSTER_NAME_1}=s3-config/thanos-s3-config-${CLUSTER_NAME_1}.yaml --namespace ${NAMESPACE_NAME} \n</code></pre></p> </li> <li> <p>deploy Prometheus with Thanos and Grafana <pre><code>echo ${CLUSTER_NAME_1} ${DEPLOY_NAME_1} ${NAMESPACE_NAME}\nhelm upgrade -i -f prometheus/values-${CLUSTER_NAME_1}-1.yaml -f prometheus/values-${CLUSTER_NAME_1}-2.yaml ${DEPLOY_NAME_1} prometheus-community/kube-prometheus-stack --namespace ${NAMESPACE_NAME}\n</code></pre></p> </li> <li> <p>create irsa in monitoring namespace for thanos (git/git-mkdocs/CLI/linux/eksctl) <pre><code>echo ${DEPLOY_NAME_1}\necho ${CLUSTER_NAME_1}\nSA_NAME=${DEPLOY_NAME_1}-prometheus\ncreate-iamserviceaccount -s ${SA_NAME} -c ${CLUSTER_NAME_1} -n monitoring -r 0\n</code></pre></p> </li> <li> <p>rollout statefulset (or using k9s to delete pod and make it restart to use new SA) <pre><code>kubectl rollout restart sts prometheus-prom-operator-${CLUSTER_NAME_1}-prometheus -n monitoring\n</code></pre></p> </li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#observee-cluster-","title":"observee-cluster-","text":"<ul> <li>switch to observee cluster (ekscluster2) <pre><code>kubectx ekscluster2\n</code></pre></li> <li> <p>on observee cluster (ekscluster2) <pre><code>DEPLOY_NAME_2=prom-operator-${CLUSTER_NAME_2}\nNAMESPACE_NAME=monitoring\n\nkubectl create ns ${NAMESPACE_NAME}\nkubectl create secret generic thanos-s3-config-${CLUSTER_NAME_2} --from-file=thanos-s3-config-${CLUSTER_NAME_2}=s3-config/thanos-s3-config-${CLUSTER_NAME_2}.yaml --namespace ${NAMESPACE_NAME}\n</code></pre></p> </li> <li> <p>deploy Prometheus with remote write and Thanos Sidecar, no Grafana <pre><code>echo ${CLUSTER_NAME_2} ${DEPLOY_NAME_2} ${NAMESPACE_NAME}\nhelm upgrade -i -f prometheus/values-${CLUSTER_NAME_2}-1.yaml -f prometheus/values-${CLUSTER_NAME_2}-2.yaml ${DEPLOY_NAME_2} prometheus-community/kube-prometheus-stack --namespace ${NAMESPACE_NAME}\n</code></pre></p> </li> <li> <p>using remote write, WAL log will be transfer to receive pod, you could query real time data from thanos receive.</p> </li> <li> <p>create irsa in monitoring namespace for thanos (git/git-mkdocs/CLI/linux/eksctl) <pre><code>echo ${DEPLOY_NAME_2}\necho ${CLUSTER_NAME_2}\nSA_NAME=${DEPLOY_NAME_2}-prometheus\ncreate-iamserviceaccount -s ${SA_NAME} -c ${CLUSTER_NAME_2} -n monitoring -r 0\n</code></pre></p> </li> <li> <p>rollout statefulset (or using k9s to delete pod and make it restart to use new SA) <pre><code>kubectl rollout restart sts prometheus-prom-operator-${CLUSTER_NAME_2}-prometheus -n monitoring\n</code></pre></p> </li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#observee-cluster-with-prometheus-agent-mode","title":"observee cluster with prometheus agent mode","text":"<ul> <li>switch to observee cluster (ekscluster3) <pre><code>kubectx ekscluster3\n</code></pre></li> <li> <p>on observee cluster (ekscluster3) <pre><code>DEPLOY_NAME_3=prom-operator-${CLUSTER_NAME_3}\nNAMESPACE_NAME=monitoring\n\nkubectl create ns ${NAMESPACE_NAME}\nkubectl create secret generic thanos-s3-config-${CLUSTER_NAME_3} --from-file=thanos-s3-config-${CLUSTER_NAME_3}=s3-config/thanos-s3-config-${CLUSTER_NAME_3}.yaml --namespace ${NAMESPACE_NAME}\n</code></pre></p> </li> <li> <p>deploy prometheus in agent mode with remote write <pre><code>echo ${CLUSTER_NAME_3} ${DEPLOY_NAME_3} ${NAMESPACE_NAME}\nhelm upgrade -i -f prometheus/values-${CLUSTER_NAME_3}-1.yaml ${DEPLOY_NAME_3} prometheus-community/kube-prometheus-stack --namespace ${NAMESPACE_NAME}\n</code></pre></p> </li> <li> <p>create irsa in monitoring namespace for thanos (git/git-mkdocs/CLI/linux/eksctl) <pre><code>echo ${DEPLOY_NAME_3}\necho ${CLUSTER_NAME_3}\nSA_NAME=${DEPLOY_NAME_3}-prometheus\ncreate-iamserviceaccount -s ${SA_NAME} -c ${CLUSTER_NAME_3} -n monitoring -r 0\n</code></pre></p> </li> <li> <p>rollout statefulset (need to delete pod and make it restart to use new SA) <pre><code>kubectl rollout restart sts prometheus-prom-operator-${CLUSTER_NAME_3}-prometheus -n monitoring\n</code></pre></p> </li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#thanos","title":"thanos","text":"<ul> <li>switch to observer cluster (ekscluster1), we will install all Thanos components on Observer cluster <pre><code>kubectx ekscluster1\n</code></pre></li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#store","title":"store","text":"<ul> <li>reuse 3 cluster s3 config file for thanos store on observer <pre><code>kubectl create ns thanos\nfor CLUSTER_NAME in ekscluster1 ekscluster2 ekscluster3 ; do\n    kubectl create secret generic thanos-s3-config-${CLUSTER_NAME} --from-file=thanos-s3-config-${CLUSTER_NAME}=./s3-config/thanos-s3-config-${CLUSTER_NAME}.yaml -n thanos\ndone\n</code></pre></li> <li>create thanos store for history data query <pre><code>kubectl apply -f store/\n</code></pre></li> <li>create role for sa (git/git-mkdocs/CLI/linux/eksctl) and annotate to existed sa <pre><code>for SA_NAME in thanos-store-cluster1 thanos-store-cluster2 thanos-store-cluster3 ; do\n    create-iamserviceaccount -s ${SA_NAME} -c ${CLUSTER_NAME_1} -n thanos -r 0\ndone\n</code></pre></li> <li>rollout 2 stores (or using k9s to delete pod and make it restart to use new SA) <pre><code>for i in thanos-store-cluster1 thanos-store-cluster2 thanos-store-cluster3 ; do\n    kubectl rollout restart sts $i -n thanos\ndone\n</code></pre></li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#query-and-query-frontend-","title":"query-and-query-frontend-","text":"<ul> <li>In query deployment yaml file, all endpoints we needed in this POC will be added to container\u2019s args, including sidecar, receive, store, etc.</li> <li>In query frontend service yaml file, it will bind domain name</li> <li>In query frontend deployment yaml file, using split parameters to improve query performance  <pre><code>        - --query-range.split-interval=1h\n        - --labels.split-interval=1h\n</code></pre></li> <li>deploy <pre><code>kubectl apply -f query/\n</code></pre></li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#receive","title":"receive","text":"<ul> <li>use existed s3 config file in secret</li> <li>deploy 2 receives, one for ekscluster2 and another for ekscluster3 <pre><code>kubectl apply -f receive/\n</code></pre></li> <li>create irsa in thanos namespace for receive (git/git-mkdocs/CLI/linux/eksctl) <pre><code>for SA_NAME in thanos-receive-cluster2 thanos-receive-cluster3 ; do\n    create-iamserviceaccount -s ${SA_NAME} -c ${CLUSTER_NAME_1} -n thanos -r 0\ndone\n</code></pre></li> <li>rollout 2 receives (or using k9s to delete pod and make it restart to use new SA) <pre><code>for i in thanos-receive-cluster2 thanos-receive-cluster3 ; do\n    kubectl rollout restart sts ${i} -n thanos\ndone\n</code></pre></li> <li>(option) get receive svc domain name to: <ul> <li>add it to prometheus remote write in ekscluster2 and ekscluster3 (git/git-mkdocs/EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos)</li> <li>add it to query deployment yaml (git/git-mkdocs/EKS/solutions/monitor/TC-prometheus-ha-architect-with-thanos)</li> </ul> </li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#grafana","title":"Grafana","text":"<p>\u4f7f\u7528 Prometheus Operator \u90e8\u7f72 Grafana \u5c06\u81ea\u5e26\u4e00\u4e9b\u5e38\u7528\u7684 Dashboard\uff0c\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c\u7b80\u5355\u914d\u7f6e\u5b9e\u73b0\u591a\u96c6\u7fa4\u6570\u636e\u67e5\u8be2\u3002\u6b64\u5904\u63d0\u4f9b\u4e00\u4e2a sample \u53ef\u4ee5\u76f4\u63a5\u5bfc\u5165\u4f7f\u7528\u3002</p>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#query-history-metrics","title":"query history metrics","text":"<ul> <li>\u8bbf\u95ee Grafana \u57df\u540d\uff0c\u53ef\u4ee5\u901a\u8fc7 <code>thanos-example/POC/prometheus/values-ekscluster1-1.yaml</code> \u4e2d\u67e5\u770b</li> <li>\u4fee\u6539 Grafana \u9ed8\u8ba4\u5bc6\u7801 </li> <li>\u6dfb\u52a0 Thanos Query Frontend \u4f5c\u4e3a Prometheus  \u7c7b\u578b\u7684\u6570\u636e\u6e90<ul> <li>\u76f4\u63a5\u4f7f\u7528 Kubernetes \u5185\u90e8\u57df\u540d: http://thanos-query-frontend.thanos.svc.cluster.local:9090</li> <li>\u6216\u8005\u4e0a\u6587\u63d0\u5230\u7684 Query Frontend Service \u7ed1\u5b9a\u7684\u57df\u540d\u8bbf\u95ee</li> </ul> </li> <li>go this dashboard <code>Kubernetes / Networking / Namespace (Pods)</code> </li> <li>we have history data, but no latest 2 hour metrics</li> <li>go to query deployment to add thanos sidecar svc (<code>xxx-kub-thanos-external</code>) to endpoint list with port <code>10901</code></li> <li>query again from grafana, we will get full metrics</li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#query-by-label-cluster-prefer","title":"query by label cluster (prefer)","text":"<ul> <li>modify existed variable to use cluster label<ul> <li>no need to change dashboard definitions  </li> </ul> </li> <li>we already label data in prometheus yaml and receive yaml with <code>cluster=my_cluster_name</code></li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#query-by-externallabels-alternative","title":"query by externalLabels (alternative)","text":"<ul> <li>custom dashboard </li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#others","title":"others","text":"<ul> <li>\u5237\u65b0 receive \u6570\u636e\u65f6\u6296\u52a8\u4e25\u91cd<ul> <li>\u68c0\u67e5\u662f\u5426\u591a\u526f\u672c receive sts\uff0c\u4e14\u672a\u505a\u6570\u636e replica</li> </ul> </li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#thanos-frontend","title":"thanos frontend","text":"<ul> <li>open svc of thanos frontend: <code>thanos-query-frontend.${DOMAIN_NAME}</code><ul> <li>min time in receive table: means prometheus remote write has valid and data has been received by thanos receive</li> <li>min time in sidecar table: data in thanos local before duration, 2 hr will write data from WAL to duration, if &lt; 2hrs \u201c-\u201d will display. if over 2hrs, oldest data in local will be display</li> <li>min time in store table: data has been store to s3, check labelset to identify data was written by receive or sidecar</li> </ul> </li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#refer","title":"refer","text":"","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#prometheus-tsdb-block-duration","title":"prometheus tsdb block duration","text":"<ul> <li>change block-duration, will cause prometheus statefulset cannot be start<ul> <li>https://github.com/prometheus-operator/prometheus-operator/issues/4414 <pre><code>  prometheusSpec:\n    additionalArgs: \n    - name: storage.tsdb.min-block-duration\n      value: 30m\n    - name: storage.tsdb.max-block-duration\n      value: 30m\n</code></pre></li> </ul> </li> <li>if using Thanos sidecar, <code>max-block-duration</code> will be <code>2h</code></li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#samples","title":"samples","text":"","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#thanos-config-sample-in-this-poc","title":"thanos config sample in this POC","text":"<ul> <li>https://github.com/panlm/thanos-example</li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#grafana-ingress-with-alb-sample","title":"grafana ingress with alb sample","text":"<ul> <li><code>DOMAIN_NAME</code> should be <code>environment_name.hosted_zone_name</code>, for example <code>thanos.eks1217.aws.panlm.xyz</code> <pre><code>grafana:\n  enabled: true\n  deploymentStrategy:\n    type: Recreate\n  service:\n    type: NodePort\n  ingress:\n    enabled: true\n    annotations:\n      kubernetes.io/ingress.class: alb\n      alb.ingress.kubernetes.io/scheme: internet-facing\n    hosts:\n      - grafana-${DOMAIN_NAME%%.*}.${DOMAIN_NAME}\n</code></pre></li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#grafana-ingress-with-nginx-sample","title":"grafana ingress with nginx sample","text":"<pre><code>envsubst &gt;${TMP}-1.yaml &lt;&lt;-EOF\ngrafana:\n  deploymentStrategy:\n    type: Recreate\n  ingress:\n    enabled: true\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    hosts:\n\n      - grafana.${DOMAIN_NAME}\n    tls:\n      - secretName: grafana.${DOMAIN_NAME}\n        hosts:\n          - grafana.${DOMAIN_NAME}\n  persistence:\n    enabled: true\n    storageClassName: gp2\n    accessModes:\n      - ReadWriteOnce\n    size: 1Gi\nprometheus:\n  prometheusSpec:\n    replicas: 2\n    retention: 12h\n    retentionSize: \"10GB\"\n    ruleSelectorNilUsesHelmValues: false\n    serviceMonitorSelectorNilUsesHelmValues: false\n    podMonitorSelectorNilUsesHelmValues: false\n    storageSpec:\n      volumeClaimTemplate:\n        spec:\n          storageClassName: gp2\n          accessModes: [\"ReadWriteOnce\"]\n          resources:\n            requests:\n              storage: 10Gi\nEOF\n</code></pre>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#thanos-ingress-with-nginx-sample","title":"thanos ingress with nginx sample","text":"<pre><code># enable prometheus with thanos\nenvsubst &gt;${TMP##*.}-1-1.yaml &lt;&lt;-EOF\nprometheus:\n  thanosService:\n    enabled: true\n  thanosServiceMonitor:\n    enabled: true\n  thanosServiceExternal:\n    enabled: true\n    type: LoadBalancer\n  thanosIngress:\n    enabled: true\n    ingressClassName: nginx\n    hosts: \n\n    - thanos-gateway.${DOMAIN_NAME}\n    paths: []\n    # - /\n    pathType: ImplementationSpecific\n    tls: \n    - secretName: thanos-gateway-tls\n      hosts:\n      - thanos-gateway.${DOMAIN_NAME}\n  prometheusSpec:\n    thanos: \n      objectStorageConfig:\n        existingSecret: {}\n          key: thanos.yaml\n          name: thanos-s3-config\nEOF\n</code></pre>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#enable-sigv4-in-grafana-data-source-for-amp","title":"enable sigv4 in grafana data source for AMP","text":"<ul> <li>add following lines to values.yaml <pre><code>grafana:\n  grafana.ini: \n    auth:\n      sigv4_auth_enabled: true \n</code></pre></li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#other-samples","title":"other samples","text":"<ul> <li>https://github.com/thanos-io/kube-thanos/tree/main/examples</li> <li>https://github.com/infracloudio/thanos-receiver-demo/tree/main/manifests</li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#receive-controller","title":"receive controller","text":"<ul> <li>https://github.com/observatorium/thanos-receive-controller/tree/main</li> <li>receive controller does not included in this POC, it could based on header in remote write traffic to forward data to specific receive, refer (https://www.infracloud.io/blogs/multi-tenancy-monitoring-thanos-receiver/)</li> <li>In this POC we use dedicate receive. you could use receive route with receive controller project. refer (https://thanos.io/tip/proposals-accepted/202012-receive-split.md/)</li> <li>download  receive-controller.tar.gz </li> <li>create receive controller in thanos namespace <pre><code>kubectx $c1\nk apply -f receive-controller/\n</code></pre></li> <li>receive controller will generate <code>thanos-receive-generated</code> configmap with endpoint for receive route scenarios, include this file as hashring-config</li> <li>create default s3 config <pre><code>CLUSTER_NAME=default\nNAMESPACE=thanos\n</code></pre></li> </ul> <pre><code>DEPLOY_NAME_1=prom-operator-${CLUSTER_NAME_1}\nNAMESPACE_NAME=monitoring\nkubectl create ns ${NAMESPACE_NAME}\nkubectl create secret generic thanos-s3-config-${CLUSTER_NAME_1} --\nfrom-file=thanos-s3-config-${CLUSTER_NAME_1}=s3-config/thanos-s3-\nconfig-${CLUSTER_NAME_1}.yaml --namespace ${NAMESPACE_NAME}\n</code></pre> <ul> <li>create sa <pre><code>SA_NAME=thanos-receive-default\nCLUSTER_NAME=ekscluster1\ncreate-iamserviceaccount ${SA_NAME} ${CLUSTER_NAME} thanos 1\n</code></pre></li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#todo","title":"todo","text":"<ul> <li>thanos receive router</li> <li>thanos compact component, and crash issue</li> <li>configmap in prometheus </li> <li>store hpa and query hpa<ul> <li>store startup speed for large history data</li> </ul> </li> </ul>","tags":["prometheus"]},{"location":"others/POC-prometheus-ha-architect-with-thanos-manually/#_2","title":"\u53c2\u8003\u94fe\u63a5","text":"<ul> <li>https://observability.thomasriley.co.uk/prometheus/using-thanos/high-availability/</li> <li>https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/high-availability.md</li> <li>https://medium.com/@kakashiliu/deploy-prometheus-operator-with-thanos-60210eff172b</li> <li>https://particule.io/en/blog/thanos-monitoring/</li> <li>https://blog.csdn.net/kingu_crimson/article/details/123840099</li> <li>thanos </li> <li>prometheus</li> <li>prometheus</li> <li>https://github.com/terraform-aws-modules/terraform-aws-eks/issues/2009</li> <li>prometheus-agent</li> <li>https://p8s.io/docs/operator/install/</li> </ul>","tags":["prometheus"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/","title":"Cross Region Reverse Proxy with NLB and Cloudfront","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#cross-region-reverse-proxy-with-nlb-and-cloudfront","title":"Cross Region Reverse Proxy with NLB and Cloudfront","text":"","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#diagram","title":"diagram","text":"<ul> <li>original region in global on right hand side</li> <li>china region on left hand side </li> </ul>","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#prepare-application-on-eks","title":"prepare application on eks","text":"","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#host-zone","title":"host zone","text":"<ul> <li>2 host zones, one for each region <pre><code>DOMAIN_NAME=poc1009.aws.panlm.xyz # for original region \nAWS_REGION=us-east-2 # for original region\nCN_DOMAIN_NAME=poc1010.aws.panlm.xyz # for china region\n</code></pre></li> </ul> right-click &amp; open-in-new-tab:","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#eks-cluster","title":"eks cluster","text":"<ul> <li>create eks cluster (refer: ../CLI/linux/eksdemo)</li> <li>install addons (refer: ../CLI/linux/eksdemo)<ul> <li>externaldns</li> <li>aws load balancer controller</li> <li>certificate</li> </ul> </li> <li>httpbin app</li> </ul>","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#httpbin","title":"httpbin","text":"<ul> <li>TC-private-apigw-dataflow (github)</li> <li>ingress setting with multiple certificates and host wildcard <pre><code>metadata:\n  annotations:\n    alb.ingress.kubernetes.io/certificate-arn: arn_cert1,arn_cert2\nspec:\n  ingressClassName: alb\n  rules:\n    - host: '*.${DOMAIN_NAME#*.}' # match DOMAIN_NAME and CN_DOMAIN_NAME\n</code></pre></li> <li>ensure <code>*.domain_name</code> existed in both host zones, any domains could access to ALB directly, for example <code>httpbin.${DOMAIN_NAME}</code> <pre><code>curl https://httpbin.${DOMAIN_NAME}/anything\n</code></pre></li> </ul>","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#prep-nlb-1-in-front-of-alb","title":"prep NLB-1 in front of ALB","text":"<ul> <li>ALB type target group to tcp 80/443 (ALB \u7c7b\u578b TG \u53ea\u80fd\u9009 TCP\uff0c\u6ca1\u6709 TLS)</li> <li>create NLB-1 with 2 listeners, 80/443 (TCP only, no TLS)</li> <li>on route53, add DNS record alias to NLB-1, called <code>nlbtoalb.${DOMAIN_NAME}</code> <pre><code>curl https://nlbtoalb.${DOMAIN_NAME}/anything\ncurl -L http://nlbtoalb.${DOMAIN_NAME}/anything\n</code></pre></li> <li>both could access application successfully</li> </ul>","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#reverse-proxy-in-china-region-","title":"reverse proxy in china region-","text":"<ul> <li>setup 2 EC2 instances fake-waf-on-ec2-forwarding-https (github)</li> <li> <p>forward request to NLB-1\u2019s public IP addresses. </p> <ul> <li>We have 2 destination IPs, using probability 50% in first rule and keep 2nd rule always been hit.</li> <li>If your have 3 destination IPs, using 0.33/0.5 in first 2 rules and keep last one always been hit. <pre><code>instance_ip=172.31.17.223 # instance internal ip address\nnext_ip=3.115.136.123 # one ip address of vpce domain name\nnext_ip2=3.241.89.18\n\n# get alb/nlb internal ip addresses\nfor i in 172.31.20.112 172.31.33.21; do\n    iptables -t nat -A PREROUTING -p tcp -s $i -d $instance_ip --dport 443 \\\n        -m statistic --mode random --probability 0.5 \\\n        -i eth0 -j DNAT --to-destination $next_ip:443;\n    iptables -t nat -A PREROUTING -p tcp -s $i -d $instance_ip --dport 443 \\\n        -i eth0 -j DNAT --to-destination $next_ip2:443;\ndone\n\niptables -t nat -A POSTROUTING -p tcp --dport 443 -s 172.31.0.0/16 -d $next_ip -o eth0 -j MASQUERADE;\n</code></pre></li> </ul> </li> <li> <p>NLB-2 in front of these EC2 instances</p> </li> <li>on route53, add dedicate host zone for this region and add DNS record CNAME to NLB-2, called <code>test.${CN_DOMAIN_NAME}</code></li> <li>\u8bf7\u6c42\u5c06\u83b7\u53d6\u5230 reverse proxy \u516c\u7f51\u5730\u5740 <pre><code>curl https://test.${CN_DOMAIN_NAME}/ip\n</code></pre></li> </ul>","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#cloudfront-in-front-of-nlb-2","title":"cloudfront in front of NLB-2","text":"<ul> <li>create certificate for cn domain name for cloudfront<ul> <li>\u5982\u679c\u4e0d\u4f7f\u7528 cloudfront \u5219\u4e0d\u9700\u8981\u521b\u5efa\u8bc1\u4e66</li> </ul> </li> <li>create origin to NLB-2 <ul> <li>using aws default domain name </li> <li>or NLB-2\u2019s domain name (<code>test.${CN_DOMAIN_NAME}</code>)</li> </ul> </li> <li>create route53, add DNS record CNAME to cloudfront, called <code>abc.${CN_DOMAIN_NAME}</code> <pre><code>curl https://abc.${CN_DOMAIN_NAME}/ip\n</code></pre></li> </ul> <ul> <li>no CORS needed</li> </ul>","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#more","title":"more","text":"<ul> <li>iptables DNAT will exhaust ports or not ?<ul> <li>No. refer link</li> </ul> </li> </ul>","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/cross-region-reverse-proxy-with-nlb-cloudfront/#refer","title":"refer","text":"<ul> <li>../CLI/linux/iptables</li> <li>fake-waf-on-ec2-forwarding-https<ul> <li>github </li> </ul> </li> <li>using alb + nginx as reverse proxy <ul> <li>Extend Your Web Application Deployment to the China Region Using AWS Direct Connect</li> </ul> </li> <li>https://scalingo.com/blog/iptables</li> </ul>","tags":["aws/network/nlb","aws/network/cloudfront","aws/china"]},{"location":"others/ecr-scan-on-push-notification-sns/","title":"Enable scan on push in ECR and send notification to SNS","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/container/ecr","aws/integration/sns"]},{"location":"others/ecr-scan-on-push-notification-sns/#enable-scan-on-push-in-ecr-and-send-notification-to-sns","title":"Enable scan on push in ECR and send notification to SNS","text":"","tags":["aws/container/ecr","aws/integration/sns"]},{"location":"others/ecr-scan-on-push-notification-sns/#_1","title":"\u9700\u6c42","text":"<p>\u542f\u7528 ECR \u7684 Scan on push \u4e4b\u540e\uff0c\u81ea\u52a8\u5c06\u626b\u63cf\u7ed3\u679c\u4e2d CRITICAL \u7684\u4fe1\u606f\u53d1\u9001\u5230\u76ee\u6807 SNS \u544a\u8b66\u3002</p>","tags":["aws/container/ecr","aws/integration/sns"]},{"location":"others/ecr-scan-on-push-notification-sns/#_2","title":"\u89e3\u51b3\u65b9\u6848","text":"<p>\u4f7f\u7528\u5df2\u6709\u7684 blog \u63cf\u8ff0\u573a\u666f\u53ef\u4ee5\u81ea\u52a8\u5c06\u626b\u63cf\u540e\u7684\u4fe1\u606f\u5206\u7c7b\u4fdd\u5b58\u5230 cloudwatch \u4e2d\uff0c\u53ef\u4ee5\u5728\u4e2d\u56fd\u533a\u4f7f\u7528 cloudformation \u90e8\u7f72\u6210\u529f\u3002\u5982\u4e0b\u67b6\u6784\u56fe\uff1a</p> <p></p> <p>\u4e0b\u8f7d cloudformation \u6a21\u677f\uff1atemplate-ecr.yml</p> <p>\u6211\u4eec\u5728\u4e0a\u8ff0\u67b6\u6784\u57fa\u7840\u4e0a\u505a\u4e86\u989d\u5916\u624b\u5de5\u4fee\u6539\uff1a</p> <ul> <li>\u521b\u5efa\u7279\u5b9a\u7684 SNS topic\uff0c\u6ce8\u518c\u90ae\u7bb1\u5e76\u63a5\u6536\u544a\u8b66</li> <li>\u7ed9 Lambda \u7684\u6267\u884c role \u6dfb\u52a0 SNS topic \u7684\u6743\u9650</li> <li>\u66f4\u65b0\u4e86 lambda \u51fd\u6570\u76f4\u63a5\u5c06 CRITICAL \u7684\u6d88\u606f\u540c\u65f6\u53d1\u9001\u5230 SNS \u544a\u8b66<ul> <li>\u4e0b\u8f7d\u53c2\u8003 new-lambda.py</li> </ul> </li> </ul>","tags":["aws/container/ecr","aws/integration/sns"]},{"location":"others/ecr-scan-on-push-notification-sns/#_3","title":"\u53c2\u8003","text":"<ul> <li>https://aws.amazon.com/blogs/containers/logging-image-scan-findings-from-amazon-ecr-in-cloudwatch-using-an-aws-lambda-function/</li> <li>https://aws.amazon.com/blogs/mt/get-notified-specific-lambda-function-error-patterns-using-cloudwatch/</li> </ul>","tags":["aws/container/ecr","aws/integration/sns"]},{"location":"others/file-storage-gateway-lab/","title":"Storage File Gateway","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/storage/storage-gateway"]},{"location":"others/file-storage-gateway-lab/#storage-file-gateway","title":"Storage File Gateway","text":"","tags":["aws/storage/storage-gateway"]},{"location":"others/file-storage-gateway-lab/#prepare","title":"prepare","text":"<ul> <li>create a cloud9 desktop for this lab</li> <li>ensure you have enough privilege to create resources</li> </ul>","tags":["aws/storage/storage-gateway"]},{"location":"others/file-storage-gateway-lab/#create-s3-bucket","title":"create s3 bucket","text":"<pre><code>AWS_REGION=us-east-1\nBUCKET_NAME=fgwlab-$RANDOM\nPREFIX_NAME=fgw\naws s3 mb s3://${BUCKET_NAME}\naws s3api put-object --bucket ${BUCKET_NAME} \\\n  --key ${PREFIX_NAME}/\n</code></pre>","tags":["aws/storage/storage-gateway"]},{"location":"others/file-storage-gateway-lab/#create-fgw-instance","title":"create fgw instance","text":"<pre><code># ensure this key existed\nKEY_NAME=awskey\n\nIMAGE_ID=$(aws ec2 describe-images --region ${AWS_REGION}  \\\n  --filters Name=name,Values='aws-storage-gateway-*'  \\\n  --query 'Images[*].[ImageId,CreationDate,Name]' --output text \\\n  |sort -k2 -r |head -n 1 |awk '{print $1}')\n# another way to get IMAGE_ID\n# aws ssm get-parameter --name /aws/service/storagegateway/ami/FILE_S3/latest\n\n# cloud 9 subnet\nINST_ID=$(curl http://169.254.169.254/1.0/meta-data/instance-id 2&gt;/dev/null)\nSUBNET_ID=$(aws ec2 describe-instances --instance-ids ${INST_ID} --query 'Reservations[0].Instances[0].SubnetId' --output text)\n\n# create sg\nFGW_SG_NAME=fgw-sg-$RANDOM\nVPC_ID=$(aws ec2 describe-instances --instance-ids ${INST_ID} --query 'Reservations[0].Instances[0].VpcId' --output text)\nFGW_SG_ID=$(aws ec2 create-security-group \\\n  --description ${FGW_SG_NAME} \\\n  --group-name ${FGW_SG_NAME} \\\n  --vpc-id ${VPC_ID} \\\n  --query 'GroupId' --output text )\n# all traffic allowed\naws ec2 authorize-security-group-ingress \\\n  --group-name ${FGW_SG_NAME} \\\n  --protocol -1 \\\n  --port -1 \\\n  --cidr 0.0.0.0/0\n\nFGW_INST_ID=$(aws ec2 run-instances --region ${AWS_REGION} --key-name ${KEY_NAME} \\\n  --image-id ${IMAGE_ID} --instance-type m5.xlarge \\\n  --block-device-mappings 'DeviceName=/dev/sdb,Ebs={VolumeSize=200}' \\\n  --subnet-id ${SUBNET_ID} --security-group-ids ${FGW_SG_ID} \\\n  --query Instances[*].InstanceId --output text )\n\n# wait instance spin up\ntmpfile=/tmp/instance-status-$$\nwhile true ; do\n  aws ec2 describe-instance-status \\\n  --instance-ids ${FGW_INST_ID} |tee ${tmpfile}\n  inst_stat=$(cat $tmpfile |jq -r '.InstanceStatuses[0].InstanceStatus.Status')\n  sys_stat=$(cat $tmpfile |jq -r '.InstanceStatuses[0].SystemStatus.Status')\n  if [[ ${inst_stat} == \"ok\" &amp;&amp;  ${sys_stat} == \"ok\" ]]; then\n    break\n  else\n    sleep 30\n  fi\ndone\n\n# get instance ip\nINST_IP=$(aws ec2 describe-instances --instance-ids ${FGW_INST_ID} --query 'Reservations[0].Instances[0].PublicIpAddress' --output text)\nINST_PRIV_IP=$(aws ec2 describe-instances --instance-ids ${FGW_INST_ID} --query 'Reservations[0].Instances[0].PrivateIpAddress' --output text)\n\nACTIVATION_KEY=$(wget \"${INST_IP}/?activationRegion=${AWS_REGION}\" 2&gt;&amp;1 | \\\ngrep -i location | \\\ngrep -oE 'activationKey=[A-Z0-9-]+' | \\\ncut -f2 -d=)\n\nFGW_ARN=$(aws storagegateway activate-gateway \\\n--gateway-name FGW-$RANDOM \\\n--gateway-timezone \"GMT+8:00\" \\\n--gateway-region ${AWS_REGION} \\\n--gateway-type FILE_S3 \\\n--activation-key  ${ACTIVATION_KEY} \\\n--query 'GatewayARN' --output text)\n\n# aws storagegateway list-gateways --query 'Gateways[?GatewayARN==`'${FGW_ARN}'`].GatewayOperationalState' --output text\n\n# HostEnvironment --&gt; EC2\naws storagegateway list-gateways --query 'Gateways[?GatewayARN==`'${FGW_ARN}'`]' --output json\n\nwhile true ; do\n    aws storagegateway describe-gateway-information \\\n    --gateway-arn ${FGW_ARN}\n    if [[ $? -eq 0 ]]; then\n        break\n    else\n        sleep 30\n    fi\ndone\n\nDISK_IDS=$(aws storagegateway list-local-disks \\\n--gateway-arn ${FGW_ARN} \\\n--query 'Disks[*].DiskId' --output text)\n\naws storagegateway add-cache \\\n--gateway-arn ${FGW_ARN} \\\n--disk-ids ${DISK_IDS}\n</code></pre>","tags":["aws/storage/storage-gateway"]},{"location":"others/file-storage-gateway-lab/#create-nfs-share-","title":"create-nfs-share-","text":"<ul> <li> <p>create iam role <pre><code>account_id=$(aws sts get-caller-identity --query \"Account\" --output text)\nfgw_role_name=StorageGatewayBucketAccessRole-$RANDOM.$RANDOM\naws iam create-role --role-name ${fgw_role_name} --assume-role-policy-document '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"storagegateway.amazonaws.com\"},\"Action\":\"sts:AssumeRole\",\"Condition\":{\"StringEquals\":{\"aws:SourceAccount\":\"'\"${account_id}\"'\",\"aws:SourceArn\":\"'\"${FGW_ARN}\"'\"}}}]}'\nenvsubst &gt; ${fgw_role_name}-policy.yaml &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetAccelerateConfiguration\",\n                \"s3:GetBucketLocation\",\n                \"s3:GetBucketVersioning\",\n                \"s3:ListBucket\",\n                \"s3:ListBucketVersions\",\n                \"s3:ListBucketMultipartUploads\"\n            ],\n            \"Resource\": \"arn:aws:s3:::${BUCKET_NAME}\",\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Action\": [\n                \"s3:AbortMultipartUpload\",\n                \"s3:DeleteObject\",\n                \"s3:DeleteObjectVersion\",\n                \"s3:GetObject\",\n                \"s3:GetObjectAcl\",\n                \"s3:GetObjectVersion\",\n                \"s3:ListMultipartUploadParts\",\n                \"s3:PutObject\",\n                \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": \"arn:aws:s3:::${BUCKET_NAME}/*\",\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\nEOF\naws iam put-role-policy --role-name ${fgw_role_name} --policy-name ${fgw_role_name}-policy --policy-document \"file://./${fgw_role_name}-policy.yaml\"\nfgw_role_arn=$(aws iam get-role --role-name ${fgw_role_name} --query 'Role.Arn' --output text)\n</code></pre></p> </li> <li> <p>create file share <pre><code># ensure client list is correct\nclient_token=$(echo $RANDOM |md5sum |tr -d ' -')\naws storagegateway create-nfs-file-share \\\n--client-token ${client_token} \\\n--gateway-arn ${FGW_ARN} \\\n--role ${fgw_role_arn} \\\n--location-arn \"arn:aws:s3:::${BUCKET_NAME}/${PREFIX_NAME}/\" \\\n--file-share-name ${BUCKET_NAME}-${PREFIX_NAME} \\\n--client-list \"172.31.0.0/16\" \\\n--cache-attributes \"CacheStaleTimeoutInSeconds=300\" \\\n--squash NoSquash \\\n--bucket-region ${AWS_REGION}\n\nfs_arn=$(aws storagegateway list-file-shares --gateway-arn ${FGW_ARN} \\\n--query 'FileShareInfoList[0].FileShareARN' \\\n--output text)\n\necho \"On Linux:\"\necho \"mount -t nfs -o nolock,hard ${INST_PRIV_IP}:/${BUCKET_NAME}-${PREFIX_NAME} /mnt_point \"\n</code></pre></p> </li> </ul>","tags":["aws/storage/storage-gateway"]},{"location":"others/file-storage-gateway-lab/#more","title":"more","text":"<ul> <li>https://aws.amazon.com/blogs/storage/mounting-amazon-s3-to-an-amazon-ec2-instance-using-a-private-connection-to-s3-file-gateway/</li> <li>using s3 gateway endpoint to enhance security of data transferring<ul> <li>https://aws.amazon.com/blogs/architecture/connect-amazon-s3-file-gateway-using-aws-privatelink-for-amazon-s3/</li> </ul> </li> </ul>","tags":["aws/storage/storage-gateway"]},{"location":"others/github-page-howto/","title":"github-page-howto","text":"<p>[!WARNING] This is a github note</p>","tags":["github"]},{"location":"others/github-page-howto/#how-to-put-workshop-on-github","title":"how to put workshop on github","text":"","tags":["github"]},{"location":"others/github-page-howto/#build-local","title":"build local","text":"<ul> <li>https://aws-samples.github.io/aws-modernization-workshop-sample/20_build/1_setup/</li> </ul> <pre><code>git submodule init\ngit submodule update\n</code></pre>","tags":["github"]},{"location":"others/github-page-howto/#hosted-on-github-page","title":"hosted on github page","text":"<ul> <li>https://gohugo.io/hosting-and-deployment/hosting-on-github/</li> </ul>","tags":["github"]},{"location":"others/github-page-howto/#remove-custom-domain","title":"remove custom domain","text":"<ul> <li>remove <code>static/CNAME</code> file<ul> <li>this file include line: <code>aws-labs.panlm.xyz</code></li> </ul> </li> <li>remove custom domain from github <code>Pages</code> page</li> <li>rename repo name to <code>git-ghpages</code></li> <li>change in <code>config.toml</code><ul> <li><code>baseURL</code> \u2013&gt; <code>https://panlm.github.io/git-ghpages/</code></li> </ul> </li> </ul>","tags":["github"]},{"location":"others/github-page-howto/#awesome-pages-plugin","title":"awesome pages plugin","text":"<ul> <li>https://github.com/lukasgeiter/mkdocs-awesome-pages-plugin</li> </ul>","tags":["github"]},{"location":"others/github-page-howto/#highlight-block","title":"highlight block","text":"","tags":["github"]},{"location":"others/github-page-howto/#in-github","title":"in github","text":"<p>[!WARNING]  This is a github note</p> <p>[!IMPORTANT] Crucial information necessary for users to succeed.</p> <p>[!NOTE] Critical content demanding immediate user attention due to potential risks.</p>","tags":["github"]},{"location":"others/github-page-howto/#in-material-mkdocs","title":"in material mkdocs","text":"<ul> <li>https://squidfunk.github.io/mkdocs-material/reference/admonitions/ <pre><code>!!! warning \"This is a github note\"\n</code></pre></li> </ul>","tags":["github"]},{"location":"others/github-page-howto/#plugins-for-mkdocs","title":"plugins for mkdocs","text":"<ul> <li>https://github.com/mkdocs/catalog</li> <li>For full documentation visit mkdocs.org</li> </ul>","tags":["github"]},{"location":"others/github-page-howto/#mkdocs","title":"mkdocs","text":"","tags":["github"]},{"location":"others/github-page-howto/#review-local","title":"review local","text":"<pre><code>mkdocs serve\n</code></pre>","tags":["github"]},{"location":"others/global-sso-and-china-aws-accounts/","title":"Using Global SSO to Login China AWS Accounts","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/security/identity-center"]},{"location":"others/global-sso-and-china-aws-accounts/#using-global-sso-to-login-china-aws-accounts","title":"Using Global SSO to Login China AWS Accounts","text":"","tags":["aws/security/identity-center"]},{"location":"others/global-sso-and-china-aws-accounts/#walkthrough","title":"walkthrough","text":"","tags":["aws/security/identity-center"]},{"location":"others/global-sso-and-china-aws-accounts/#use-identity-center-directory-as-identity-source","title":"use identity center directory as identity source","text":"<ul> <li>create application <code>External AWS Account Application</code> from sso <code>Applications</code> </li> <li> <p>download <code>IAM Identity Center SAML metadata file</code> </p> </li> <li> <p>create identity provider in aws china account </p> </li> <li> <p>create role for <code>SAML 2.0 federation</code> in aws china account, and assign policy to it </p> </li> <li> <p>back to create application page, review application metadata</p> <ul> <li>using <code>https://signin.amazonaws.cn/saml</code></li> <li>original is <code>https://signin.aws.amazon.com/saml</code> </li> </ul> </li> <li> <p>create application</p> </li> <li>edit <code>attribute mappings</code> for this application, ensure following two field existed</li> </ul> Field Value Format <code>https://aws.amazon.com/SAML/Attributes/Role</code> arn:aws:iam::ACCOUNTID:saml-provider/SAMLPROVIDERNAME,arn:aws:iam::ACCOUNTID:role/ROLENAME unspecified <code>https://aws.amazon.com/SAML/Attributes/RoleSessionName</code>  must match [a-zA-Z_0-9+=,.@-]{2,64} unspecified <p></p> <ul> <li>assign user to application and login<ul> <li>find login url from sso dashboard or reset user\u2019s password </li> </ul> </li> </ul>","tags":["aws/security/identity-center"]},{"location":"others/global-sso-and-china-aws-accounts/#use-managed-ad-as-identity-source","title":"use managed AD as identity source","text":"<ul> <li> <p>configure attribute mapping in <code>manage sync</code> in <code>settings</code> </p> </li> <li> <p>others steps are same </p> </li> </ul>","tags":["aws/security/identity-center"]},{"location":"others/global-sso-and-china-aws-accounts/#use-external-idp-as-identity-source","title":"use external IdP as identity source","text":"<ul> <li>todo</li> </ul>","tags":["aws/security/identity-center"]},{"location":"others/global-sso-and-china-aws-accounts/#in-same-organization-user-and-role","title":"in same organization user and role","text":"<ul> <li>in multi-account permissions, choose account, and assign user/group to it, assign permission set to it.</li> <li>assume from CLI <pre><code>assume \nassume --sso --sso-start-url https://xxx.awsapps.com/start \\\n    --sso-region ap-southeast-1 \\\n    --account-id xxx\u00a0\\\n    --role-name AWSAdministratorAccess \\\n    --verbose\n</code></pre></li> </ul>","tags":["aws/security/identity-center"]},{"location":"others/global-sso-and-china-aws-accounts/#refer","title":"refer","text":"<ul> <li>https://static.global.sso.amazonaws.com/app-4a24b6fe5e450fa2/instructions/index.htm</li> <li>https://aws.amazon.com/cn/blogs/china/use-amazon-cloud-technology-single-sign-on-service-for-amazon-cloud-technology-china/</li> <li>https://static.global.sso.amazonaws.com/app-4a24b6fe5e450fa2/instructions/index.htm</li> </ul>","tags":["aws/security/identity-center"]},{"location":"others/lab-create-cloudwatch-dashboard-cpu-metric/","title":"create-dashboard-for-instance-cpu-matrics","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/mgmt/cloudwatch"]},{"location":"others/lab-create-cloudwatch-dashboard-cpu-metric/#create-dashboard-for-instance-cpu-matrics","title":"create-dashboard-for-instance-cpu-matrics","text":"","tags":["aws/mgmt/cloudwatch"]},{"location":"others/lab-create-cloudwatch-dashboard-cpu-metric/#permission","title":"permission","text":"<ul> <li><code>describe-instances</code></li> <li><code>put-dashboard</code></li> </ul>","tags":["aws/mgmt/cloudwatch"]},{"location":"others/lab-create-cloudwatch-dashboard-cpu-metric/#each-metrics-with-anomaly-detection","title":"each metrics with anomaly detection","text":"<pre><code>#!/bin/bash \n\nif [[ $# -eq 0 ]]; then\n    echo 'Usage: $0 i-xxxx1 i-xxxx2'\n    exit 9\nfi\n\nexport AWS_DEFAULT_REGION=cn-northwest-1\n\naws sts get-caller-identity 2&gt;&amp;1 &gt;/dev/null\nif [[ $? -ne 0 ]]; then\n    export AWSCLI=0\nelse\n    export AWSCLI=1\nfi\n\nDASHBOARD_FILE=dash1.json\nDASHBOARD_FILE_TEMP=dash1-$$.json\n\ncat &gt; ${DASHBOARD_FILE} &lt;&lt;-EOF\n{\n    \"widgets\": [\n    ]\n}\nEOF\n\nTEMPLATE_FILE=temp1.json\necho '\n        {\n            \"type\": \"metric\",\n            \"x\": 0,\n            \"y\": 0,\n            \"width\": 8,\n            \"height\": 5,\n            \"properties\": {\n                \"metrics\": [\n                    [ \"AWS/EC2\", \"CPUUtilization\", \"InstanceId\", \"${INSTANCE_ID}\", { \"id\": \"m1\", \"stat\": \"Maximum\" } ],\n                    [ { \"expression\": \"ANOMALY_DETECTION_BAND(m1, 2)\", \"label\": \"${INSTANCE_ID} (${INSTANCE_NAME}) (expected)\", \"id\": \"ad1\", \"color\": \"#95A5A6\", \"region\": \"${AWS_DEFAULT_REGION}\" } ]\n                ],\n                \"view\": \"timeSeries\",\n                \"stacked\": false,\n                \"region\": \"${AWS_DEFAULT_REGION}\",\n                \"title\": \"CPUUtilization-${INSTANCE_NAME}\",\n                \"period\": 300,\n                \"yAxis\": {\n                    \"left\": {\n                        \"min\": 0,\n                        \"max\": 30\n                    }\n                }                \n            }\n        }\n' |base64 &gt; ${TEMPLATE_FILE}.b64\n\nif [[ ${AWSCLI} -eq 1 ]]; then\n    aws ec2 describe-instances &gt; /tmp/all-instances.out\n    INSTANCE_NUMBER=$(cat /tmp/all-instances.out |jq -r '.Reservations | length')\necho \"total instance number: ${INSTANCE_NUMBER}\"\nfi\n\nTMP=/tmp/tmp-$$\nfor i in $@ ; do\n    export INSTANCE_ID=${i}\n    export INSTANCE_NAME=$(cat /tmp/all-instances.out |jq -r '.Reservations[] | select (.Instances[0].InstanceId == \"'\"${INSTANCE_ID}\"'\") | .Instances[] |  (del((.Tags[]|select(.Key!=\"Name\")))|.Tags[]|.Value|tostring)')\n    cat ${TEMPLATE_FILE}.b64 |base64 --decode |envsubst &gt;${TMP}\n    jq --argjson groupInfo \"$(&lt;$TMP)\" '.widgets += [$groupInfo]' ${DASHBOARD_FILE} &gt; ${DASHBOARD_FILE_TEMP}\n    mv ${DASHBOARD_FILE_TEMP} ${DASHBOARD_FILE}\ndone\n\nif [[ ${AWSCLI} -eq 1 ]]; then\n    DASHBOARD_NAME=dash-$(date +%Y%m%d%H%M%S) \n    aws cloudwatch put-dashboard --dashboard-name ${DASHBOARD_NAME} --dashboard-body file://${DASHBOARD_FILE} &gt;/dev/null\n    echo \"please access dashboard: ${DASHBOARD_NAME}\"\nfi\n</code></pre>","tags":["aws/mgmt/cloudwatch"]},{"location":"others/lab-create-cloudwatch-dashboard-cpu-metric/#sample","title":"sample","text":"","tags":["aws/mgmt/cloudwatch"]},{"location":"others/lab-create-cloudwatch-dashboard-cpu-metric/#all-metrics","title":"all metrics","text":"<pre><code>#!/bin/bash \n\nif [[ $# -eq 0 ]]; then\n    echo 'Usage: $0 i-xxxx1 i-xxxx2'\n    exit 9\nfi\n\nexport AWS_DEFAULT_REGION=cn-north-1\n\naws sts get-caller-identity 2&gt;&amp;1 &gt;/dev/null\nif [[ $? -ne 0 ]]; then\n    export AWSCLI=0\nelse\n    export AWSCLI=1\nfi\n\nDASHBOARD_FILE=dash1.json\nDASHBOARD_FILE_TEMP=dash1-$$.json\n\ncat &gt; ${DASHBOARD_FILE} &lt;&lt;-EOF\n{\n    \"widgets\": [\n        {\n            \"type\": \"metric\",\n            \"x\": 0,\n            \"y\": 0,\n            \"width\": 16,\n            \"height\": 12,\n            \"properties\": {\n                \"metrics\": [\n                ],\n                \"view\": \"timeSeries\",\n                \"stacked\": false,\n                \"region\": \"${AWS_DEFAULT_REGION}\",\n                \"title\": \"CPUUtilization\",\n                \"period\": 300\n            }\n        }\n    ]\n}\nEOF\n\nTEMPLATE_FILE=temp1.json\necho '\n                    [ \"AWS/EC2\", \"CPUUtilization\", \"InstanceId\", \"${INSTANCE_ID}\", { \"id\": \"m${num}\", \"stat\": \"Maximum\" } ]\n' |base64 &gt; ${TEMPLATE_FILE}.b64\n\nif [[ ${AWSCLI} -eq 1 ]]; then\n    aws ec2 describe-instances &gt; /tmp/all-instances.out\n    INSTANCE_NUMBER=$(cat /tmp/all-instances.out |jq -r '.Reservations | length')\necho \"total instance number: ${INSTANCE_NUMBER}\"\nfi\n\nexport num=1\nTMP=/tmp/tmp-$$\nfor i in $@ ; do\n    export INSTANCE_ID=${i}\n    export INSTANCE_NAME=$(cat /tmp/all-instances.out |jq -r '.Reservations[] | select (.Instances[0].InstanceId == \"'\"${INSTANCE_ID}\"'\") | .Instances[] |  (del((.Tags[]|select(.Key!=\"Name\")))|.Tags[]|.Value|tostring)')\n    cat ${TEMPLATE_FILE}.b64 |base64 --decode |envsubst &gt;${TMP}\n    jq --argjson groupInfo \"$(&lt;$TMP)\" '.widgets[0].properties.metrics += [$groupInfo]' ${DASHBOARD_FILE} &gt; ${DASHBOARD_FILE_TEMP}\n    mv ${DASHBOARD_FILE_TEMP} ${DASHBOARD_FILE}\n    export num=$((num+1))\ndone\n\nexit\n\nif [[ ${AWSCLI} -eq 1 ]]; then\n    DASHBOARD_NAME=dash-$(date +%Y%m%d%H%M%S) \n    aws cloudwatch put-dashboard --dashboard-name ${DASHBOARD_NAME} --dashboard-body file://${DASHBOARD_FILE} &gt;/dev/null\n    echo \"please access dashboard: ${DASHBOARD_NAME}\"\nfi\n</code></pre>","tags":["aws/mgmt/cloudwatch"]},{"location":"others/lab-create-cloudwatch-dashboard-cpu-metric/#sample_1","title":"sample","text":"<ul> <li>\u4e8c\u5341\u4e2a\u6307\u6807\u540e\uff0c\u989c\u8272\u5f00\u59cb\u91cd\u590d</li> </ul>","tags":["aws/mgmt/cloudwatch"]},{"location":"others/lab-create-cloudwatch-dashboard-cpu-metric/#followup","title":"followup","text":"<ul> <li>aws-amg-managed-grafana</li> </ul>","tags":["aws/mgmt/cloudwatch"]},{"location":"others/rescue-ec2-instance/","title":"Rescue EC2 Instance","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/compute/ec2"]},{"location":"others/rescue-ec2-instance/#rescue-ec2-instance","title":"Rescue EC2 Instance","text":"<ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-boot-issues/</li> </ul>","tags":["aws/compute/ec2"]},{"location":"others/rescue-ec2-instance/#ssh-to-rescue","title":"ssh to rescue","text":"<pre><code>sudo su -\n\nlsblk\nrescuedev=/dev/xvdf1\n\nrescuemnt=/mnt\nmkdir -p $rescuemnt\nmount $rescuedev $rescuemnt\nfor i in proc sys dev run; do mount --bind /$i $rescuemnt/$i ; done\nchroot $rescuemnt\n</code></pre> <ul> <li>refer: ../CLI/linux/linux-cmd</li> </ul>","tags":["aws/compute/ec2"]},{"location":"others/rescue-ec2-instance/#umount","title":"umount","text":"<pre><code>exit\n\numount $rescuemnt/{proc,sys,dev,run,}\n</code></pre>","tags":["aws/compute/ec2"]},{"location":"others/rescue-ec2-instance/#refer","title":"refer","text":"<p>automation runbook</p> <ul> <li>https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html</li> </ul>","tags":["aws/compute/ec2"]},{"location":"others/script-api-resource-method/","title":"script-api-resource-method","text":"<p>[!WARNING] This is a github note</p> <pre><code>title: Obseleted\n\nnewest version is [here](https://github.com/panlm/blog-private-api-gateway-dataflow#appendix)\n</code></pre>","tags":["aws/serverless/api-gateway"]},{"location":"others/script-api-resource-method/#script-api-resource-method","title":"script-api-resource-method","text":"<ul> <li>\u6bcf\u4e2a api \u7684\u6bcf\u4e2a resource \u7684\u6bcf\u4e2a method \u90fd\u9700\u8981\u5355\u72ec\u901a\u8fc7\u547d\u4ee4\u884c\u542f\u7528\u201ctlsConfig/insecureSkipVerification\u201d\uff0c\u901a\u8fc7\u8fd9\u4e2a\u811a\u672c\u7b80\u5316\u5de5\u4f5c</li> </ul> <pre><code>#!/bin/bash\n\nif [[ $# -ne 1 ]]; then\n  echo \"$0 API_ID\"\n  exit 99\nfi\n\nexport AWS_PAGER=\"\"\n\nAPI_ID=$1\n\nRESOURCE_FILE=/tmp/${API_ID}.json\naws apigateway get-resources --rest-api-id ${API_ID} &gt;${RESOURCE_FILE}\nif [[ $? -ne 0 ]]; then\n  echo \"api id error\"\n  exit 99\nfi\n\n# get resource ids\nRESOURCE_ID=$(cat ${RESOURCE_FILE} |jq -r '.items[].id' |xargs)\nfor i in ${RESOURCE_ID}; do\n  # get method\n  METHOD=$(cat ${RESOURCE_FILE} |jq -r '.items[] | select (.id==\"'$i'\") | .resourceMethods|keys[]' |xargs)\n  for j in ${METHOD}; do\n    METHOD_FILE=${API_ID}-$i-$j.json\n    # save all resource/method json\n    aws apigateway get-method --rest-api-id ${API_ID} --resource-id $i --http-method $j &gt; ${METHOD_FILE}\n    # if file has specific string, print aws cli to enable tlsConfig\n    egrep -ql 'connectionType.*VPC_LINK' ${METHOD_FILE}\n    if [[ $? -eq 0 ]]; then\n      echo \"aws apigateway update-integration --rest-api-id ${API_ID} --resource-id $i --http-method $j --patch-operations \\\"op='replace',path='/tlsConfig/insecureSkipVerification',value=true\\\"\"\n    fi\n  done\ndone\n</code></pre>","tags":["aws/serverless/api-gateway"]},{"location":"others/script-api-resource-method/#refer","title":"refer","text":"<ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-ssl-certificate-errors/</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"others/script-convert-mp3-to-text/","title":"script-convert-mp3-to-text","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/aiml/transcribe","aws/cmd"]},{"location":"others/script-convert-mp3-to-text/#script-convert-mp3-to-text","title":"script-convert-mp3-to-text","text":"<p>i wrote this script several years ago. I have chance to use it in real life today and update to here</p>","tags":["aws/aiml/transcribe","aws/cmd"]},{"location":"others/script-convert-mp3-to-text/#updated-version","title":"updated version","text":"<ul> <li>ensure you have export the <code>AWS_DEFAULT_REGION</code> <pre><code>#!/bin/bash\n# \n\nif [[ $# -ne 1 ]]; then\n  echo \"Usage: $0 filename\"\n  exit 99\nfi\n\nfilename=$1\n\nstring=`TZ=EAT-8 date +%Y%m%d%H%M%S`\nmp3file=mp3-${string}.mp3\njobname=job-${string}\ntranscriptfile=${filename%.*}-${string}.txt\nsrtfile=${filename%.*}-${string}.en.srt\nbucket_name=temp-${string}-$(uuidgen |tr 'A-Z' 'a-z')\n\naws s3 mb s3://${bucket_name}\nif [[ $? -ne 0 ]]; then\n  echo \"create bucket failed\"\nfi\n\naws s3 cp $filename s3://${bucket_name}/$mp3file\n#aws transcribe start-transcription-job --transcription-job-name $jobname \\\n# --language-code en-US --media MediaFileUri=s3://$bucket_name/$mp3file\naws transcribe start-transcription-job --transcription-job-name $jobname \\\n  --identify-language \\\n  --media MediaFileUri=s3://$bucket_name/$mp3file\n\nif [[ $? -ne 0 ]]; then\n  exit \nfi\n\noutput=/tmp/$$.output\necho \"status file: $output\"\nwhile true ; do\n  aws transcribe get-transcription-job --transcription-job-name $jobname &gt; $output\n  status=$(cat $output |jq -r '.TranscriptionJob.TranscriptionJobStatus')\n  if [[ $status == \"COMPLETED\" ]]; then\n    echo\n    break\n  else\n    echo -e '.\\c'\n  fi\n  sleep 60\ndone\n\ncat $output |jq -r '.TranscriptionJob.Transcript.TranscriptFileUri' |xargs -J {} wget -O $output.wget '{}'\ncat $output.wget |jq -r '.results.transcripts[0].transcript' &gt; $transcriptfile\nif [[ -f $home/conv-srt.py ]]; then\n  $home/conv-srt.py $output.wget &gt; $srtfile\nfi\n\n#clean\naws s3 rm s3://$bucket_name/$mp3file\n</code></pre></li> </ul>","tags":["aws/aiml/transcribe","aws/cmd"]},{"location":"others/script-convert-mp3-to-text/#refer","title":"refer","text":"<ul> <li>https://github.com/panlm/NTNX/blob/master/scripts/translate/speechtext-aws.sh</li> </ul>","tags":["aws/aiml/transcribe","aws/cmd"]},{"location":"others/self-signed-certificates/","title":"self-signed-certificates","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/security/acm","aws/container/eks"]},{"location":"others/self-signed-certificates/#self-signed-certificates","title":"self-signed-certificates","text":"","tags":["aws/security/acm","aws/container/eks"]},{"location":"others/self-signed-certificates/#1-has-certificate-chain","title":"1 has certificate chain","text":"","tags":["aws/security/acm","aws/container/eks"]},{"location":"others/self-signed-certificates/#11-has-certificate-chain-with-intermediate","title":"1.1 has certificate chain (with intermediate)","text":"<ul> <li>works for api gateway and alb</li> <li>http endpoint in integration request need this kind certificate, and also set <code>insecureSkipVerification</code> to <code>true</code></li> </ul> <pre><code>mkdir myrootca\ncd myrootca/\ngit clone https://github.com/OpenVPN/easy-rsa.git\n# create root ca and no password\n./easy-rsa/easyrsa3/easyrsa init-pki\n./easy-rsa/easyrsa3/easyrsa build-ca nopass\ncd ..\n</code></pre> <pre><code>mkdir myinterca\ncd myinterca/\nln -sf ../myrootca/easy-rsa\n# create intermedia ca and no password\n./easy-rsa/easyrsa3/easyrsa init-pki\n./easy-rsa/easyrsa3/easyrsa build-ca subca nopass\n\n# sign intermedia ca\ncd ../myrootca/\n./easy-rsa/easyrsa3/easyrsa import-req ../myinterca/pki/reqs/ca.req myinterca\n./easy-rsa/easyrsa3/easyrsa sign-req ca myinterca\ncp -i pki/issued/myinterca.crt ../myinterca/pki/ca.crt\ncd ..\n</code></pre> <pre><code>mkdir mycert\ncd mycert\nln -sf ../myrootca/easy-rsa/\n# create certificate req and no password\n./easy-rsa/easyrsa3/easyrsa init-pki\n./easy-rsa/easyrsa3/easyrsa gen-req mycert nopass\n# Common Name --&gt; poc.aws.panlm.xyz\n\n# sign certificate\ncd ../myinterca/\n./easy-rsa/easyrsa3/easyrsa import-req ../mycert/pki/reqs/mycert.req mycert\n./easy-rsa/easyrsa3/easyrsa sign-req server mycert\ncp ./pki/issued/mycert.crt ../mycert/\ncd ..\n</code></pre> <pre><code>cd mycert\nopenssl x509 -inform PEM -in mycert.crt &gt;mycert.pem\nopenssl rsa -in ./pki/private/mycert.key &gt;mycert-key.pem\nopenssl x509 -inform PEM -in ../myinterca/pki/ca.crt &gt;mycert-chain-interca.pem\nopenssl x509 -inform PEM -in ../myrootca/pki/ca.crt &gt;mycert-chain-root.pem\n# first pem is certificate body \n# second pem is certificate private key\n# rest of pems are certificate chain (last one should be root ca)\n</code></pre>","tags":["aws/security/acm","aws/container/eks"]},{"location":"others/self-signed-certificates/#refer","title":"refer","text":"<ul> <li>https://wavecn.com/content.php?id=334</li> <li>https://docs.aws.amazon.com/acm/latest/userguide/import-certificate-format.html</li> </ul>","tags":["aws/security/acm","aws/container/eks"]},{"location":"others/self-signed-certificates/#12-has-certificate-chain-root-only","title":"1.2 has certificate chain (root only)","text":"<ul> <li>works for api gateway and alb</li> <li>http endpoint in integration request need this kind certificate, and also set <code>insecureSkipVerification</code> to <code>true</code></li> </ul> <pre><code>mkdir mycert\ncd mycert\ngit clone https://github.com/OpenVPN/easy-rsa.git\n\n# create root ca and no password\n./easy-rsa/easyrsa3/easyrsa init-pki\n./easy-rsa/easyrsa3/easyrsa build-ca nopass\n\n# create cert req\nopenssl genrsa -out my-server.key\nopenssl req -new -key my-server.key -out my-server.req\n# Common Name --&gt; *.aws.panlm.xyz\n# display. if you want to modify, check the first link below\nopenssl req -in my-server.req -noout -subject\n\n# sign cert\n./easy-rsa/easyrsa3/easyrsa import-req my-server.req my-server\n./easy-rsa/easyrsa3/easyrsa sign-req server my-server\n# need root ca password\n\n# convert to pem\nopenssl x509 -inform PEM -in pki/issued/my-server.crt &gt;my-server.pem\nopenssl rsa -in my-server.key  &gt; my-server-key.pem\nopenssl x509 -inform PEM -in pki/ca.crt &gt;my-server-chain.pem\n# 3 pems for certificate body / certificate private key / certificate chain\n</code></pre>","tags":["aws/security/acm","aws/container/eks"]},{"location":"others/self-signed-certificates/#refer_1","title":"refer","text":"<ul> <li>How To Set Up and Configure a Certificate Authority (CA) On Ubuntu 20.04 | DigitalOcean</li> <li>https://github.com/OpenVPN/easy-rsa/blob/master/README.quickstart.md</li> </ul>","tags":["aws/security/acm","aws/container/eks"]},{"location":"others/self-signed-certificates/#2-no-certificate-chain-","title":"2-no-certificate-chain-","text":"<ul> <li>works for alb, not for api gateway</li> </ul> <ol> <li> <p>create self-signed certificate <pre><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout privateKey.key -out certificate.crt\n\n# ... need input Common Name at least\n\nopenssl rsa -in privateKey.key -check\nopenssl x509 -in certificate.crt -text -noout\nopenssl rsa -in privateKey.key -text &gt; private.pem\nopenssl x509 -inform PEM -in certificate.crt &gt; public.pem\n</code></pre></p> </li> <li> <p>import certificate (2 pem files) to ACM in your region</p> </li> <li>add following to ingress yaml and apply it <pre><code>alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\nalb.ingress.kubernetes.io/ssl-redirect: '443'\nalb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:xxxxxx:certificate/xxxxxx\n</code></pre></li> <li>add certificate to local keychain (1 crt file) / just type <code>thisisunsafe</code></li> <li>access URL</li> </ol>","tags":["aws/security/acm","aws/container/eks"]},{"location":"others/self-signed-certificates/#refer_2","title":"refer","text":"<ul> <li>works for api gateway and alb</li> <li>acm-issue-certificates</li> <li>aws_signing_helper</li> </ul>","tags":["aws/security/acm","aws/container/eks"]},{"location":"serverless/apigw-cross-account-private-endpoint/","title":"apigw-cross-account-private-endpoint","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-cross-account-private-endpoint/#how-can-i-access-an-api-gateway-private-rest-api-in-another-aws-account-using-an-interface-vpc-endpoint","title":"How can I access an API Gateway private REST API in another AWS account using an interface VPC endpoint","text":"","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-cross-account-private-endpoint/#topo","title":"topo","text":"<ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-private-cross-account-vpce/?nc1=h_ls</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-cross-account-private-endpoint/#in-account-a","title":"In Account A","text":"<ul> <li>create vpc &amp; <code>execute-api</code> endpoint</li> <li>no peering or tgw needed</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-cross-account-private-endpoint/#in-api-gateway-service-account","title":"In API gateway Service Account","text":"","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-cross-account-private-endpoint/#access-control-with-cidr","title":"access-control-with-cidr","text":"<ul> <li>works.  <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Deny\",\n            \"Principal\": \"*\",\n            \"Action\": \"execute-api:Invoke\",\n            \"Resource\": \"execute-api:/*/*/*\",\n            \"Condition\": {\n                \"NotIpAddress\": {\n                    \"aws:SourceIp\": \"10.251.0.0/16\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"execute-api:Invoke\",\n            \"Resource\": \"execute-api:/*/*/*\"\n        }\n    ]\n}\n</code></pre></li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-cross-account-private-endpoint/#access-control-with-vpce","title":"access-control-with-vpce","text":"<ul> <li>works. <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Deny\",\n      \"Principal\": \"*\",\n      \"Action\": \"execute-api:Invoke\",\n      \"Resource\": \"execute-api:/*/*/*\",\n      \"Condition\": {\n        \"StringNotEquals\": {\n          \"aws:sourceVpce\": [\"vpce-0e8c7xxb45\",\"vpce-05d2xx259\"]\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"execute-api:Invoke\",\n      \"Resource\": \"execute-api:/*/*/*\"\n    }\n  ]\n}\n</code></pre></li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-cross-account-private-endpoint/#access-control-with-vpc","title":"access-control-with-vpc","text":"<ul> <li>works <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Deny\",\n            \"Principal\": \"*\",\n            \"Action\": \"execute-api:Invoke\",\n            \"Resource\": \"execute-api:/*/*/*\",\n            \"Condition\": {\n                \"StringNotEquals\": {\n                    \"aws:sourceVpc\": \"vpc-0204axxx0\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"execute-api:Invoke\",\n            \"Resource\": \"execute-api:/*/*/*\"\n        }\n    ]\n}\n</code></pre></li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-cross-account-private-endpoint/#deploy","title":"deploy","text":"<ul> <li>redeploy after you change <code>Resource Policy</code></li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-cross-account-private-endpoint/#refer","title":"refer","text":"<ul> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-authorization-flow.html</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-custom-domain-name/","title":"Custom Domain Name in API Gateway","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-custom-domain-name/#custom-domain-name-in-api-gateway","title":"Custom Domain Name in API Gateway","text":"","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-custom-domain-name/#for-private-api","title":"for private api","text":"<ul> <li>\u5728 acm \u4e2d\u53d1\u5e03\u8bc1\u4e66 <code>*.api.aws.panlm.xyz</code></li> <li>\u5728\u4e0e API \u540c\u533a\u57df\u4e2d\u521b\u5efa route53 \u7684 public host zone <code>api.aws.panlm.xyz</code></li> <li>\u521b\u5efa \u79c1\u6709 API </li> <li>\u521b\u5efa interface endpoint \uff0c\u5e76\u4e14\u914d\u7f6e resource policy \u5141\u8bb8\u4ece\u8be5 <code>vpce</code> \u8bbf\u95ee</li> <li>\u521b\u5efa\u5b9a\u5236\u57df\u540d\uff0c\u4f8b\u5982 <code>api1.api.aws.panlm.xyz</code><ul> <li>\u6620\u5c04\u5230 private api \u7684\u67d0\u4e2a stage</li> </ul> </li> <li>\u5728 route53 \u4e2d\uff0c\u5c06\u5b9a\u5236\u57df\u540d alias \u5230 vpce \u7684 dns \u4e0a\uff0c\u8bbf\u95ee\u53ef\u4ee5\u6210\u529f\uff0c\u56e0\u4e3a\u4ece\u8be5 vpce \u8bbf\u95ee api \u662f\u88ab\u5141\u8bb8\u7684 \uff08\u4f46\u662f\u5b58\u5728\u8bc1\u4e66 issue\uff0c\u9700\u8981 <code>curl -k</code> \uff09<ul> <li>\u5982\u679c\u8be5 api \u662f\u79c1\u6709\u7684\uff0c\u5b9a\u5236\u57df\u540d alias \u5230 apigw \u4e0a\uff0c\u5219\u5c06\u8bbf\u95ee\u7981\u6b62 <code>Forbidden</code> \u3002</li> </ul> </li> <li>\u9700\u8981\u4f7f\u7528 ALB \u6216\u8005 NLB over TLS \u6d88\u9664\u8bc1\u4e66 issue \uff08link\uff09</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-custom-domain-name/#dns","title":"dns \u89e3\u6790","text":"<p>\u4f7f\u7528 route 53 \u5c06\u81ea\u5b9a\u4e49\u57df\u540d\uff08\u4f8b\u5982\uff1a <code>api1.api.aws.panlm.xyz</code> \uff09\u6309\u4e0b\u8868\u8fdb\u884c\u89e3\u6790</p> enable endpoint private dns name disable endpoint private dns name alias {\u201cmessage\u201d:\u201dForbidden\u201d} {\u201cmessage\u201d:\u201dForbidden\u201d} cname certificate issue {\u201cmessage\u201d:\u201dForbidden\u201d} api name certificate issue Could not resolve host vpce certificate issue certificate issue <ul> <li>\u81ea\u5b9a\u4e49\u57df\u540d\u901a\u8fc7 alias \u6307\u5411 api gateway \u4e2d\u914d\u7f6e\u7684 custom domain name \u7684\u57df\u540d</li> <li>\u81ea\u5b9a\u4e49\u57df\u540d\u901a\u8fc7 cname \u6307\u5411 api gateway \u4e2d\u914d\u7f6e\u7684 custom domain name \u7684\u57df\u540d</li> <li>\u81ea\u5b9a\u4e49\u57df\u540d\u901a\u8fc7 cname \u6307\u5411 api gateway \u4e2d\u914d\u7f6e\u7684 api \u7684 url \u4e2d\u7684\u57df\u540d</li> <li>\u81ea\u5b9a\u4e49\u57df\u540d\u901a\u8fc7 cname \u6307\u5411 vpc endpoint \u7684 dns \u540d\u79f0</li> </ul> <p>\u7ed3\u8bba \u4e0a\u8ff0\u64cd\u4f5c\uff0c\u5206\u522b\u4f7f\u7528 \u542f\u7528/\u7981\u7528 endpoint \u7684\u79c1\u6709 dns \u540d\u79f0\uff0c\u5747\u65e0\u6cd5\u6b63\u5e38\u8bbf\u95ee api\u3002\u8bbf\u95ee\u7981\u6b62\uff0c\u6216\u8005 certificate issue \uff08\u201dno alternative certificate subject name matches target host name\u201d\uff09</p>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-custom-domain-name/#refer","title":"refer","text":"<ul> <li>https://serverlessland.com/repos/apigw-private-custom-domain-name</li> <li>https://github.com/aws-samples/serverless-patterns/tree/main/public-alb-private-api-terraform</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-custom-domain-name/#for-regional-api","title":"for regional api","text":"<ul> <li>\u5982\u679c\u8be5 api \u662f regional \uff0c\u521b\u5efa\u5b9a\u5236\u57df\u540d\uff0c\u8bb0\u5f55\u4e0b\u5b9a\u5236\u57df\u540d\u914d\u7f6e\u4e2d\u7684 <code>API Gateway domain name</code>\uff0c\u4e0d\u662f api stage \u9875\u9762\u4e2d\u7684\u57df\u540d\uff0c\u53c2\u7167 apigw-regional-api-access-from-vpc </li> <li>\u4f7f\u7528 route53 alias\uff08\u6216 cname\uff09 \u5c06\u5b9a\u5236\u57df\u540d\u6307\u5411 <code>API Gateway domain name</code></li> <li>\u8bbf\u95ee\u5b9a\u5236\u57df\u540d\uff0c\u6d4f\u89c8\u5668\u5c06\u663e\u793a\u8bc1\u4e66\u6709\u6548</li> <li>\u5982\u679c\u5b9a\u5236\u57df\u540d\u6307\u5411 api \u7684 url \uff08\u5728 api stage \u9875\u9762\u4e2d\uff09\uff0c\u5c06\u9047\u5230\u8bc1\u4e66\u9a8c\u8bc1\u4e0d\u901a\u8fc7\u7684\u95ee\u9898</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-custom-domain-name/#dns_1","title":"dns \u89e3\u6790","text":"<p>\u4f7f\u7528 route 53 \u5c06\u81ea\u5b9a\u4e49\u57df\u540d\uff08\u4f8b\u5982\uff1a <code>api1.api.aws.panlm.xyz</code> \uff09\u6309\u4e0b\u8868\u8fdb\u884c\u89e3\u6790</p> regional api alias success cname success api name certificate issue <ul> <li>\u81ea\u5b9a\u4e49\u57df\u540d\u901a\u8fc7 alias \u6307\u5411 api gateway \u4e2d\u914d\u7f6e\u7684 custom domain name \u7684\u57df\u540d</li> <li>\u81ea\u5b9a\u4e49\u57df\u540d\u901a\u8fc7 cname \u6307\u5411 api gateway \u4e2d\u914d\u7f6e\u7684 custom domain name \u7684\u57df\u540d</li> <li>\u81ea\u5b9a\u4e49\u57df\u540d\u901a\u8fc7 cname \u6307\u5411 api gateway \u4e2d\u914d\u7f6e\u7684 api \u7684 url \u4e2d\u7684\u57df\u540d</li> </ul> <p>\u7ed3\u8bba</p> <p>\u524d\u4e24\u8005\u53ef\u4ee5\u6b63\u5e38\u8bbf\u95ee api</p>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-custom-domain-name/#refer_1","title":"refer","text":"<ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-domain-certificate/</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-get-sourceip/","title":"Get Source IP in API Gateway","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-get-sourceip/#get-source-ip-in-api-gateway","title":"Get Source IP in API Gateway","text":"","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-get-sourceip/#client-ip","title":"client ip","text":"<ul> <li> <p>create function nodejs 18x <pre><code>export const handler = async (event,context) =&gt; {\n    console.log('Received event:', JSON.stringify(event, null, 2));\n    context.succeed(event);\n};\n</code></pre></p> </li> <li> <p>create rest api </p> </li> <li>create <code>POST</code> method and integration request to lambda</li> <li> <p>in <code>method execution</code>, add <code>mapping templates</code>, content-type is <code>application/json</code>,  content as following: <pre><code>{\n  \"sourceIp\" : \"$context.identity.sourceIp\",\n  \"input\" : \"$input.path('$')\"\n}\n</code></pre></p> </li> <li> <p>deploy it and test </p> </li> <li>curl <pre><code>curl -X POST -H 'Content-type: application/json' \\\n-d '{ \"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\" }' \\\nhttps://xxx.execute-api.us-east-2.amazonaws.com/prod/\n</code></pre></li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-get-sourceip/#alb-ip","title":"alb ip","text":"<ul> <li>\u5982\u679c alb \u5728 api gateway \u524d\u7aef\uff0c\u5c06\u83b7\u53d6\u5230 alb \u7684\u5185\u7f51\u5730\u5740\uff0c\u5982\u4e0b\u573a\u666f\uff1a<ul> <li>apigw-private-api-alb-cdk.md</li> </ul> </li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-get-sourceip/#refer","title":"refer","text":"<ul> <li>https://dev.classmethod.jp/articles/api-gateway-client-ip/</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-private-api-alb-cdk/","title":"apigw-private-api-alb-cdk","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/serverless/api-gateway","aws/mgmt/cdk"]},{"location":"serverless/apigw-private-api-alb-cdk/#aws-cdk-private-api-and-application-load-balancer-demo","title":"AWS CDK Private API and Application Load Balancer Demo","text":"<ul> <li>https://github.com/markilott/aws-cdk-internal-private-api-demo</li> </ul>","tags":["aws/serverless/api-gateway","aws/mgmt/cdk"]},{"location":"serverless/apigw-private-api-alb-cdk/#prep","title":"prep","text":"<ul> <li>\u521b\u5efa host zone \u53ef\u4ee5\u88ab\u4f60\u7684\u57df\u540d\u89e3\u6790\u5230 \uff08\u5728\u4e0a\u6e38 route53 \u6dfb\u52a0 NS \u8bb0\u5f55\uff09</li> <li>\u521b\u5efa\u65b0vpc\uff0c\u4e0d\u8981\u521b\u5efa api gateway \u7684 endpoint</li> <li>\u521b\u5efa cloud9 \u5728\u65b0 vpc</li> </ul>","tags":["aws/serverless/api-gateway","aws/mgmt/cdk"]},{"location":"serverless/apigw-private-api-alb-cdk/#lab-setup-","title":"lab-setup-","text":"<ul> <li>clone repo</li> <li> <p>edit <code>config/index.ts</code> <pre><code>export const options = {\n    vpcAttr: {\n        customVpcId: 'vpc-0a766975xxxxxxd45',\n        // These are the AWS default VPC subnets. Update to your own CIDR's if using a custom VPC\n        subnetCidr1: '10.251.192.0/24',\n        subnetCidr2: '10.251.193.0/24',\n    },\n    createCertificate: false,\n    certificateArn: 'arn:aws:acm:us-east-2:7933xxxx2775:certificate/cc5xxxx07fc3',\n    dnsAttr: {\n        zoneName: 'api0320.aws.panlm.xyz',\n        hostedZoneId: 'Z0xxxx73xxxxYEARSVSP',\n    },\n    albHostname: 'test-alb',\n    apiPath1: 'test-api1',\n    apiPath2: 'test-api2',\n};\n</code></pre></p> </li> <li> <p>deploy <pre><code>npm install\ncdk bootstrap\ncdk deploy --all --require-approval never\n</code></pre></p> </li> </ul>","tags":["aws/serverless/api-gateway","aws/mgmt/cdk"]},{"location":"serverless/apigw-private-api-alb-cdk/#data-flow","title":"data flow","text":"","tags":["aws/serverless/api-gateway","aws/mgmt/cdk"]},{"location":"serverless/apigw-private-api-alb-cdk/#target-group-settings","title":"target group settings","text":"","tags":["aws/serverless/api-gateway","aws/mgmt/cdk"]},{"location":"serverless/apigw-private-api-alb-cdk/#refer","title":"refer","text":"<ul> <li>https://georgemao.medium.com/enabling-private-apis-with-custom-domain-names-aws-api-gateway-df1b62b0ba7c</li> <li>https://serverlessland.com/repos/apigw-private-custom-domain-name</li> </ul>","tags":["aws/serverless/api-gateway","aws/mgmt/cdk"]},{"location":"serverless/apigw-regional-api-access-from-vpc/","title":"apigw-regional-api-access-from-vpc","text":"<p>[!WARNING] This is a github note</p>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-regional-api-access-from-vpc/#apigw-regional-api-access-from-vpc","title":"apigw-regional-api-access-from-vpc","text":"<ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-vpc-connections/</li> </ul> <p>Connect to public APIs with private DNS enabled</p> <p>If private DNS is enabled, set up edge-optimized custom domain names or regional custom domain names to connect to your public APIs.</p> <p>Important: Resources in your VPC that try to connect to your public APIs must have internet connectivity. Also, when configuring DNS records for a regional custom domain name, you must use A type alias records. However, with edge-optimized custom domain names, use either A type alias records or CNAME records.</p>","tags":["aws/serverless/api-gateway"]},{"location":"serverless/apigw-regional-api-access-from-vpc/#refer","title":"refer","text":"<ul> <li>https://docs.aws.amazon.com/whitepapers/latest/best-practices-api-gateway-private-apis-integration/rest-api.html</li> </ul>","tags":["aws/serverless/api-gateway"]},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#awsaimltranscribe","title":"aws/aiml/transcribe","text":"<ul> <li>script-convert-mp3-to-text</li> </ul>"},{"location":"tags/#awsanalytics","title":"aws/analytics","text":"<ul> <li>mwaa-lab</li> </ul>"},{"location":"tags/#awsanalyticsgluedatabrew","title":"aws/analytics/glue/databrew","text":"<ul> <li>Stream EKS Control Panel Logs to S3</li> </ul>"},{"location":"tags/#awsanalyticskinesisfirehose","title":"aws/analytics/kinesis/firehose","text":"<ul> <li>cloudwatch-to-firehose-python</li> <li>Stream EKS Control Panel Logs to S3</li> </ul>"},{"location":"tags/#awschina","title":"aws/china","text":"<ul> <li>Create Public Access EKS Cluster in China Region</li> <li>Cross Region Reverse Proxy with NLB and Cloudfront</li> </ul>"},{"location":"tags/#awscloud9","title":"aws/cloud9","text":"<ul> <li>cloud9</li> <li>quick setup cloud9 script</li> <li>Setup Cloud9 for EKS</li> </ul>"},{"location":"tags/#awscmd","title":"aws/cmd","text":"<ul> <li>acm</li> <li>api-gateway</li> <li>ip-ranges</li> <li>cloud9</li> <li>cloudformation</li> <li>directory service</li> <li>ebs</li> <li>ec2</li> <li>ecr</li> <li>ecs</li> <li>efs</li> <li>iam</li> <li>lambda-cmd</li> <li>rds</li> <li>s3</li> <li>ssm</li> <li>vpc</li> <li>func-create-sg.sh</li> <li>eksdemo</li> <li>script-convert-mp3-to-text</li> </ul>"},{"location":"tags/#awscomputeec2","title":"aws/compute/ec2","text":"<ul> <li>ec2</li> <li>Rescue EC2 Instance</li> </ul>"},{"location":"tags/#awscontainerappmesh","title":"aws/container/appmesh","text":"<ul> <li>appmesh-workshop-eks</li> <li>automated-canary-deployment-using-flagger</li> </ul>"},{"location":"tags/#awscontainerecr","title":"aws/container/ecr","text":"<ul> <li>ecr</li> <li>Enable scan on push in ECR and send notification to SNS</li> </ul>"},{"location":"tags/#awscontainerecs","title":"aws/container/ecs","text":"<ul> <li>ecs</li> </ul>"},{"location":"tags/#awscontainereks","title":"aws/container/eks","text":"<ul> <li>eksctl</li> <li>eksdemo</li> <li>aws-for-fluent-bit</li> <li>aws-load-balancer-controller</li> <li>cert-manager</li> <li>cluster-autoscaler</li> <li>cni-metrics-helper</li> <li>ebs-for-eks</li> <li>efs-for-eks</li> <li>eks-addons-coredns</li> <li>eks-addons-kube-proxy</li> <li>eks-addons-vpc-cni</li> <li>eks-custom-network</li> <li>eks-fargate</li> <li>eksup</li> <li>enable-sg-on-pod</li> <li>karpenter-install-lab</li> <li>metrics-server</li> <li>nginx-ingress-controller-community-ver</li> <li>nginx-ingress-controller-nginx-ver</li> <li>EKS Addons</li> <li>Create EKS Cluster with Terraform</li> <li>Create Private Only EKS Cluster</li> <li>Create Public Access EKS Cluster in China Region</li> <li>Create Public Access EKS Cluster</li> <li>EKS Upgrade Procedure</li> <li>appmesh-workshop-eks</li> <li>automated-canary-deployment-using-flagger</li> <li>flux</li> <li>Stream EKS Control Panel Logs to S3</li> <li>Building Prometheus HA Architect with Thanos</li> <li>EKS Container Insights</li> <li>enable-prometheus-in-cloudwatch</li> <li>install-prometheus-grafana-on-eks</li> <li>quick setup cloud9 script</li> <li>Setup Cloud9 for EKS</li> <li>self-signed-certificates</li> </ul>"},{"location":"tags/#awscontainerfargate","title":"aws/container/fargate","text":"<ul> <li>eks-fargate</li> </ul>"},{"location":"tags/#awscontainerkarpenter","title":"aws/container/karpenter","text":"<ul> <li>karpenter-install-lab</li> </ul>"},{"location":"tags/#awsdatabaserds","title":"aws/database/rds","text":"<ul> <li>rds</li> <li>rds-mysql-replica-cross-region-cross-account</li> </ul>"},{"location":"tags/#awsdatabaseredshift","title":"aws/database/redshift","text":"<ul> <li>redshift</li> <li>redshift-data-api-lab</li> </ul>"},{"location":"tags/#awsintegrationsns","title":"aws/integration/sns","text":"<ul> <li>sns</li> <li>Enable scan on push in ECR and send notification to SNS</li> </ul>"},{"location":"tags/#awsintegrationsqs","title":"aws/integration/sqs","text":"<ul> <li>sqs</li> </ul>"},{"location":"tags/#awsmgmtcdk","title":"aws/mgmt/cdk","text":"<ul> <li>apigw-private-api-alb-cdk</li> </ul>"},{"location":"tags/#awsmgmtcloudformation","title":"aws/mgmt/cloudformation","text":"<ul> <li>cloudformation</li> <li>create standard vpc for lab in china region</li> </ul>"},{"location":"tags/#awsmgmtcloudwatch","title":"aws/mgmt/cloudwatch","text":"<ul> <li>cloudwatch</li> <li>cloudwatch-to-firehose-python</li> <li>Export Cloudwatch Log Group to S3</li> <li>EKS Container Insights</li> <li>enable-prometheus-in-cloudwatch</li> <li>create-dashboard-for-instance-cpu-matrics</li> </ul>"},{"location":"tags/#awsmgmtdirectory-service","title":"aws/mgmt/directory-service","text":"<ul> <li>directory service</li> <li>migrate-filezilla-to-transfer-family</li> </ul>"},{"location":"tags/#awsmgmtsystems-manager","title":"aws/mgmt/systems-manager","text":"<ul> <li>ssm</li> </ul>"},{"location":"tags/#awsnetwork","title":"aws/network","text":"<ul> <li>ip-ranges</li> </ul>"},{"location":"tags/#awsnetworkcloudfront","title":"aws/network/cloudfront","text":"<ul> <li>Cross Region Reverse Proxy with NLB and Cloudfront</li> </ul>"},{"location":"tags/#awsnetworknlb","title":"aws/network/nlb","text":"<ul> <li>Cross Region Reverse Proxy with NLB and Cloudfront</li> </ul>"},{"location":"tags/#awsnetworkroute53","title":"aws/network/route53","text":"<ul> <li>route53</li> <li>externaldns-for-route53</li> </ul>"},{"location":"tags/#awsnetworksecurity-group","title":"aws/network/security-group","text":"<ul> <li>enable-sg-on-pod</li> </ul>"},{"location":"tags/#awsnetworkvpc","title":"aws/network/vpc","text":"<ul> <li>vpc</li> <li>create standard vpc for lab in china region</li> </ul>"},{"location":"tags/#awssecurityacm","title":"aws/security/acm","text":"<ul> <li>acm</li> <li>self-signed-certificates</li> </ul>"},{"location":"tags/#awssecurityiam","title":"aws/security/iam","text":"<ul> <li>iam</li> <li>assume-tool</li> </ul>"},{"location":"tags/#awssecurityidentity-center","title":"aws/security/identity-center","text":"<ul> <li>Using Global SSO to Login China AWS Accounts</li> </ul>"},{"location":"tags/#awsserverlessapi-gateway","title":"aws/serverless/api-gateway","text":"<ul> <li>script-api-resource-method</li> <li>apigw-cross-account-private-endpoint</li> <li>Custom Domain Name in API Gateway</li> <li>Get Source IP in API Gateway</li> <li>apigw-private-api-alb-cdk</li> <li>apigw-regional-api-access-from-vpc</li> </ul>"},{"location":"tags/#awsserverlesslambda","title":"aws/serverless/lambda","text":"<ul> <li>lambda-cmd</li> <li>cloudwatch-to-firehose-python</li> </ul>"},{"location":"tags/#awsstorageebs","title":"aws/storage/ebs","text":"<ul> <li>ebs</li> <li>ebs-for-eks</li> </ul>"},{"location":"tags/#awsstorageefs","title":"aws/storage/efs","text":"<ul> <li>efs</li> <li>efs-for-eks</li> </ul>"},{"location":"tags/#awsstorages3","title":"aws/storage/s3","text":"<ul> <li>s3</li> <li>Export Cloudwatch Log Group to S3</li> <li>Stream EKS Control Panel Logs to S3</li> </ul>"},{"location":"tags/#awsstoragestorage-gateway","title":"aws/storage/storage-gateway","text":"<ul> <li>Storage File Gateway</li> </ul>"},{"location":"tags/#awsstoragetransfer-family","title":"aws/storage/transfer-family","text":"<ul> <li>migrate-filezilla-to-transfer-family</li> </ul>"},{"location":"tags/#bashfunction","title":"bash/function","text":"<ul> <li>func-create-sg.sh</li> </ul>"},{"location":"tags/#cmd","title":"cmd","text":"<ul> <li>api-gateway</li> <li>assume-tool</li> <li>docker</li> <li>iptables</li> <li>linux-cmd</li> </ul>"},{"location":"tags/#cmdjq","title":"cmd/jq","text":"<ul> <li>jq</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>docker</li> </ul>"},{"location":"tags/#flagger","title":"flagger","text":"<ul> <li>automated-canary-deployment-using-flagger</li> </ul>"},{"location":"tags/#github","title":"github","text":"<ul> <li>github-page-howto</li> </ul>"},{"location":"tags/#gitopsargo","title":"gitops/argo","text":"<ul> <li>argocd</li> </ul>"},{"location":"tags/#gitopsweaveworksflux","title":"gitops/weaveworks/flux","text":"<ul> <li>flux</li> </ul>"},{"location":"tags/#grafana","title":"grafana","text":"<ul> <li>Install Grafana on Beanstalk</li> <li>install-prometheus-grafana-on-eks</li> </ul>"},{"location":"tags/#grafanaloki","title":"grafana/loki","text":"<ul> <li>Using Loki for Logging</li> </ul>"},{"location":"tags/#kubernetes","title":"kubernetes","text":"<ul> <li>aws-for-fluent-bit</li> <li>cert-manager</li> <li>cluster-autoscaler</li> <li>eksup</li> <li>externaldns-for-route53</li> <li>kube-no-trouble</li> <li>kube-state-metrics</li> <li>metrics-server</li> <li>pluto</li> <li>EKS Addons</li> <li>horizontal pod autoscaler</li> <li>topology spread constraints</li> <li>Building Prometheus HA Architect with Thanos</li> </ul>"},{"location":"tags/#kubernetescni","title":"kubernetes/cni","text":"<ul> <li>eks-custom-network</li> </ul>"},{"location":"tags/#kubernetesingress","title":"kubernetes/ingress","text":"<ul> <li>aws-load-balancer-controller</li> <li>nginx-ingress-controller-community-ver</li> <li>nginx-ingress-controller-nginx-ver</li> <li>nginx-ingress-controller</li> </ul>"},{"location":"tags/#linux","title":"linux","text":"<ul> <li>docker</li> <li>iptables</li> <li>linux-cmd</li> </ul>"},{"location":"tags/#microsoftpowershell","title":"microsoft/powershell","text":"<ul> <li>powershell</li> </ul>"},{"location":"tags/#microsoftwindows","title":"microsoft/windows","text":"<ul> <li>powershell</li> </ul>"},{"location":"tags/#nginx","title":"nginx","text":"<ul> <li>nginx-ingress-controller-community-ver</li> <li>nginx-ingress-controller-nginx-ver</li> </ul>"},{"location":"tags/#prometheus","title":"prometheus","text":"<ul> <li>Building Prometheus HA Architect with Thanos</li> <li>enable-prometheus-in-cloudwatch</li> <li>install-prometheus-grafana-on-eks</li> <li>POC-prometheus-with-thanos-manually</li> </ul>"},{"location":"tags/#python","title":"python","text":"<ul> <li>jq</li> <li>cloudwatch-to-firehose-python</li> </ul>"},{"location":"tags/#terraform","title":"terraform","text":"<ul> <li>terraform</li> <li>Create EKS Cluster with Terraform</li> </ul>"},{"location":"tags/#todo","title":"todo","text":"<ul> <li>vpc</li> </ul>"}]}